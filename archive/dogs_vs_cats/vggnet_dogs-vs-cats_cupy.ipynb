{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8590005,"sourceType":"datasetVersion","datasetId":5137997}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import cupy as cp\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:10:05.025839Z","iopub.execute_input":"2024-06-03T04:10:05.026499Z","iopub.status.idle":"2024-06-03T04:10:05.034184Z","shell.execute_reply.started":"2024-06-03T04:10:05.026466Z","shell.execute_reply":"2024-06-03T04:10:05.033239Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import cupy as cp\n\n# Generate a random array on GPU\nx_gpu = cp.random.randn(3, 3)\n\n# Perform some operations on the array\ny_gpu = cp.sin(x_gpu) + cp.cos(x_gpu)\n\n# Transfer the result back to CPU\ny_cpu = cp.asnumpy(y_gpu)\n\n# Print the result\nprint(\"Original array on GPU:\")\nprint(x_gpu)\nprint(\"\\nResult array on GPU:\")\nprint(y_gpu)\nprint(\"\\nResult array on CPU:\")\nprint(y_cpu)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:10:06.077779Z","iopub.execute_input":"2024-06-03T04:10:06.078553Z","iopub.status.idle":"2024-06-03T04:10:07.561556Z","shell.execute_reply.started":"2024-06-03T04:10:06.078517Z","shell.execute_reply":"2024-06-03T04:10:07.560499Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Original array on GPU:\n[[ 1.41887935 -1.24628621 -1.5654162 ]\n [ 0.29132549 -2.01655029 -0.248686  ]\n [-1.05822349 -0.43371707 -0.71966692]]\n\nResult array on GPU:\n[[ 1.1398161  -0.62896253 -0.99460543]\n [ 1.24508613 -1.33342415  0.72310608]\n [-0.38106456  0.48716333  0.09289109]]\n\nResult array on CPU:\n[[ 1.1398161  -0.62896253 -0.99460543]\n [ 1.24508613 -1.33342415  0.72310608]\n [-0.38106456  0.48716333  0.09289109]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# image_path = '/kaggle/input/training/dog.3.jpg' \n# image = Image.open(image_path)\n\n# # Resize the image\n# resized_image = image.resize((224, 224))\n\n# # If the image is not in RGB mode, convert it to RGB\n# if resized_image.mode != 'RGB':\n#     resized_image = resized_image.convert('RGB')\n\n# # Convert the resized image to a numpy array\n# image_array = np.array(resized_image)\n\n# # Display the resized image\n# plt.imshow(image_array)\n# plt.axis('off')\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dogs_num = 2000\ncats_num = 2000\nimage_size = (224, 224)\ntrain_dir = '/kaggle/input/training'\nvalidation_split = 0.2\n\n# Function to load and process images\ndef load_image(image_path):\n    image = Image.open(image_path)\n    resized_image = image.resize(image_size)\n    if resized_image.mode != 'RGB':\n        resized_image = resized_image.convert('RGB')\n    image_array = cp.array(resized_image)  # Convert to CuPy array\n    return image_array\n\n# Function to create dataset\ndef create_dataset(dir_path, dogs_num, cats_num):\n    data = []\n    labels = []\n    \n    for i in range(dogs_num):\n        image_path = os.path.join(dir_path, f'dog.{i}.jpg')\n        image_array = load_image(image_path)\n        label = cp.array([[1, 0]])  # Dog label, converted to CuPy array\n        data.append((image_array, label))\n        \n    for i in range(cats_num):\n        image_path = os.path.join(dir_path, f'cat.{i}.jpg')\n        image_array = load_image(image_path)\n        label = cp.array([[0, 1]])  # Cat label, converted to CuPy array\n        data.append((image_array, label))\n    \n    return data\n\n# Load training data\ntrain_data = create_dataset(train_dir, dogs_num, cats_num)\n\n# Convert the list of tuples to CuPy arrays\ntrain_images = cp.array([image for image, label in train_data])\ntrain_labels = cp.array([label for image, label in train_data])\n\n# Shuffle data\ncp.random.shuffle(train_images)\n\n# Manually split into training and validation sets\nsplit_index = int(len(train_data) * (1 - validation_split))\ntrain_set = list(zip(train_images, train_labels))[:split_index]\nval_set = list(zip(train_images, train_labels))[split_index:]\n\n# Example of accessing data\nprint(f'Training set size: {len(train_set)}')\nprint(f'Validation set size: {len(val_set)}')\n\n# Example: Access an image and label from the training set\nexample_image, example_label = train_set[0]\nprint(f'Example image shape: {example_image.shape}')\nprint(f'Example label: {example_label}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:12:12.545202Z","iopub.execute_input":"2024-06-03T04:12:12.545634Z","iopub.status.idle":"2024-06-03T04:12:32.547962Z","shell.execute_reply.started":"2024-06-03T04:12:12.545598Z","shell.execute_reply":"2024-06-03T04:12:32.546940Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Training set size: 3200\nValidation set size: 800\nExample image shape: (224, 224, 3)\nExample label: [[1 0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"isinstance(train_set[4][0], cp.ndarray)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:12:38.647951Z","iopub.execute_input":"2024-06-03T04:12:38.648347Z","iopub.status.idle":"2024-06-03T04:12:38.655788Z","shell.execute_reply.started":"2024-06-03T04:12:38.648306Z","shell.execute_reply":"2024-06-03T04:12:38.654687Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import cupy as cp\n\n# Setting up functions in forward propagation\n\n# We will forward propagate one piece of data at a time, then average over the examples, to simulate batches\ndef batch_norm_forward(x, gamma, beta, eps=1e-5):\n    mean = cp.mean(x, axis=0)\n    variance = cp.var(x, axis=0)  # std_dev ** 2\n    x_normalized = (x - mean) / cp.sqrt(variance + eps)\n    # gamma and beta are learned\n    out = gamma * x_normalized + beta\n    cache = (x, x_normalized, mean, variance, gamma, beta, eps)\n    return out, cache\n\ndef batch_norm_backward(dout, cache):\n    x, x_normalized, mean, variance, gamma, beta, eps = cache\n    N = x.shape[0]\n    \n    dbeta = cp.sum(dout, axis=0)\n    dgamma = cp.sum(dout * x_normalized, axis=0)\n    \n    dx_normalized = dout * gamma\n    dvariance = cp.sum(dx_normalized * (x - mean) * -0.5 * cp.power(variance + eps, -1.5), axis=0)\n    dmean = cp.sum(dx_normalized * -1 / cp.sqrt(variance + eps), axis=0) + dvariance * cp.sum(-2 * (x - mean), axis=0) / N\n    \n    dx = dx_normalized / cp.sqrt(variance + eps) + dvariance * 2 * (x - mean) / N + dmean / N\n    return dx, dgamma, dbeta\n\ndef max_pooling(input_data):\n    input_height, input_width, input_depth = input_data.shape\n\n    # Calculate the output dimensions\n    output_height = input_height // 2 \n    output_width = input_width // 2\n    output_depth = input_depth\n\n    # Initialize the output array and array to store indices\n    output_data = cp.zeros((output_height, output_width, output_depth))\n    indices = cp.zeros((output_height, output_width, output_depth, 2), dtype=int)\n\n    # Apply max pooling\n    for h in range(output_height):\n        for w in range(output_width):\n            for d in range(output_depth):\n                # Extract the 2x2 region of interest from the input data\n                region = input_data[h*2:(h+1)*2, w*2:(w+1)*2, d]\n                # Compute the maximum value in the region\n                max_val = cp.max(region)\n                output_data[h, w, d] = max_val\n                # Find the indices of the maximum value in the region\n                max_indices = cp.unravel_index(cp.argmax(region), region.shape)\n                # Store the indices relative to the region and convert to global indices\n                indices[h, w, d] = [h*2 + max_indices[0], w*2 + max_indices[1]]\n\n    return output_data, indices\n\ndef correlate(input_image, kernel):\n    # Dimensions of the input and kernel\n    input_height, input_width, input_channels = input_image.shape\n    filter_height, filter_width, filter_channels, num_filters = kernel.shape\n    \n    # Output dimensions\n    output_height = input_height - filter_height + 1  # (H - S + 1)\n    output_width = input_width - filter_width + 1  # (W - S + 1)\n\n    # Initialize the output\n    output = cp.zeros((output_height, output_width, num_filters))\n    \n    # Loop through each filter\n    for filter in range(num_filters):\n        # Loop through height and width of the output\n        for i in range(output_height):\n            for j in range(output_width):\n                # Extract the patch from the input image\n                input_patch = input_image[i:i+filter_height, j:j+filter_width, :]\n                # Perform the correlation operation\n                output[i, j, filter] = cp.sum(input_patch * kernel[:, :, :, filter])\n    return output\n\ndef convolve(input_image, kernel):\n    # Dimensions of the input and kernel\n    input_height, input_width, input_channels = input_image.shape\n    filter_height, filter_width, filter_channels, num_filters = kernel.shape\n    \n    # Padding size (pad_height and pad_width are half the kernel size, rounded down)\n    pad_height = filter_height - 1\n    pad_width = filter_width - 1\n    \n    # Output dimensions\n    output_height = input_height + pad_height\n    output_width = input_width + pad_width\n\n    # Initialize the output\n    output = cp.zeros((output_height, output_width, num_filters))\n    \n    # Pad the input image with zeros\n    padded_input = cp.pad(input_image, ((pad_height, pad_height), (pad_width, pad_width), (0, 0)), mode='constant')\n    \n    # Flip kernel horizontally and vertically\n    rotated_kernel = cp.flip(kernel, axis=(0, 1))\n    \n    # Loop through each filter\n    for filter in range(num_filters):\n        # Loop through the height and width of the output\n        for i in range(output_height):\n            for j in range(output_width):\n                # Extract the patch from the padded input image\n                input_patch = padded_input[i:i+filter_height, j:j+filter_width, :]\n                # Perform the convolution operation\n                output[i, j, filter] = cp.sum(input_patch * rotated_kernel[:, :, :, filter])\n    return output\n\ndef der_ReLU(Z):\n    return Z > 0\n\ndef ReLU(Z):\n    return cp.maximum(Z, 0)\n\ndef ReLU2(Z, alpha=0.01):\n    return cp.where(Z > 0, Z, alpha * Z)\n\ndef sigmoid(z):\n    # Compute the sigmoid function element-wise\n    return 1.0 / (1.0 + cp.exp(-z))\n\ndef sigmoid_prime(z):\n    return sigmoid(z) * (1 - sigmoid(z))\n\ndef softmax(Z):\n    exp_Z = cp.exp(Z - cp.max(Z, axis=1, keepdims=True))\n    sum_exp_Z = cp.sum(exp_Z, axis=1, keepdims=True)\n    return exp_Z / sum_exp_Z\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:12:41.681002Z","iopub.execute_input":"2024-06-03T04:12:41.681386Z","iopub.status.idle":"2024-06-03T04:12:41.709822Z","shell.execute_reply.started":"2024-06-03T04:12:41.681354Z","shell.execute_reply":"2024-06-03T04:12:41.708651Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Define parameters\n# Convolutional layer -> ReLU activation -> Batch normalization -> Dropout -> Max pooling\n\n# Layer 1\nconv1_1_kernel = cp.random.randn(3, 3, 3, 64)\nconv1_1_bias = cp.zeros((222, 222, 64))\nlayer1_1_output = cp.zeros((222, 222, 64))\n\nconv1_2_kernel = cp.random.randn(3, 3, 64, 64)\nconv1_2_bias = cp.zeros((220, 220, 64))\nlayer1_2_output = cp.zeros((220, 220, 64))\n\ngamma1_conv = cp.ones((220, 220, 64))\nbeta1_conv = cp.zeros((220, 220, 64))\n\nlayer1_pool = cp.zeros((110, 110, 64))\n\n# Layer 2\nconv2_1_kernel = cp.random.randn(3, 3, 64, 128)\nconv2_1_bias = cp.zeros((108, 108, 128))\nlayer2_1_output = cp.zeros((108, 108, 128))\n\nconv2_2_kernel = cp.random.randn(3, 3, 128, 128)\nconv2_2_bias = cp.zeros((106, 106, 128))\nlayer2_2_output = cp.zeros((106, 106, 128))\n\nconv2_3_kernel = cp.random.randn(3, 3, 128, 128)\nconv2_3_bias = cp.zeros((104, 104, 128))\nlayer2_3_output = cp.zeros((104, 104, 128))\n\ngamma2_conv = cp.ones((104, 104, 128))\nbeta2_conv = cp.zeros((104, 104, 128))\n\nlayer2_pool = cp.zeros((52, 52, 128))\n\n# Layer 3\nconv3_1_kernel = cp.random.randn(3, 3, 128, 256)\nconv3_1_bias = cp.zeros((50, 50, 256))\nlayer3_1_output = cp.zeros((50, 50, 256))\n\nconv3_2_kernel = cp.random.randn(3, 3, 256, 256)\nconv3_2_bias = cp.zeros((48, 48, 256))\nlayer3_2_output = cp.zeros((48, 48, 256))\n\ngamma3_conv = cp.ones((48, 48, 256))\nbeta3_conv = cp.zeros((48, 48, 256))\n\nlayer3_pool = cp.zeros((24, 24, 256))\n\n# Layer 4\nconv4_1_kernel = cp.random.randn(3, 3, 256, 512)\nconv4_1_bias = cp.zeros((22, 22, 512))\nlayer4_1_output = cp.zeros((22, 22, 512))\n\nconv4_2_kernel = cp.random.randn(3, 3, 512, 512)\nconv4_2_bias = cp.zeros((20, 20, 512))\nlayer4_2_output = cp.zeros((20, 20, 512))\n\nconv4_3_kernel = cp.random.randn(3, 3, 512, 512)\nconv4_3_bias = cp.zeros((18, 18, 512))\nlayer4_3_output = cp.zeros((18, 18, 512))\n\ngamma4_conv = cp.ones((18, 18, 512))\nbeta4_conv = cp.zeros((18, 18, 512))\n\nlayer4_pool = cp.zeros((9, 9, 512))\n\n# Layer 5\nconv5_1_kernel = cp.random.randn(3, 3, 512, 512)\nconv5_1_bias = cp.zeros((7, 7, 512))\nlayer5_1_output = cp.zeros((7, 7, 512))\n\nconv5_2_kernel = cp.random.randn(3, 3, 512, 512)\nconv5_2_bias = cp.zeros((5, 5, 512))\nlayer5_2_output = cp.zeros((5, 5, 512))\n\ngamma5_conv = cp.ones((5, 5, 512))\nbeta5_conv = cp.zeros((5, 5, 512))\n\n# Layer 6\nfc1_weights = cp.random.randn(12800, 4096)\nfc1_bias = cp.zeros((1, 4096))\nlayer6_1_output = cp.zeros((1, 4096))\n\nfc2_weights = cp.random.randn(4096, 2)\nfc2_bias = cp.zeros((1, 2))\nlayer6_2_output = cp.zeros((1, 2))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:12:44.695170Z","iopub.execute_input":"2024-06-03T04:12:44.695962Z","iopub.status.idle":"2024-06-03T04:12:44.997780Z","shell.execute_reply.started":"2024-06-03T04:12:44.695927Z","shell.execute_reply":"2024-06-03T04:12:44.996833Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def forward_propagation(\n    layer_input, conv1_1_kernel, conv1_1_bias, layer1_1_output, conv1_2_kernel, conv1_2_bias, layer1_2_output, gamma1_conv, beta1_conv, layer1_pool,\n    conv2_1_kernel, conv2_1_bias, layer2_1_output, conv2_2_kernel, conv2_2_bias, layer2_2_output, conv2_3_kernel, conv2_3_bias, layer2_3_output, gamma2_conv, beta2_conv, layer2_pool,\n    conv3_1_kernel, conv3_1_bias, layer3_1_output, conv3_2_kernel, conv3_2_bias, layer3_2_output, gamma3_conv, beta3_conv, layer3_pool,\n    conv4_1_kernel, conv4_1_bias, layer4_1_output, conv4_2_kernel, conv4_2_bias, layer4_2_output, conv4_3_kernel, conv4_3_bias, layer4_3_output, gamma4_conv, beta4_conv, layer4_pool,\n    conv5_1_kernel, conv5_1_bias, layer5_1_output, conv5_2_kernel, conv5_2_bias, layer5_2_output, gamma5_conv, beta5_conv, \n    fc1_weights, fc1_bias, layer6_1_output, fc2_weights, fc2_bias, layer6_2_output, dropout_rate = 0.25):\n    \n    layer1_1_output = correlate(layer_input, conv1_1_kernel)\n    layer1_1_output += conv1_1_bias\n    layer1_1_output = ReLU(layer1_1_output)\n    \n    layer1_2_output = correlate(layer1_1_output, conv1_2_kernel)\n    layer1_2_output += conv1_2_bias\n    layer1_2_output = ReLU(layer1_2_output)\n\n    layer1_2_output, layer1_cache = batch_norm_forward(layer1_2_output, gamma1_conv, beta1_conv)\n    \n    dropout1_mask = (cp.random.rand(*layer1_2_output.shape) < dropout_rate) / dropout_rate\n    layer1_2_output *= dropout1_mask\n\n    layer1_pool, layer1_indices = max_pooling(layer1_2_output)\n    print(\"here1\")\n\n    layer2_1_output = correlate(layer1_pool, conv2_1_kernel)\n    layer2_1_output += conv2_1_bias\n    layer2_1_output = ReLU(layer2_1_output)\n    \n    layer2_2_output = correlate(layer2_1_output, conv2_2_kernel)\n    layer2_2_output += conv2_2_bias\n    layer2_2_output = ReLU(layer2_2_output)\n\n    layer2_3_output = correlate(layer2_2_output, conv2_3_kernel)\n    layer2_3_output += conv2_3_bias\n    layer2_3_output = ReLU(layer2_3_output)\n\n    layer2_3_output, layer2_cache = batch_norm_forward(layer2_3_output, gamma2_conv, beta2_conv)\n    \n    dropout2_mask = (cp.random.rand(*layer2_3_output.shape) < dropout_rate) / dropout_rate \n    layer2_3_output *= dropout2_mask\n\n    layer2_pool, layer2_indices = max_pooling(layer2_3_output)\n    print(\"here2\")\n\n    layer3_1_output = correlate(layer2_pool, conv3_1_kernel)\n    layer3_1_output += conv3_1_bias\n    layer3_1_output = ReLU(layer3_1_output)\n    \n    layer3_2_output = correlate(layer3_1_output, conv3_2_kernel)\n    layer3_2_output += conv3_2_bias\n    layer3_2_output = ReLU(layer3_2_output)\n\n    layer3_2_output, layer3_cache = batch_norm_forward(layer3_2_output, gamma3_conv, beta3_conv)\n    \n    dropout3_mask = (cp.random.rand(*layer3_2_output.shape) < dropout_rate) / dropout_rate\n    layer3_2_output *= dropout3_mask\n\n    layer3_pool, layer3_indices = max_pooling(layer3_2_output)\n    print(\"here3\")\n\n    layer4_1_output = correlate(layer3_pool, conv4_1_kernel)\n    layer4_1_output += conv4_1_bias\n    layer4_1_output = ReLU(layer4_1_output)\n    \n    layer4_2_output = correlate(layer4_1_output, conv4_2_kernel)\n    layer4_2_output += conv4_2_bias\n    layer4_2_output = ReLU(layer4_2_output)\n\n    layer4_3_output = correlate(layer4_2_output, conv4_3_kernel)\n    layer4_3_output += conv4_3_bias\n    layer4_3_output = ReLU(layer4_3_output)\n\n    layer4_3_output, layer4_cache = batch_norm_forward(layer4_3_output, gamma4_conv, beta4_conv)\n    \n    dropout4_mask = (cp.random.rand(*layer4_3_output.shape) < dropout_rate) / dropout_rate \n    layer4_3_output *= dropout4_mask\n\n    layer4_pool, layer4_indices = max_pooling(layer4_3_output)\n    print(\"here4\")\n    \n    layer5_1_output = correlate(layer4_pool, conv5_1_kernel)\n    layer5_1_output += conv5_1_bias\n    layer5_1_output = ReLU(layer5_1_output)\n    \n    layer5_2_output = correlate(layer5_1_output, conv5_2_kernel)\n    layer5_2_output += conv5_2_bias\n    layer5_2_output = ReLU(layer5_2_output)\n\n    layer5_2_output, layer5_cache = batch_norm_forward(layer5_2_output, gamma5_conv, beta5_conv)\n    \n    dropout5_mask = (cp.random.rand(*layer5_2_output.shape) < dropout_rate) / dropout_rate\n    layer5_2_output *= dropout5_mask\n    print(\"here5\")\n        \n    layer6_1_output = (layer5_2_output.reshape(1, -1)) @ fc1_weights\n    layer6_1_output += fc1_bias\n    layer6_1_output = ReLU(layer6_1_output)\n    \n    layer6_2_output = layer6_1_output @ fc2_weights\n    layer6_2_output += fc2_bias\n    print(layer6_2_output)\n    layer6_2_output = softmax(layer6_2_output)\n    print(layer6_2_output)\n    \n    return (layer1_indices, layer1_cache, dropout1_mask, layer2_indices, layer2_cache, dropout2_mask, layer3_indices, layer3_cache, dropout3_mask, layer4_indices, layer4_cache, dropout4_mask, layer5_cache, dropout5_mask)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:12:47.564218Z","iopub.execute_input":"2024-06-03T04:12:47.564942Z","iopub.status.idle":"2024-06-03T04:12:47.589405Z","shell.execute_reply.started":"2024-06-03T04:12:47.564904Z","shell.execute_reply":"2024-06-03T04:12:47.588439Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def backward_pass(layer_input, label, conv1_1_kernel, conv1_1_bias, layer1_1_output, conv1_2_kernel, conv1_2_bias, layer1_2_output, gamma1_conv, beta1_conv, layer1_pool, layer1_indices, layer1_cache, dropout1_mask, \n            conv2_1_kernel, conv2_1_bias, layer2_1_output, conv2_2_kernel, conv2_2_bias, layer2_2_output, conv2_3_kernel, conv2_3_bias, layer2_3_output, gamma2_conv, beta2_conv, layer2_pool, layer2_indices, layer2_cache, dropout2_mask,\n            conv3_1_kernel, conv3_1_bias, layer3_1_output, conv3_2_kernel, conv3_2_bias, layer3_2_output, gamma3_conv, beta3_conv, layer3_pool, layer3_indices, layer3_cache, dropout3_mask,\n            conv4_1_kernel, conv4_1_bias, layer4_1_output, conv4_2_kernel, conv4_2_bias, layer4_2_output, conv4_3_kernel, conv4_3_bias, layer4_3_output, gamma4_conv, beta4_conv, layer4_pool, layer4_indices, layer4_cache, dropout4_mask,\n            conv5_1_kernel, conv5_1_bias, layer5_1_output, conv5_2_kernel, conv5_2_bias, layer5_2_output, gamma5_conv, beta5_conv, layer5_cache, dropout5_mask,\n            fc1_weights, fc1_bias, layer6_1_output, fc2_weights, fc2_bias, layer6_2_output):\n    \n    print(\"starting backprop\")\n    dconv5_2_kernel = cp.zeros_like(conv5_2_kernel)\n    dlayer5_2_output = cp.zeros_like(layer5_2_output)\n    \n    dconv5_1_kernel = cp.zeros_like(conv5_1_kernel)\n    dlayer5_1_output = cp.zeros_like(layer5_1_output)\n    \n    dlayer4_pool = cp.zeros_like(layer4_pool)\n\n    dconv4_1_kernel = cp.zeros_like(conv4_1_kernel)\n    dlayer4_1_output = cp.zeros_like(layer4_1_output)\n    \n    dconv4_2_kernel = cp.zeros_like(conv4_2_kernel)\n    dlayer4_2_output = cp.zeros_like(layer4_2_output)\n    \n    dconv4_3_kernel = cp.zeros_like(conv4_3_kernel)\n    dlayer4_3_output = cp.zeros_like(layer4_3_output)\n\n    dlayer3_pool = cp.zeros_like(layer3_pool)\n\n    dconv3_2_kernel = cp.zeros_like(conv3_2_kernel)\n    dlayer3_2_output = cp.zeros_like(layer3_2_output)\n    \n    dconv3_1_kernel = cp.zeros_like(conv3_1_kernel)\n    dlayer3_1_output = cp.zeros_like(layer3_1_output)\n    \n    dlayer2_pool = cp.zeros_like(layer2_pool)\n\n    dconv2_1_kernel = cp.zeros_like(conv2_1_kernel)\n    dlayer2_1_output = cp.zeros_like(layer2_1_output)\n    \n    dconv2_2_kernel = cp.zeros_like(conv2_2_kernel)\n    dlayer2_2_output = cp.zeros_like(layer2_2_output)\n    \n    dconv2_3_kernel = cp.zeros_like(conv2_3_kernel)\n    dlayer2_3_output = cp.zeros_like(layer2_3_output)\n\n    dlayer1_pool = cp.zeros_like(layer1_pool)\n\n    dconv1_2_kernel = cp.zeros_like(conv1_2_kernel)\n    dlayer1_2_output = cp.zeros_like(layer1_2_output)\n    \n    dconv1_1_kernel = cp.zeros_like(conv1_1_kernel)\n    dlayer1_1_output = cp.zeros_like(layer1_1_output)\n\n     ### Layer 6 ----------------------------------------------------------------------------------------------------------------------------------------\n    # Layer 6 - Softmax\n    dlayer6_2_output = layer6_2_output - label # (1,2)\n    \n    # fc2\n    dfc2_bias = dlayer6_2_output # (1,2)\n    \n    dfc2_weights = layer6_1_output.T @ dlayer6_2_output # (4096, 1) @ (1, 2) = (4096, 2)\n    \n    dlayer6_1_output = (dlayer6_2_output @ fc2_weights.T) * der_ReLU(layer6_1_output) # (1, 2) @ (2, 4096) = (1, 4096)\n    \n    # fc1\n    dfc1_bias = dlayer6_1_output\n    \n    dfc1_weights = (layer5_2_output.reshape(1, 12800).T) @ dlayer6_1_output # (12800, 1) @ (1, 4096) = (12800, 4096)\n    \n    dlayer5_2_output = (dlayer6_1_output @ fc1_weights.T) # (1, 4096) @ (4096, 12800) = (1, 12800)\n\n    \n    ### Layer 5 ----------------------------------------------------------------------------------------------------------------------------------------\n    # Unflatten\n    dlayer5_2_output = dlayer5_2_output.reshape(5, 5, 512)\n\n    # Layer 5 - Dropout\n    dlayer5_2_output *= dropout5_mask\n    \n    # Layer 5 - Batch norm\n    dlayer5_2_output, dgamma5_conv, dbeta5_conv = batch_norm_backward(dlayer5_2_output, layer5_cache)\n\n    # Layer 5 - conv2\n    dlayer5_2_output *= der_ReLU(layer5_2_output) # (5,5,512)\n    dconv5_2_bias = dlayer5_2_output # (5,5,512)\n\n    # Looping over filters\n    for filter in range(conv5_2_kernel.shape[3]): # 512\n        temp2 = dlayer5_2_output[:, :, filter].reshape((dlayer5_2_output.shape[0], dlayer5_2_output.shape[0], 1, 1))\n        for slice in range(conv5_2_kernel.shape[2]): # 512\n            temp1 = layer5_1_output[:, :, slice].reshape((layer5_1_output.shape[0], layer5_1_output.shape[0], 1))\n            dconv5_2_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,512,512)\n    print('done51')\n    \n    for filter in range(conv5_2_kernel.shape[3]): # 512\n        temp1 = dlayer5_2_output[:, :, filter].reshape((dlayer5_2_output.shape[0], dlayer5_2_output.shape[0], 1))\n        for slice in range(conv5_2_kernel.shape[2]): # 512\n            temp2 = conv5_2_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n            dlayer5_1_output += convolve(temp1, temp2)\n    print('done52')\n    \n     # Layer 5 - BP conv1\n    dlayer5_1_output *= der_ReLU(layer5_1_output) # (7,7,512)\n    dconv5_1_bias = dlayer5_1_output # (7,7,512)\n    \n    \n    # Looping over filters\n    for filter in range(conv5_1_kernel.shape[3]): # 512\n        temp2 = dlayer5_1_output[:, :, filter].reshape((dlayer5_1_output.shape[0], dlayer5_1_output.shape[0], 1, 1))\n        for slice in range(conv5_1_kernel.shape[2]): # 512\n            temp1 = layer4_pool[:, :, slice].reshape((layer4_pool.shape[0], layer4_pool.shape[0], 1))\n            dconv5_1_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,512,512)\n    print('done51')\n    \n    for filter in range(conv5_1_kernel.shape[3]): # 512\n        temp1 = dlayer5_1_output[:, :, filter].reshape((dlayer5_1_output.shape[0], dlayer5_1_output.shape[0], 1))\n        for slice in range(conv5_1_kernel.shape[2]): # 512\n            temp2 = conv5_1_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n            layer4_pool += convolve(temp1, temp2)\n    print('done52')\n\n    ### Layer 4 ----------------------------------------------------------------------------------------------------------------------------------------\n    \n    # Unpooling - (9, 9, 512) -> (18, 18, 512)\n    for i in range(dlayer4_pool.shape[0]):  # 9\n        for j in range(dlayer4_pool.shape[1]):  # 9\n            for k in range(dlayer4_pool.shape[2]): # 512\n                # Get the global indices from layer_indices\n                x_index, y_index = layer4_indices[i, j, k] # -> (18, 18, 512, 2)\n                # Assign the gradient from dZ_pool_output to the corresponding position in dZ_pool_input\n                dlayer4_3_output[x_index, y_index, k] = dlayer4_pool[i, j, k]\n    \n    # Layer 4 - Dropout\n    dlayer4_3_output *= dropout4_mask\n    \n    # Layer 4 - Batch norm\n    dlayer4_3_output, dgamma4_conv, dbeta4_conv = batch_norm_backward(dlayer4_3_output, layer4_cache)\n\n    # Layer 4 - BP conv3\n    dlayer4_3_output *= der_ReLU(layer4_3_output) # (18,18,512)\n    dconv4_3_bias = dlayer4_3_output # (18,18,512)\n\n    # Looping over filters\n    for filter in range(conv4_3_kernel.shape[3]): # 512\n        temp2 = dlayer4_3_output[:, :, filter].reshape((dlayer4_3_output.shape[0], dlayer4_3_output.shape[0], 1, 1))\n        for slice in range(conv4_3_kernel.shape[2]): # 512\n            temp1 = layer4_2_output[:, :, slice].reshape((layer4_2_output.shape[0], layer4_2_output.shape[0], 1))\n            dconv4_3_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,512,512)\n    print('done41')\n    \n    for filter in range(conv4_3_kernel.shape[3]): # 512\n        temp1 = dlayer4_3_output[:, :, filter].reshape((dlayer4_3_output.shape[0], dlayer4_3_output.shape[0], 1))\n        for slice in range(conv4_3_kernel.shape[2]): # 512\n            temp2 = conv4_3_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n            dlayer4_2_output += convolve(temp1, temp2)\n    print('done42')\n    \n    # Layer 4 - BP conv2\n    dlayer4_2_output *= der_ReLU(layer4_2_output) # (18,18,512)\n    dconv4_2_bias = dlayer4_2_output # (18,18,512)\n\n     # Looping over filters\n    for filter in range(conv4_2_kernel.shape[3]): # 512\n        temp2 = dlayer4_2_output[:, :, filter].reshape((dlayer4_2_output.shape[0], dlayer4_2_output.shape[0], 1, 1))\n        for slice in range(conv4_2_kernel.shape[2]): # 512\n            temp1 = layer4_1_output[:, :, slice].reshape((layer4_1_output.shape[0], layer4_1_output.shape[0], 1))\n            dconv4_2_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,512,512)\n    print('done41')\n    \n    for filter in range(conv4_2_kernel.shape[3]): # 512\n        temp1 = dlayer4_2_output[:, :, filter].reshape((dlayer4_2_output.shape[0], dlayer4_2_output.shape[0], 1))\n        for slice in range(conv4_2_kernel.shape[2]): # 512\n            temp2 = conv4_2_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n            dlayer4_1_output += convolve(temp1, temp2)\n    print('done42')\n    \n    # Layer 4 - BP conv1\n    dlayer4_1_output *= der_ReLU(layer4_1_output) # (18,18,512)\n    dconv4_1_bias = dlayer4_1_output # (18,18,512)\n\n    # Looping over filters\n    for filter in range(conv4_1_kernel.shape[3]): # 512\n        temp2 = dlayer4_1_output[:, :, filter].reshape((dlayer4_1_output.shape[0], dlayer4_1_output.shape[0], 1, 1))\n        for slice in range(conv4_1_kernel.shape[2]): # 512\n            temp1 = layer3_pool[:, :, slice].reshape((layer3_pool.shape[0], layer3_pool.shape[0], 1))\n            dconv4_1_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,512,512)\n    print('done41')\n    \n    for filter in range(conv4_1_kernel.shape[3]): # 512\n        temp1 = dlayer4_1_output[:, :, filter].reshape((dlayer4_1_output.shape[0], dlayer4_1_output.shape[0], 1))\n        for slice in range(conv4_1_kernel.shape[2]): # 512\n            temp2 = conv4_1_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n            dlayer3_pool += convolve(temp1, temp2)\n    print('done42')\n\n\n    ### Layer 3 ----------------------------------------------------------------------------------------------------------------------------------------\n    \n    # Unpooling - (24, 24, 256) -> (48, 48, 256)\n    for i in range(dlayer3_pool.shape[0]):\n        for j in range(dlayer3_pool.shape[1]): \n            for k in range(dlayer3_pool.shape[2]):\n                # Get the global indices from layer_indices\n                x_index, y_index = layer3_indices[i, j, k] # -> (18, 18, 512, 2)\n                # Assign the gradient from dZ_pool_output to the corresponding position in dZ_pool_input\n                dlayer3_2_output[x_index, y_index, k] = dlayer3_pool[i, j, k]\n    \n    # Layer 3 - Dropout\n    dlayer3_2_output *= dropout3_mask\n\n    # Layer 3 - Batch norm\n    dlayer3_2_output, dgamma3_conv, dbeta3_conv = batch_norm_backward(dlayer3_2_output, layer3_cache)\n\n    # Layer 3 - BP conv2\n    dlayer3_2_output *= der_ReLU(layer3_2_output) # (36,36,256)\n    dconv3_2_bias = dlayer3_2_output # (36,36,256)\n\n\n    # Looping over filters\n    for filter in range(conv3_2_kernel.shape[3]): # 256\n        temp2 = dlayer3_2_output[:, :, filter].reshape((dlayer3_2_output.shape[0], dlayer3_2_output.shape[0], 1, 1))\n        for slice in range(conv3_2_kernel.shape[2]): # 256\n            temp1 = layer3_1_output[:, :, slice].reshape((layer3_1_output.shape[0], layer3_1_output.shape[0], 1))\n            dconv3_2_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,256,256)\n    print('done31')\n\n    for filter in range(conv3_2_kernel.shape[3]): # 256\n        temp1 = dlayer3_2_output[:, :, filter].reshape((dlayer3_2_output.shape[0], dlayer3_2_output.shape[0], 1))\n        for slice in range(conv3_2_kernel.shape[2]): # 256\n            temp2 = conv3_2_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n            dlayer3_1_output += convolve(temp1, temp2)\n    print('done32')\n    \n    # Layer 3 - BP conv1\n    dlayer3_1_output *= der_ReLU(layer3_1_output) # (36,36,256)\n    dconv3_1_bias = dlayer3_1_output # (36,36,256)\n\n    # Looping over filters\n    for filter in range(conv3_1_kernel.shape[3]): # 256\n        temp2 = dlayer3_1_output[:, :, filter].reshape((dlayer3_1_output.shape[0], dlayer3_1_output.shape[0], 1, 1))\n        for slice in range(conv3_1_kernel.shape[2]): # 256\n            temp1 = layer2_pool[:, :, slice].reshape((layer2_pool.shape[0], layer2_pool.shape[0], 1))\n            dconv3_1_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,256,256)\n    print('done31')\n\n    for filter in range(conv3_1_kernel.shape[3]): # 256\n        temp1 = dlayer3_1_output[:, :, filter].reshape((dlayer3_1_output.shape[0], dlayer3_1_output.shape[0], 1))\n        for slice in range(conv3_1_kernel.shape[2]): # 256\n            temp2 = conv3_1_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n            dlayer2_pool += convolve(temp1, temp2)\n    print('done32')\n\n    ### Layer 2 ----------------------------------------------------------------------------------------------------------------------------------------\n    # Unpooling - (24, 24, 256) -> (48, 48, 256)\n    for i in range(dlayer2_pool.shape[0]):\n        for j in range(dlayer2_pool.shape[1]): \n            for k in range(dlayer2_pool.shape[2]):\n                # Get the global indices from layer_indices\n                x_index, y_index = layer2_indices[i, j, k] # -> (18, 18, 512, 2)\n                # Assign the gradient from dZ_pool_output to the corresponding position in dZ_pool_input\n                dlayer2_3_output[x_index, y_index, k] = dlayer2_pool[i, j, k]\n    \n    # Layer 2 - Dropout\n    dlayer2_3_output *= dropout2_mask\n\n    # Layer 2 - Batch norm\n    dlayer2_3_output, dgamma2_conv, dbeta2_conv = batch_norm_backward(dlayer2_3_output, layer2_cache)\n\n    # Layer 2 - BP conv3\n    dlayer2_3_output *= der_ReLU(layer2_3_output) # (72,72,128)\n    dconv2_3_bias = dlayer2_3_output # (72,72,128)\n\n    # Looping over filters\n    for filter in range(conv2_3_kernel.shape[3]): # 128\n        temp2 = dlayer2_3_output[:, :, filter].reshape((dlayer2_3_output.shape[0], dlayer2_3_output.shape[0], 1, 1))\n        for slice in range(conv2_3_kernel.shape[2]): # 128\n            temp1 = layer2_2_output[:, :, slice].reshape((layer2_2_output.shape[0], layer2_2_output.shape[0], 1))\n            dconv2_3_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,128,128)\n    print('done21')\n\n    for filter in range(conv2_3_kernel.shape[3]): # 128\n        temp1 = dlayer2_3_output[:, :, filter].reshape((dlayer2_3_output.shape[0], dlayer2_3_output.shape[0], 1))\n        for slice in range(conv2_3_kernel.shape[2]): # 128\n            temp2 = conv2_3_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n            dlayer2_2_output += convolve(temp1, temp2)\n    print('done22')\n\n    # Layer 2 - BP conv2\n    dlayer2_2_output *= der_ReLU(layer2_2_output) # (72,72,128)\n    dconv2_2_bias = dlayer2_2_output # (72,72,128)\n\n    # Looping over filters\n    for filter in range(conv2_2_kernel.shape[3]): # 128\n        temp2 = dlayer2_2_output[:, :, filter].reshape((dlayer2_2_output.shape[0], dlayer2_2_output.shape[0], 1, 1))\n        for slice in range(conv2_2_kernel.shape[2]): # 128\n            temp1 = layer2_1_output[:, :, slice].reshape((layer2_1_output.shape[0], layer2_1_output.shape[0], 1))\n            dconv2_2_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,128,128)\n    print('done21')\n    \n    \n    for filter in range(conv2_2_kernel.shape[3]): # 128\n        temp1 = dlayer2_2_output[:, :, filter].reshape((dlayer2_2_output.shape[0], dlayer2_2_output.shape[0], 1))\n        for slice in range (conv2_2_kernel.shape[2]): # 512\n            # Broadcast the corresponding kernel to the input depth\n            temp2 = conv2_2_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n            dlayer2_1_output += convolve(temp1, temp2)\n    print('done22')\n\n    # Layer 2 - BP conv1\n    dlayer2_1_output *= der_ReLU(layer2_1_output) # (72,72,128)\n    dconv2_1_bias = dlayer2_1_output # (72,72,128)\n\n    # Looping over filters\n    for filter in range(conv2_1_kernel.shape[3]): # 128\n        temp2 = dlayer2_1_output[:, :, filter].reshape((dlayer2_1_output.shape[0], dlayer2_1_output.shape[0], 1, 1))\n        for slice in range(conv2_1_kernel.shape[2]): # 128\n            temp1 = layer1_pool[:, :, slice].reshape((layer1_pool.shape[0], layer1_pool.shape[0], 1))\n            dconv2_1_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,128,128)\n    print('done21')\n\n    for filter in range(conv2_1_kernel.shape[3]): # 128\n        temp1 = dlayer2_1_output[:, :, filter].reshape((dlayer2_1_output.shape[0], dlayer2_1_output.shape[0], 1))\n        for slice in range(conv2_1_kernel.shape[2]): # 128\n            temp2 = conv2_1_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n            dlayer1_pool += convolve(temp1, temp2)\n    print('done22')\n    \n\n    ### Layer 1 ----------------------------------------------------------------------------------------------------------------------------------------\n    \n    # Unpooling - (110, 110, 64) -> (220, 220, 64)\n    for i in range(dlayer1_pool.shape[0]):\n        for j in range(dlayer1_pool.shape[1]): \n            for k in range(dlayer1_pool.shape[2]):\n                # Get the global indices from layer_indices\n                x_index, y_index = layer1_indices[i, j, k] # -> (18, 18, 512, 2)\n                # Assign the gradient from dZ_pool_output to the corresponding position in dZ_pool_input\n                dlayer1_2_output[x_index, y_index, k] = dlayer1_pool[i, j, k]\n    \n    # Layer 3 - Dropout\n    dlayer1_2_output *= dropout1_mask\n    \n    # Layer 3 - Batch norm\n    dlayer1_2_output, dgamma1_conv, dbeta1_conv = batch_norm_backward(dlayer1_2_output, layer1_cache)\n\n    # Layer 1 - BP conv2\n    dlayer1_2_output *= der_ReLU(layer1_2_output) # (144,144,64)\n    dconv1_2_bias = dlayer1_2_output # (144,144,64)\n\n    # Looping over filters\n    for filter in range(conv1_2_kernel.shape[3]): # 64\n        temp2 = dlayer1_2_output[:, :, filter].reshape((dlayer1_2_output.shape[0], dlayer1_2_output.shape[0], 1, 1))\n        for slice in range(conv1_2_kernel.shape[2]): # 64\n            temp1 = layer1_1_output[:, :, slice].reshape((layer1_1_output.shape[0], layer1_1_output.shape[0], 1))\n            dconv1_2_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,64,64)\n    print('done11')\n\n    for filter in range(conv1_2_kernel.shape[3]): # 64\n        temp1 = dlayer1_2_output[:, :, filter].reshape((dlayer1_2_output.shape[0], dlayer1_2_output.shape[0], 1))\n        for slice in range(conv1_2_kernel.shape[2]): # 64\n            temp2 = conv1_2_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n            dlayer1_1_output += convolve(temp1, temp2)\n    print('done12')\n    \n    # Layer 1 - BP conv1\n    dlayer1_1_output *= der_ReLU(layer1_1_output) # (144,144,64)\n    dconv1_1_bias = dlayer1_1_output # (144,144,64)\n\n    # Looping over filters\n    for filter in range(conv1_1_kernel.shape[3]): # 64\n        temp2 = dlayer1_1_output[:, :, filter].reshape((dlayer1_1_output.shape[0], dlayer1_1_output.shape[0], 1, 1))\n        for slice in range(layer_input.shape[2]): # 3\n            temp1 = layer_input[:, :, slice].reshape((layer_input.shape[0], layer_input.shape[0], 1))\n            dconv1_1_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,3,64)\n    print('done11')\n    \n    print('no backprop into input :)')\n\n    return (dconv1_1_kernel, dconv1_1_bias, dconv1_2_kernel, dconv1_2_bias, dgamma1_conv, dbeta1_conv,\n    dconv2_1_kernel, dconv2_1_bias, dconv2_2_kernel, dconv2_2_bias, dconv2_3_kernel, dconv2_3_bias,\n    dgamma2_conv, dbeta2_conv, dconv3_1_kernel, dconv3_1_bias, dconv3_2_kernel, dconv3_2_bias,\n    dgamma3_conv, dbeta3_conv, dconv4_1_kernel, dconv4_1_bias, dconv4_2_kernel, dconv4_2_bias,\n    dconv4_3_kernel, dconv4_3_bias, dgamma4_conv, dbeta4_conv, dconv5_1_kernel, dconv5_1_bias,\n    dconv5_2_kernel, dconv5_2_bias, dgamma5_conv, dbeta5_conv, dfc1_weights, dfc1_bias, dfc2_weights, dfc2_bias)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:12:49.312234Z","iopub.execute_input":"2024-06-03T04:12:49.312644Z","iopub.status.idle":"2024-06-03T04:12:49.402270Z","shell.execute_reply.started":"2024-06-03T04:12:49.312610Z","shell.execute_reply":"2024-06-03T04:12:49.401225Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def update_params(\n    dconv1_1_kernel, dconv1_1_bias, dconv1_2_kernel, dconv1_2_bias, dgamma1_conv, dbeta1_conv,\n    dconv2_1_kernel, dconv2_1_bias, dconv2_2_kernel, dconv2_2_bias, dconv2_3_kernel, dconv2_3_bias,\n    dgamma2_conv, dbeta2_conv, dconv3_1_kernel, dconv3_1_bias, dconv3_2_kernel, dconv3_2_bias,\n    dgamma3_conv, dbeta3_conv, dconv4_1_kernel, dconv4_1_bias, dconv4_2_kernel, dconv4_2_bias,\n    dconv4_3_kernel, dconv4_3_bias, dgamma4_conv, dbeta4_conv, dconv5_1_kernel, dconv5_1_bias,\n    dconv5_2_kernel, dconv5_2_bias, dgamma5_conv, dbeta5_conv, dfc1_weights, dfc1_bias, dfc2_weights, dfc2_bias,\n    learning_rate\n):\n    conv1_1_kernel -= learning_rate * dconv1_1_kernel\n    conv1_1_bias -= learning_rate * dconv1_1_bias\n    conv1_2_kernel -= learning_rate * dconv1_2_kernel\n    conv1_2_bias -= learning_rate * dconv1_2_bias\n    gamma1_conv -= learning_rate * dgamma1_conv\n    beta1_conv -= learning_rate * dbeta1_conv\n\n    conv2_1_kernel -= learning_rate * dconv2_1_kernel\n    conv2_1_bias -= learning_rate * dconv2_1_bias\n    conv2_2_kernel -= learning_rate * dconv2_2_kernel\n    conv2_2_bias -= learning_rate * dconv2_2_bias\n    conv2_3_kernel -= learning_rate * dconv2_3_kernel\n    conv2_3_bias -= learning_rate * dconv2_3_bias\n    gamma2_conv -= learning_rate * dgamma2_conv\n    beta2_conv -= learning_rate * dbeta2_conv\n\n    conv3_1_kernel -= learning_rate * dconv3_1_kernel\n    conv3_1_bias -= learning_rate * dconv3_1_bias\n    conv3_2_kernel -= learning_rate * dconv3_2_kernel\n    conv3_2_bias -= learning_rate * dconv3_2_bias\n    gamma3_conv -= learning_rate * dgamma3_conv\n    beta3_conv -= learning_rate * dbeta3_conv\n\n    conv4_1_kernel -= learning_rate * dconv4_1_kernel\n    conv4_1_bias -= learning_rate * dconv4_1_bias\n    conv4_2_kernel -= learning_rate * dconv4_2_kernel\n    conv4_2_bias -= learning_rate * dconv4_2_bias\n    conv4_3_kernel -= learning_rate * dconv4_3_kernel\n    conv4_3_bias -= learning_rate * dconv4_3_bias\n    gamma4_conv -= learning_rate * dgamma4_conv\n    beta4_conv -= learning_rate * dbeta4_conv\n\n    conv5_1_kernel -= learning_rate * dconv5_1_kernel\n    conv5_1_bias -= learning_rate * dconv5_1_bias\n    conv5_2_kernel -= learning_rate * dconv5_2_kernel\n    conv5_2_bias -= learning_rate * dconv5_2_bias\n    gamma5_conv -= learning_rate * dgamma5_conv\n    beta5_conv -= learning_rate * dbeta5_conv\n\n    fc1_weights -= learning_rate * dfc1_weights\n    fc1_bias -= learning_rate * dfc1_bias\n    fc2_weights -= learning_rate * dfc2_weights\n    fc2_bias -= learning_rate * dfc2_bias","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:12:54.320939Z","iopub.execute_input":"2024-06-03T04:12:54.321313Z","iopub.status.idle":"2024-06-03T04:12:54.333737Z","shell.execute_reply.started":"2024-06-03T04:12:54.321284Z","shell.execute_reply":"2024-06-03T04:12:54.332468Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"params = {\n    # Layer 1\n    'conv1_1_kernel': conv1_1_kernel,\n    'conv1_1_bias': conv1_1_bias,\n    'conv1_2_kernel': conv1_2_kernel,\n    'conv1_2_bias': conv1_2_bias,\n    'gamma1_conv': gamma1_conv,\n    'beta1_conv': beta1_conv,\n    \n    # Layer 2\n    'conv2_1_kernel': conv2_1_kernel,\n    'conv2_1_bias': conv2_1_bias,\n    'conv2_2_kernel': conv2_2_kernel,\n    'conv2_2_bias': conv2_2_bias,\n    'conv2_3_kernel': conv2_3_kernel,\n    'conv2_3_bias': conv2_3_bias,\n    'gamma2_conv': gamma2_conv,\n    'beta2_conv': beta2_conv,\n\n    # Layer 3\n    'conv3_1_kernel': conv3_1_kernel,\n    'conv3_1_bias': conv3_1_bias,\n    'conv3_2_kernel': conv3_2_kernel,\n    'conv3_2_bias': conv3_2_bias,\n    'gamma3_conv': gamma3_conv,\n    'beta3_conv': beta3_conv,\n\n    # Layer 4\n    'conv4_1_kernel': conv4_1_kernel,\n    'conv4_1_bias': conv4_1_bias,\n    'conv4_2_kernel': conv4_2_kernel,\n    'conv4_2_bias': conv4_2_bias,\n    'conv4_3_kernel': conv4_3_kernel,\n    'conv4_3_bias': conv4_3_bias,\n    'gamma4_conv': gamma4_conv,\n    'beta4_conv': beta4_conv,\n\n    # Layer 5\n    'conv5_1_kernel': conv5_1_kernel,\n    'conv5_1_bias': conv5_1_bias,\n    'conv5_2_kernel': conv5_2_kernel,\n    'conv5_2_bias': conv5_2_bias,\n    'gamma5_conv': gamma5_conv,\n    'beta5_conv': beta5_conv,\n\n    # Layer 6\n    'fc1_weights': fc1_weights,\n    'fc1_bias': fc1_bias,\n    'fc2_weights': fc2_weights,\n    'fc2_bias': fc2_bias\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:13:08.876246Z","iopub.execute_input":"2024-06-03T04:13:08.877123Z","iopub.status.idle":"2024-06-03T04:13:08.885252Z","shell.execute_reply.started":"2024-06-03T04:13:08.877087Z","shell.execute_reply":"2024-06-03T04:13:08.884213Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# TODO: Implement randomization - currently not used in training\ndef stochastic_gradient_descent(train_set, val_set, epochs, learning_rate, batch_size):\n    num_examples = len(train_set)\n    \n    for epoch in range(epochs):\n        print(\"Epoch:\", epoch + 1)\n        \n        for batch_start in range(0, num_examples, batch_size):\n            batch_end = min(batch_start + batch_size, num_examples)\n            \n            # Initialize batch_gradients to accumulate gradients\n            batch_gradients = [\n                cp.zeros_like(conv1_1_kernel), cp.zeros_like(conv1_1_bias),\n                cp.zeros_like(conv1_2_kernel), cp.zeros_like(conv1_2_bias),\n                cp.zeros_like(gamma1_conv), cp.zeros_like(beta1_conv),\n                \n                cp.zeros_like(conv2_1_kernel), cp.zeros_like(conv2_1_bias),\n                cp.zeros_like(conv2_2_kernel), cp.zeros_like(conv2_2_bias),\n                cp.zeros_like(conv2_3_kernel), cp.zeros_like(conv2_3_bias),\n                cp.zeros_like(gamma2_conv), cp.zeros_like(beta2_conv),\n                \n                cp.zeros_like(conv3_1_kernel), cp.zeros_like(conv3_1_bias),\n                cp.zeros_like(conv3_2_kernel), cp.zeros_like(conv3_2_bias),\n                cp.zeros_like(gamma3_conv), cp.zeros_like(beta3_conv),\n                \n                cp.zeros_like(conv4_1_kernel), cp.zeros_like(conv4_1_bias),\n                cp.zeros_like(conv4_2_kernel), cp.zeros_like(conv4_2_bias),\n                cp.zeros_like(conv4_3_kernel), cp.zeros_like(conv4_3_bias),\n                cp.zeros_like(gamma4_conv), cp.zeros_like(beta4_conv),\n                \n                cp.zeros_like(conv5_1_kernel), cp.zeros_like(conv5_1_bias),\n                cp.zeros_like(conv5_2_kernel), cp.zeros_like(conv5_2_bias),\n                cp.zeros_like(gamma5_conv), cp.zeros_like(beta5_conv),\n                \n                cp.zeros_like(fc1_weights), cp.zeros_like(fc1_bias),\n                cp.zeros_like(fc2_weights), cp.zeros_like(fc2_bias)\n            ]\n        \n            for j in range(batch_start, batch_end):\n                # Get a single training example\n                layer_input, label = train_set[j]\n                \n                # Forward propagation\n                layer1_indices, layer1_cache, dropout1_mask, layer2_indices, layer2_cache, dropout2_mask, layer3_indices, layer3_cache, dropout3_mask, layer4_indices, layer4_cache, dropout4_mask, layer5_cache, dropout5_mask = forward_propagation(\n                    layer_input, conv1_1_kernel, conv1_1_bias, layer1_1_output, conv1_2_kernel, conv1_2_bias, layer1_2_output, gamma1_conv, beta1_conv, layer1_pool,\n                    conv2_1_kernel, conv2_1_bias, layer2_1_output, conv2_2_kernel, conv2_2_bias, layer2_2_output, conv2_3_kernel, conv2_3_bias, layer2_3_output, gamma2_conv, beta2_conv, layer2_pool,\n                    conv3_1_kernel, conv3_1_bias, layer3_1_output, conv3_2_kernel, conv3_2_bias, layer3_2_output, gamma3_conv, beta3_conv, layer3_pool,\n                    conv4_1_kernel, conv4_1_bias, layer4_1_output, conv4_2_kernel, conv4_2_bias, layer4_2_output, conv4_3_kernel, conv4_3_bias, layer4_3_output, gamma4_conv, beta4_conv, layer4_pool,\n                    conv5_1_kernel, conv5_1_bias, layer5_1_output, conv5_2_kernel, conv5_2_bias, layer5_2_output, gamma5_conv, beta5_conv, \n                    fc1_weights, fc1_bias, layer6_1_output, fc2_weights, fc2_bias, layer6_2_output, dropout_rate = 0.25\n                )\n                \n                # Back propagation\n                gradients = backward_pass(\n                    layer_input, label, conv1_1_kernel, conv1_1_bias, layer1_1_output, conv1_2_kernel, conv1_2_bias, layer1_2_output, gamma1_conv, beta1_conv, layer1_pool, layer1_indices, layer1_cache, dropout1_mask,\n                    conv2_1_kernel, conv2_1_bias, layer2_1_output, conv2_2_kernel, conv2_2_bias, layer2_2_output, conv2_3_kernel, conv2_3_bias, layer2_3_output, gamma2_conv, beta2_conv, layer2_pool, layer2_indices, layer2_cache, dropout2_mask,\n                    conv3_1_kernel, conv3_1_bias, layer3_1_output, conv3_2_kernel, conv3_2_bias, layer3_2_output, gamma3_conv, beta3_conv, layer3_pool, layer3_indices, layer3_cache, dropout3_mask,\n                    conv4_1_kernel, conv4_1_bias, layer4_1_output, conv4_2_kernel, conv4_2_bias, layer4_2_output, conv4_3_kernel, conv4_3_bias, layer4_3_output, gamma4_conv, beta4_conv, layer4_pool, layer4_indices, layer4_cache, dropout4_mask,\n                    conv5_1_kernel, conv5_1_bias, layer5_1_output, conv5_2_kernel, conv5_2_bias, layer5_2_output, gamma5_conv, beta5_conv, layer5_cache, dropout5_mask,\n                    fc1_weights, fc1_bias, layer6_1_output, fc2_weights, fc2_bias, layer6_2_output\n                )\n                \n                # Accumulate gradients\n                for k in range(len(batch_gradients)):\n                    # Access batch_gradients list, access gradients tuple\n                    batch_gradients[k] += gradients[k]\n            \n            # Average gradients after processing the batch\n            batch_gradients = [grad / batch_size for grad in batch_gradients]\n            \n            # Update parameters after processing the batch\n            update_params(*batch_gradients, learning_rate)\n\n        # Get training accuracy\n        train_accuracy = 0\n        for _ in range(100):\n            random_idx = np.random.randint(len(train_set))\n            test_input, true_label = train_set[random_idx]\n            layer1_indices, layer1_cache, dropout1_mask, layer2_indices, layer2_cache, dropout2_mask, layer3_indices, layer3_cache, dropout3_mask, layer4_indices, layer4_cache, dropout4_mask, layer5_cache, dropout5_mask = forward_propagation(\n                    test_input, conv1_1_kernel, conv1_1_bias, layer1_1_output, conv1_2_kernel, conv1_2_bias, layer1_2_output, gamma1_conv, beta1_conv, layer1_pool,\n                    conv2_1_kernel, conv2_1_bias, layer2_1_output, conv2_2_kernel, conv2_2_bias, layer2_2_output, conv2_3_kernel, conv2_3_bias, layer2_3_output, gamma2_conv, beta2_conv, layer2_pool,\n                    conv3_1_kernel, conv3_1_bias, layer3_1_output, conv3_2_kernel, conv3_2_bias, layer3_2_output, gamma3_conv, beta3_conv, layer3_pool,\n                    conv4_1_kernel, conv4_1_bias, layer4_1_output, conv4_2_kernel, conv4_2_bias, layer4_2_output, conv4_3_kernel, conv4_3_bias, layer4_3_output, gamma4_conv, beta4_conv, layer4_pool,\n                    conv5_1_kernel, conv5_1_bias, layer5_1_output, conv5_2_kernel, conv5_2_bias, layer5_2_output, gamma5_conv, beta5_conv, \n                    fc1_weights, fc1_bias, layer6_1_output, fc2_weights, fc2_bias, layer6_2_output, dropout_rate = 0.25\n            )\n            prediction = get_prediction(layer6_2_output)\n            if np.argmax(true_label) == np.argmax(prediction):\n                train_accuracy += 1\n        print(\"Training Accuracy:\", train_accuracy / 100)\n        \n        # Get validation accuracy\n        val_accuracy = 0\n        for _ in range(100):\n            random_idx = np.random.randint(len(val_set))\n            test_input, true_label = val_set[random_idx]\n            layer1_indices, layer1_cache, dropout1_mask, layer2_indices, layer2_cache, dropout2_mask, layer3_indices, layer3_cache, dropout3_mask, layer4_indices, layer4_cache, dropout4_mask, layer5_cache, dropout5_mask = forward_propagation(\n                    test_input, conv1_1_kernel, conv1_1_bias, layer1_1_output, conv1_2_kernel, conv1_2_bias, layer1_2_output, gamma1_conv, beta1_conv, layer1_pool,\n                    conv2_1_kernel, conv2_1_bias, layer2_1_output, conv2_2_kernel, conv2_2_bias, layer2_2_output, conv2_3_kernel, conv2_3_bias, layer2_3_output, gamma2_conv, beta2_conv, layer2_pool,\n                    conv3_1_kernel, conv3_1_bias, layer3_1_output, conv3_2_kernel, conv3_2_bias, layer3_2_output, gamma3_conv, beta3_conv, layer3_pool,\n                    conv4_1_kernel, conv4_1_bias, layer4_1_output, conv4_2_kernel, conv4_2_bias, layer4_2_output, conv4_3_kernel, conv4_3_bias, layer4_3_output, gamma4_conv, beta4_conv, layer4_pool,\n                    conv5_1_kernel, conv5_1_bias, layer5_1_output, conv5_2_kernel, conv5_2_bias, layer5_2_output, gamma5_conv, beta5_conv, \n                    fc1_weights, fc1_bias, layer6_1_output, fc2_weights, fc2_bias, layer6_2_output, dropout_rate = 0.25\n            )\n            prediction = get_prediction(layer6_2_output)\n            if np.argmax(true_label) == np.argmax(prediction):\n                val_accuracy += 1\n        print(\"Validation Accuracy:\", val_accuracy / 100)\n        \n        # Save the parameters to a .npz file\n        np.savez('model_parameters.npz', **params)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:13:21.945990Z","iopub.execute_input":"2024-06-03T04:13:21.946376Z","iopub.status.idle":"2024-06-03T04:13:21.982035Z","shell.execute_reply.started":"2024-06-03T04:13:21.946345Z","shell.execute_reply":"2024-06-03T04:13:21.980895Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.01\nbatch_size = 2\nepochs = 10\nstochastic_gradient_descent(train_set, val_set, epochs, learning_rate, batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Load model\n# loaded_params = np.load('model_parameters.npz')\n\n# # Extract the parameters\n# conv1_1_kernel = loaded_params['conv1_1_kernel']\n# conv1_1_bias = loaded_params['conv1_1_bias']\n# conv1_2_kernel = loaded_params['conv1_2_kernel']\n# conv1_2_bias = loaded_params['conv1_2_bias']\n# gamma1_conv = loaded_params['gamma1_conv']\n# beta1_conv = loaded_params['beta1_conv']\n\n# conv2_1_kernel = loaded_params['conv2_1_kernel']\n# conv2_1_bias = loaded_params['conv2_1_bias']\n# conv2_2_kernel = loaded_params['conv2_2_kernel']\n# conv2_2_bias = loaded_params['conv2_2_bias']\n# conv2_3_kernel = loaded_params['conv2_3_kernel']\n# conv2_3_bias = loaded_params['conv2_3_bias']\n# gamma2_conv = loaded_params['gamma2_conv']\n# beta2_conv = loaded_params['beta2_conv']\n\n# conv3_1_kernel = loaded_params['conv3_1_kernel']\n# conv3_1_bias = loaded_params['conv3_1_bias']\n# conv3_2_kernel = loaded_params['conv3_2_kernel']\n# conv3_2_bias = loaded_params['conv3_2_bias']\n# gamma3_conv = loaded_params['gamma3_conv']\n# beta3_conv = loaded_params['beta3_conv']\n\n# conv4_1_kernel = loaded_params['conv4_1_kernel']\n# conv4_1_bias = loaded_params['conv4_1_bias']\n# conv4_2_kernel = loaded_params['conv4_2_kernel']\n# conv4_2_bias = loaded_params['conv4_2_bias']\n# conv4_3_kernel = loaded_params['conv4_3_kernel']\n# conv4_3_bias = loaded_params['conv4_3_bias']\n# gamma4_conv = loaded_params['gamma4_conv']\n# beta4_conv = loaded_params['beta4_conv']\n\n# conv5_1_kernel = loaded_params['conv5_1_kernel']\n# conv5_1_bias = loaded_params['conv5_1_bias']\n# conv5_2_kernel = loaded_params['conv5_2_kernel']\n# conv5_2_bias = loaded_params['conv5_2_bias']\n# gamma5_conv = loaded_params['gamma5_conv']\n# beta5_conv = loaded_params['beta5_conv']\n\n# fc1_weights = loaded_params['fc1_weights']\n# fc1_bias = loaded_params['fc1_bias']\n# fc2_weights = loaded_params['fc2_weights']\n# fc2_bias = loaded_params['fc2_bias']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}