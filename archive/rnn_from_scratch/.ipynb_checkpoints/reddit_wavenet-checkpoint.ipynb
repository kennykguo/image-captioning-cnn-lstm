{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b6ca40-e5dc-46a8-af55-d52eff915b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Download NLTK model data (you need to do this once)\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5db966f-27da-4d16-b1a3-494f83403277",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b786b46f-be5f-40bb-b0d7-b2a2f978a063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 646 sentences.\n"
     ]
    }
   ],
   "source": [
    "with open('../data/random.csv', 'r', newline='', encoding='utf-8') as f:\n",
    "    # Initalize a reader object\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    # Skip the header row\n",
    "    # next(reader)  \n",
    "    # Split full comments into sentences  - [nltk.sent_tokenize(x[0].lower()) for x in reader] - for the paragraph x[0] from the csv file, make it lowercase and tokenize all sentence\n",
    "    # For all pararaphs in the csv file. * operator unpacks the list into individual sentences, and creates a single iterable\n",
    "    # sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(str(x).lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    # Replace all sentence x in sentences with the start token, sentence body, and text token\"\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print (f\"Parsed {len(sentences)} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c0fb4a5-c389-48f6-b991-0bda8872e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e0437a29-5dd0-4ad3-b96b-c8de9d8cde71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [[word for word in sentence if word not in {'[', ']', '(', ')'}] for sentence in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3bed2afd-8e2d-41e7-af29-a53ae26dbcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SENTENCE_START', \"'she\", 'tilted', 'her', 'head', 'back', 'and', 'let', 'whip', 'cream', 'stream', 'into', 'her', 'mouth', 'while', 'taking', 'a', 'bath', '.', \"'\", 'SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "# # List of lists\n",
    "print(tokenized_sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0ddc70e7-83ec-4d85-8f10-7aaff86f42de",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}\n",
    "\n",
    "for i in range (len(tokenized_sentences)):\n",
    "    for word1, word2, in zip(tokenized_sentences[i], tokenized_sentences[i][1:]):\n",
    "        # Create a tuple\n",
    "        bigram = (word1, word2)\n",
    "        # Index into the dictionary, update it by one\n",
    "        b[bigram] = b.get(bigram, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2248ad9c-9e51-4662-bedb-fffb10226bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((\"'\", 'SENTENCE_END'), 542),\n",
       " (('.', \"'\"), 528),\n",
       " ((\"'\", ','), 116),\n",
       " (('SENTENCE_START', \"'the\"), 109),\n",
       " ((\"''\", 'SENTENCE_END'), 102),\n",
       " (('SENTENCE_START', '``'), 102),\n",
       " (('.', \"''\"), 97),\n",
       " (('SENTENCE_START', \"'he\"), 70),\n",
       " (('in', 'the'), 50),\n",
       " (('SENTENCE_START', \"'\"), 46),\n",
       " (('SENTENCE_START', \"'she\"), 41),\n",
       " ((\"'\", 'i'), 41),\n",
       " (('of', 'the'), 40),\n",
       " ((',', \"'but\"), 29),\n",
       " (('on', 'the'), 25),\n",
       " (('to', 'be'), 19),\n",
       " (('``', 'it'), 18),\n",
       " (('from', 'the'), 18),\n",
       " (('``', 'i'), 17),\n",
       " (('at', 'the'), 16),\n",
       " ((',', \"'he\"), 16),\n",
       " (('was', 'the'), 16),\n",
       " (('SENTENCE_START', \"'it\"), 15),\n",
       " ((',', '``'), 14),\n",
       " ((\"''\", ','), 14),\n",
       " (('to', 'the'), 14),\n",
       " (('by', 'the'), 14),\n",
       " (('had', 'a'), 13),\n",
       " (('as', 'a'), 13),\n",
       " (('it', '.'), 13),\n",
       " (('he', 'was'), 13),\n",
       " (('for', 'the'), 12),\n",
       " (('it', 'was'), 12),\n",
       " (('in', 'a'), 12),\n",
       " (('SENTENCE_START', \"'there\"), 12),\n",
       " (('did', \"n't\"), 12),\n",
       " (('SENTENCE_START', \"'when\"), 11),\n",
       " (('``', 'he'), 11),\n",
       " ((\"'he\", 'was'), 11),\n",
       " (('SENTENCE_START', \"'they\"), 11),\n",
       " (('when', 'he'), 10),\n",
       " (('it', \"'s\"), 10),\n",
       " (('into', 'the'), 10),\n",
       " ((\"'it\", 'was'), 10),\n",
       " (('on', 'a'), 10),\n",
       " (('``', 'the'), 10),\n",
       " (('was', \"n't\"), 9),\n",
       " (('decided', 'to'), 9),\n",
       " (('way', 'to'), 9),\n",
       " (('i', \"'m\"), 9),\n",
       " (('that', 'the'), 9),\n",
       " (('out', 'of'), 9),\n",
       " (('with', 'a'), 9),\n",
       " (('would', 'be'), 8),\n",
       " (('to', 'do'), 8),\n",
       " (('was', 'a'), 8),\n",
       " (('the', 'beach'), 8),\n",
       " (('do', \"n't\"), 8),\n",
       " (('ca', \"n't\"), 8),\n",
       " (('i', 'was'), 8),\n",
       " (('she', 'could'), 8),\n",
       " (('is', 'the'), 8),\n",
       " ((',', \"'and\"), 8),\n",
       " (('there', \"'s\"), 8),\n",
       " (('want', 'to'), 8),\n",
       " (('SENTENCE_START', \"'if\"), 7),\n",
       " (('of', 'a'), 7),\n",
       " ((\"'s\", 'a'), 7),\n",
       " (('to', 'go'), 7),\n",
       " (('it', 'would'), 7),\n",
       " (('``', 'you'), 7),\n",
       " (('all', 'the'), 7),\n",
       " (('must', 'be'), 7),\n",
       " ((\"'he\", 'had'), 7),\n",
       " ((',', \"'she\"), 7),\n",
       " (('SENTENCE_START', \"'as\"), 7),\n",
       " (('that', 'he'), 7),\n",
       " (('in', 'his'), 7),\n",
       " (('he', 'could'), 7),\n",
       " (('``', 'there'), 7),\n",
       " (('understand', 'why'), 6),\n",
       " (('``', 'but'), 6),\n",
       " ((\"'\", 'a'), 6),\n",
       " (('and', 'a'), 6),\n",
       " (('full', 'of'), 6),\n",
       " (('i', \"'ve\"), 6),\n",
       " (('wanted', 'to'), 6),\n",
       " (('go', 'to'), 6),\n",
       " ((\"'she\", 'was'), 6),\n",
       " (('does', \"n't\"), 6),\n",
       " (('i', 'am'), 6),\n",
       " (('as', 'the'), 6),\n",
       " (('could', 'see'), 6),\n",
       " (('the', 'best'), 6),\n",
       " (('his', 'life'), 6),\n",
       " (('SENTENCE_START', \"'his\"), 6),\n",
       " (('would', 'have'), 6),\n",
       " (('``', 'she'), 6),\n",
       " (('he', 'had'), 6),\n",
       " (('SENTENCE_START', \"'we\"), 6),\n",
       " (('he', 'saw'), 6),\n",
       " (('candy', '.'), 6),\n",
       " (('random', 'sentence'), 5),\n",
       " (('SENTENCE_START', \"'her\"), 5),\n",
       " (('a', 'good'), 5),\n",
       " (('need', 'to'), 5),\n",
       " (('you', \"'re\"), 5),\n",
       " (('he', \"'d\"), 5),\n",
       " (('there', '.'), 5),\n",
       " (('i', 'will'), 5),\n",
       " (('a', 'long'), 5),\n",
       " (('to', 'keep'), 5),\n",
       " (('look', 'at'), 5),\n",
       " (('she', 'was'), 5),\n",
       " (('was', 'to'), 5),\n",
       " (('the', 'same'), 5),\n",
       " (('if', 'she'), 5),\n",
       " (('if', 'you'), 5),\n",
       " (('she', 'had'), 5),\n",
       " (('for', 'a'), 5),\n",
       " (('of', 'his'), 5),\n",
       " (('with', 'the'), 5),\n",
       " (('could', \"n't\"), 5),\n",
       " (('had', 'been'), 5),\n",
       " (('found', 'the'), 5),\n",
       " (('the', 'world'), 5),\n",
       " (('the', 'most'), 5),\n",
       " (('was', 'not'), 4),\n",
       " (('there', 'was'), 4),\n",
       " (('was', 'no'), 4),\n",
       " (('and', 'i'), 4),\n",
       " (('the', 'house'), 4),\n",
       " (('used', 'to'), 4),\n",
       " (('the', 'night'), 4),\n",
       " (('ended', 'up'), 4),\n",
       " (('was', 'her'), 4),\n",
       " (('the', 'first'), 4),\n",
       " (('SENTENCE_START', \"'sometimes\"), 4),\n",
       " (('when', 'you'), 4),\n",
       " (('made', 'the'), 4),\n",
       " (('with', 'her'), 4),\n",
       " (('for', 'your'), 4),\n",
       " (('at', 'what'), 4),\n",
       " (('you', 'do'), 4),\n",
       " ((\"'he\", 'found'), 4),\n",
       " ((',', \"'the\"), 4),\n",
       " (('about', 'the'), 4),\n",
       " (('them', '.'), 4),\n",
       " ((\"'as\", 'he'), 4),\n",
       " (('saw', 'a'), 4),\n",
       " (('is', 'a'), 4),\n",
       " (('like', 'a'), 4),\n",
       " (('to', 'get'), 4),\n",
       " (('that', 'moment'), 4),\n",
       " (('the', 'only'), 4),\n",
       " (('it', 'is'), 4),\n",
       " (('SENTENCE_START', \"'this\"), 4),\n",
       " (('going', 'to'), 4),\n",
       " (('to', 'stop'), 4),\n",
       " (('the', 'sky'), 4),\n",
       " ((',', \"'so\"), 4),\n",
       " (('room', '.'), 4),\n",
       " (('what', 'he'), 4),\n",
       " (('than', 'a'), 4),\n",
       " (('have', 'to'), 4),\n",
       " (('could', 'hear'), 4),\n",
       " (('time', '.'), 4),\n",
       " (('to', 'see'), 4),\n",
       " (('had', 'to'), 4),\n",
       " (('that', 'my'), 4),\n",
       " (('cotton', 'candy'), 4),\n",
       " (('at', 'a'), 4),\n",
       " (('him', 'a'), 4),\n",
       " (('is', 'still'), 4),\n",
       " (('no', 'one'), 4),\n",
       " (('out', '.'), 4),\n",
       " (('SENTENCE_START', \"'everyone\"), 4),\n",
       " (('down', 'the'), 4),\n",
       " ((\"'but\", 'i'), 4),\n",
       " (('SENTENCE_START', \"'my\"), 4),\n",
       " (('under', 'the'), 4),\n",
       " (('the', 'table'), 4),\n",
       " (('why', 'the'), 4),\n",
       " (('that', 'it'), 4),\n",
       " (('SENTENCE_START', \"'you\"), 4),\n",
       " (('they', 'could'), 4),\n",
       " (('but', 'i'), 4),\n",
       " (('are', 'a'), 4),\n",
       " (('see', 'the'), 4),\n",
       " (('be', 'a'), 4),\n",
       " (('and', 'the'), 4),\n",
       " (('was', 'going'), 4),\n",
       " ((',', 'but'), 4),\n",
       " (('a', 'random'), 3),\n",
       " (('part', 'of'), 3),\n",
       " (('peanut', 'butter'), 3),\n",
       " ((\"'he\", 'wondered'), 3),\n",
       " (('if', 'it'), 3),\n",
       " (('could', 'be'), 3),\n",
       " (('SENTENCE_START', \"'nobody\"), 3),\n",
       " (('as', 'he'), 3),\n",
       " (('or', 'a'), 3),\n",
       " (('of', 'life'), 3),\n",
       " (('pudding', '.'), 3),\n",
       " (('i', 'donâ€™t'), 3),\n",
       " (('life', 'in'), 3),\n",
       " (('liked', 'to'), 3),\n",
       " (('to', 'play'), 3),\n",
       " (('didnâ€™t', 'understand'), 3),\n",
       " (('to', 'live'), 3),\n",
       " (('in', 'my'), 3),\n",
       " (('to', 'his'), 3),\n",
       " (('understood', 'that'), 3),\n",
       " ((\"'when\", 'he'), 3),\n",
       " (('thought', 'it'), 3),\n",
       " (('if', 'the'), 3),\n",
       " (('you', 'need'), 3),\n",
       " (('away', 'from'), 3),\n",
       " (('the', 'difference'), 3),\n",
       " (('difference', 'between'), 3),\n",
       " ((\"'but\", 'my'), 3),\n",
       " (('me', '.'), 3),\n",
       " ((\"'he\", 'decided'), 3),\n",
       " (('the', 'local'), 3),\n",
       " (('well', '.'), 3),\n",
       " (('the', 'red'), 3),\n",
       " (('is', 'that'), 3),\n",
       " (('that', 'you'), 3),\n",
       " ((\"'she\", 'did'), 3),\n",
       " ((',', \"'for\"), 3),\n",
       " (('do', '.'), 3),\n",
       " (('was', 'still'), 3),\n",
       " (('what', 'she'), 3),\n",
       " (('the', 'end'), 3),\n",
       " (('the', 'rainbow'), 3),\n",
       " (('the', 'people'), 3),\n",
       " (('about', 'being'), 3),\n",
       " (('you', 'ca'), 3),\n",
       " ((',', \"'yet\"), 3),\n",
       " (('us', '.'), 3),\n",
       " (('on', 'my'), 3),\n",
       " (('seemed', 'to'), 3),\n",
       " (('all', 'of'), 3),\n",
       " (('night', '.'), 3),\n",
       " (('ice', 'cream'), 3),\n",
       " (('fish', '.'), 3),\n",
       " (('out', 'the'), 3),\n",
       " (('the', 'window'), 3),\n",
       " (('me', 'to'), 3),\n",
       " (('long', 'way'), 3),\n",
       " (('trying', 'to'), 3),\n",
       " (('to', 'just'), 3),\n",
       " (('to', 'them'), 3),\n",
       " (('home', '.'), 3),\n",
       " (('head', '.'), 3),\n",
       " (('best', 'way'), 3),\n",
       " (('nails', '.'), 3),\n",
       " ((\"'there\", 'are'), 3),\n",
       " (('are', 'no'), 3),\n",
       " (('the', 'large'), 3),\n",
       " (('is', \"n't\"), 3),\n",
       " (('difficult', 'to'), 3),\n",
       " (('i', 'ca'), 3),\n",
       " (('SENTENCE_START', \"'all\"), 3),\n",
       " ((\"'there\", 'were'), 3),\n",
       " (('able', 'to'), 3),\n",
       " (('pie', '.'), 3),\n",
       " (('had', 'the'), 3),\n",
       " ((',', \"'to\"), 3),\n",
       " (('that', 'there'), 3),\n",
       " ((\"n't\", 'like'), 3),\n",
       " ((\"n't\", 'have'), 3),\n",
       " ((\"'there\", 'was'), 3),\n",
       " (('and', 'he'), 3),\n",
       " (('entered', 'the'), 3),\n",
       " (('hear', 'the'), 3),\n",
       " (('into', 'a'), 3),\n",
       " (('if', 'he'), 3),\n",
       " (('was', 'an'), 3),\n",
       " (('of', 'being'), 3),\n",
       " (('SENTENCE_START', \"'in\"), 3),\n",
       " (('the', 'back'), 3),\n",
       " (('years', 'later'), 3),\n",
       " (('later', \"'\"), 3),\n",
       " (('and', 'was'), 3),\n",
       " ((\"'but\", 'not'), 3),\n",
       " (('to', 'tell'), 3),\n",
       " (('tell', 'the'), 3),\n",
       " (('to', 'what'), 3),\n",
       " (('the', 'air'), 3),\n",
       " (('of', 'cotton'), 3),\n",
       " (('to', 'improve'), 3),\n",
       " (('was', 'actually'), 3),\n",
       " (('him', '.'), 3),\n",
       " (('waiting', 'for'), 3),\n",
       " (('have', 'been'), 3),\n",
       " (('i', 'want'), 3),\n",
       " (('dreamed', 'of'), 3),\n",
       " (('created', 'a'), 3),\n",
       " (('was', 'so'), 3),\n",
       " (('food', '.'), 3),\n",
       " ((\"n't\", 'want'), 3),\n",
       " (('the', 'room'), 3),\n",
       " (('against', 'the'), 3),\n",
       " (('as', 'if'), 3),\n",
       " (('if', 'they'), 3),\n",
       " (('was', 'surprised'), 3),\n",
       " ((\"'she\", 'had'), 3),\n",
       " (('to', 'share'), 3),\n",
       " (('wanted', 'a'), 3),\n",
       " (('what', 'was'), 3),\n",
       " (('the', 'shower'), 3),\n",
       " (('he', 'did'), 3),\n",
       " (('through', 'the'), 3),\n",
       " (('a', 'turtle'), 3),\n",
       " (('a', 'lot'), 3),\n",
       " ((\"'if\", 'you'), 3),\n",
       " (('he', 'decided'), 3),\n",
       " (('until', 'you'), 3),\n",
       " (('high', 'school'), 3),\n",
       " (('the', 'type'), 3),\n",
       " (('type', 'of'), 3),\n",
       " (('over', 'the'), 3),\n",
       " (('a', 'wall'), 3),\n",
       " (('a', 'new'), 3),\n",
       " (('of', 'nails'), 3),\n",
       " (('is', 'not'), 3),\n",
       " (('an', 'interesting'), 3),\n",
       " (('while', 'the'), 3),\n",
       " ((\"'he\", 'would'), 3),\n",
       " (('the', 'rabbit'), 3),\n",
       " (('is', 'no'), 3),\n",
       " (('no', 'longer'), 3),\n",
       " (('house', '.'), 3),\n",
       " (('the', 'sound'), 3),\n",
       " (('sound', 'of'), 3),\n",
       " (('ability', 'to'), 3),\n",
       " (('of', 'your'), 3),\n",
       " (('the', 'sun'), 3),\n",
       " (('the', 'time'), 3),\n",
       " (('dreams', '.'), 3),\n",
       " (('the', 'meal'), 3),\n",
       " (('had', 'ever'), 3),\n",
       " (('SENTENCE_START', \"'for\"), 3),\n",
       " ((\"'we\", 'have'), 3),\n",
       " (('meal', '.'), 3),\n",
       " (('SENTENCE_START', \"'at\"), 3),\n",
       " (('that', 'she'), 3),\n",
       " (('not', 'to'), 3),\n",
       " (('``', 'do'), 3),\n",
       " (('to', 'make'), 3),\n",
       " (('willing', 'to'), 3),\n",
       " (('the', 'way'), 3),\n",
       " (('SENTENCE_START', \"'nancy\"), 3),\n",
       " ((\"n't\", 'read'), 3),\n",
       " ((\"'s\", 'never'), 3),\n",
       " (('never', 'been'), 3),\n",
       " (('to', 'help'), 3),\n",
       " (('SENTENCE_START', \"'after\"), 3),\n",
       " (('more', 'than'), 3),\n",
       " (('were', 'a'), 3),\n",
       " (('you', 'look'), 3),\n",
       " (('her', '.'), 3),\n",
       " (('across', 'the'), 3),\n",
       " (('a', 'great'), 3),\n",
       " (('i', \"didn\\\\'t\"), 3),\n",
       " (('time', 'and'), 3),\n",
       " (('right', 'to'), 3),\n",
       " (('sentence', '.'), 2),\n",
       " ((\"'the\", 'best'), 2),\n",
       " (('her', 'own'), 2),\n",
       " (('hair', 'in'), 2),\n",
       " (('to', 'give'), 2),\n",
       " (('wondered', 'if'), 2),\n",
       " (('a', 'pig'), 2),\n",
       " (('calculator', 'had'), 2),\n",
       " (('a', 'history'), 2),\n",
       " ((',', \"'it\"), 2),\n",
       " (('be', 'more'), 2),\n",
       " (('more', 'embarrassing'), 2),\n",
       " (('embarrassing', 'than'), 2),\n",
       " (('between', 'a'), 2),\n",
       " ((\"'the\", 'old'), 2),\n",
       " ((\"'the\", 'door'), 2),\n",
       " (('SENTENCE_START', \"'three\"), 2),\n",
       " (('experience', '.'), 2),\n",
       " ((\"'she\", 'found'), 2),\n",
       " (('found', 'it'), 2),\n",
       " (('to', 'actually'), 2),\n",
       " (('can', 'be'), 2),\n",
       " (('the', 'depths'), 2),\n",
       " (('depths', 'of'), 2),\n",
       " (('of', 'chocolate'), 2),\n",
       " (('her', 'new'), 2),\n",
       " (('play', 'with'), 2),\n",
       " (('words', 'in'), 2),\n",
       " (('the', 'bathtub'), 2),\n",
       " (('i', 'think'), 2),\n",
       " (('SENTENCE_START', \"'jason\"), 2),\n",
       " (('the', 'garage'), 2),\n",
       " (('i', 'used'), 2),\n",
       " (('live', 'in'), 2),\n",
       " (('neighbor', \"'s\"), 2),\n",
       " (('but', 'the'), 2),\n",
       " (('SENTENCE_START', \"'two\"), 2),\n",
       " (('and', 'all'), 2),\n",
       " (('all', 'his'), 2),\n",
       " (('a', 'purple'), 2),\n",
       " (('the', 'middle'), 2),\n",
       " (('middle', 'of'), 2),\n",
       " (('felt', 'that'), 2),\n",
       " (('the', 'bridge'), 2),\n",
       " (('with', 'his'), 2),\n",
       " (('go', '.'), 2),\n",
       " (('first', 'time'), 2),\n",
       " (('time', \"'\"), 2),\n",
       " (('i', 'would'), 2),\n",
       " (('the', 'sea'), 2),\n",
       " (('i', 'know'), 2),\n",
       " (('.', 'SENTENCE_END'), 2),\n",
       " ((',', \"'all\"), 2),\n",
       " ((\"'all\", 'you'), 2),\n",
       " (('do', 'is'), 2),\n",
       " (('yourself', 'and'), 2),\n",
       " (('all', '.'), 2),\n",
       " (('to', 'use'), 2),\n",
       " (('use', '.'), 2),\n",
       " (('good', 'at'), 2),\n",
       " (('you', 'know'), 2),\n",
       " (('beach', 'as'), 2),\n",
       " (('hobby', '.'), 2),\n",
       " (('made', 'him'), 2),\n",
       " ((\"'he\", 'said'), 2),\n",
       " (('said', 'he'), 2),\n",
       " (('because', 'she'), 2),\n",
       " (('she', 'can'), 2),\n",
       " (('made', 'of'), 2),\n",
       " (('car', \"'\"), 2),\n",
       " ((',', \"'or\"), 2),\n",
       " (('the', 'blue'), 2),\n",
       " (('the', 'piano'), 2),\n",
       " (('did', 'not'), 2),\n",
       " (('the', 'test'), 2),\n",
       " (('the', 'right'), 2),\n",
       " (('embraced', 'his'), 2),\n",
       " (('his', 'new'), 2),\n",
       " (('as', 'an'), 2),\n",
       " (('nothing', 'is'), 2),\n",
       " (('is', 'as'), 2),\n",
       " ((\"'nancy\", 'was'), 2),\n",
       " (('end', 'of'), 2),\n",
       " (('``', 'please'), 2),\n",
       " (('tell', 'me'), 2),\n",
       " (('me', 'you'), 2),\n",
       " (('in', 'your'), 2),\n",
       " (('found', 'his'), 2),\n",
       " (('be', 'the'), 2),\n",
       " (('that', 'does'), 2),\n",
       " ((\"n't\", 'go'), 2),\n",
       " (('paint', '.'), 2),\n",
       " (('people', 'who'), 2),\n",
       " (('it', 'must'), 2),\n",
       " (('happy', '.'), 2),\n",
       " (('where', 'he'), 2),\n",
       " (('phone', '.'), 2),\n",
       " (('i', 'really'), 2),\n",
       " (('really', 'want'), 2),\n",
       " (('the', 'windshield'), 2),\n",
       " (('the', 'darkness'), 2),\n",
       " (('shower', '.'), 2),\n",
       " ((\"'when\", 'i'), 2),\n",
       " (('i', 'had'), 2),\n",
       " (('door', 'slammed'), 2),\n",
       " (('i', 'still'), 2),\n",
       " (('wore', 'his'), 2),\n",
       " (('the', 'high'), 2),\n",
       " (('of', 'her'), 2),\n",
       " (('at', 'night'), 2),\n",
       " (('to', 'look'), 2),\n",
       " ((\"'she\", 'is'), 2),\n",
       " (('and', 'they'), 2),\n",
       " (('he', 'looked'), 2),\n",
       " (('a', 'clown'), 2),\n",
       " (('because', 'people'), 2),\n",
       " (('on', 'her'), 2),\n",
       " (('could', 'make'), 2),\n",
       " ((\"'but\", 'only'), 2),\n",
       " (('here', '.'), 2),\n",
       " (('guy', 'who'), 2),\n",
       " (('i', \"'d\"), 2),\n",
       " (('i', 'could'), 2),\n",
       " (('``', 'they'), 2),\n",
       " (('they', \"'re\"), 2),\n",
       " (('playing', 'the'), 2),\n",
       " (('at', 'that'), 2),\n",
       " (('there', 'are'), 2),\n",
       " (('the', 'body'), 2),\n",
       " (('should', 'never'), 2),\n",
       " (('your', 'favorite'), 2),\n",
       " (('milk', '.'), 2),\n",
       " (('time', 'to'), 2),\n",
       " (('SENTENCE_START', \"'karen\"), 2),\n",
       " (('only', 'way'), 2),\n",
       " (('was', 'getting'), 2),\n",
       " (('back', 'to'), 2),\n",
       " (('a', 'better'), 2),\n",
       " (('mind', '.'), 2),\n",
       " (('``', 'my'), 2),\n",
       " (('lack', 'of'), 2),\n",
       " (('as', 'i'), 2),\n",
       " (('i', 'heard'), 2),\n",
       " (('a', 'loud'), 2),\n",
       " (('SENTENCE_START', \"'greetings\"), 2),\n",
       " ((\"'greetings\", 'from'), 2),\n",
       " (('saw', 'his'), 2),\n",
       " (('%', 'of'), 2),\n",
       " (('same', 'as'), 2),\n",
       " (('one', 'of'), 2),\n",
       " (('your', 'head'), 2),\n",
       " (('out', 'to'), 2),\n",
       " (('to', 'build'), 2),\n",
       " (('lights', 'on'), 2),\n",
       " (('decide', 'if'), 2),\n",
       " (('her', 'nails'), 2),\n",
       " (('?', \"'\"), 2),\n",
       " (('in', 'hopes'), 2),\n",
       " (('hopes', 'of'), 2),\n",
       " (('alive', '.'), 2),\n",
       " ((\"'this\", 'is'), 2),\n",
       " (('will', 'be'), 2),\n",
       " (('do', 'a'), 2),\n",
       " (('on', 'your'), 2),\n",
       " (('time', 'i'), 2),\n",
       " (('!', \"''\"), 2),\n",
       " (('to', 'realize'), 2),\n",
       " (('realize', 'that'), 2),\n",
       " (('had', \"n't\"), 2),\n",
       " ((\"'all\", 'she'), 2),\n",
       " (('she', 'wanted'), 2),\n",
       " (('she', 'would'), 2),\n",
       " (('money', \"'\"), 2),\n",
       " ((\"'so\", 'he'), 2),\n",
       " (('the', 'water'), 2),\n",
       " (('be', 'able'), 2),\n",
       " (('life', '.'), 2),\n",
       " (('really', 'good'), 2),\n",
       " (('of', 'pie'), 2),\n",
       " ((\"'he\", 'learned'), 2),\n",
       " (('learned', 'the'), 2),\n",
       " (('should', 'be'), 2),\n",
       " (('you', 'were'), 2),\n",
       " ((',', \"'just\"), 2),\n",
       " (('life', 'by'), 2),\n",
       " (('enough', '.'), 2),\n",
       " (('until', 'it'), 2),\n",
       " (('it', 'actually'), 2),\n",
       " (('at', 'your'), 2),\n",
       " ((\"'s\", 'not'), 2),\n",
       " (('the', 'street'), 2),\n",
       " ((',', \"'nor\"), 2),\n",
       " (('friend', 'in'), 2),\n",
       " (('the', 'group'), 2),\n",
       " (('SENTENCE_START', \"'nothing\"), 2),\n",
       " (('a', 'pet'), 2),\n",
       " (('are', 'not'), 2),\n",
       " (('all', 'he'), 2),\n",
       " (('SENTENCE_START', \"'even\"), 2),\n",
       " (('the', 'snow'), 2),\n",
       " ((\"'she\", 'felt'), 2),\n",
       " (('to', 'wear'), 2),\n",
       " (('as', 'she'), 2),\n",
       " (('day', '.'), 2),\n",
       " (('top', 'of'), 2),\n",
       " ((\"n't\", 'decide'), 2),\n",
       " (('consisted', 'of'), 2),\n",
       " ((\"'he\", 'realized'), 2),\n",
       " ((\"'the\", 'efficiency'), 2),\n",
       " (('on', 'an'), 2),\n",
       " (('back', 'of'), 2),\n",
       " (('worms', '.'), 2),\n",
       " (('of', 'jello'), 2),\n",
       " (('jello', '.'), 2),\n",
       " (('see', 'what'), 2),\n",
       " (('saw', '.'), 2),\n",
       " (('old', 'enough'), 2),\n",
       " (('enough', 'to'), 2),\n",
       " (('to', 'buy'), 2),\n",
       " (('road', \"'\"), 2),\n",
       " (('saw', 'the'), 2),\n",
       " (('you', 'have'), 2),\n",
       " ((\"'the\", 'fish'), 2),\n",
       " (('to', 'say'), 2),\n",
       " (('say', '.'), 2),\n",
       " (('the', 'car'), 2),\n",
       " (('to', 'hear'), 2),\n",
       " (('hear', 'that'), 2),\n",
       " (('life', 'was'), 2),\n",
       " (('all', 'around'), 2),\n",
       " (('hole', 'in'), 2),\n",
       " (('roof', '.'), 2),\n",
       " (('obsession', '.'), 2),\n",
       " (('to', 'come'), 2),\n",
       " (('along', '.'), 2),\n",
       " ((\"'the\", 'beauty'), 2),\n",
       " (('beauty', 'of'), 2),\n",
       " (('been', 'a'), 2),\n",
       " (('ran', 'from'), 2),\n",
       " (('of', 'milk'), 2),\n",
       " (('forest', '.'), 2),\n",
       " (('SENTENCE_START', \"'with\"), 2),\n",
       " ((\"'he\", 'dreamed'), 2),\n",
       " (('sky', 'is'), 2),\n",
       " (('her', 'favorite'), 2),\n",
       " (('filled', 'with'), 2),\n",
       " (('a', 'bicycle'), 2),\n",
       " (('bicycle', '.'), 2),\n",
       " (('with', 'your'), 2),\n",
       " (('your', 'pet'), 2),\n",
       " (('!', \"'\"), 2),\n",
       " (('whether', 'or'), 2),\n",
       " (('or', 'not'), 2),\n",
       " (('failed', 'to'), 2),\n",
       " (('their', 'first'), 2),\n",
       " (('could', 'think'), 2),\n",
       " (('that', 'they'), 2),\n",
       " (('they', 'were'), 2),\n",
       " (('of', 'july'), 2),\n",
       " (('july', '.'), 2),\n",
       " (('life', 'well'), 2),\n",
       " (('surprised', 'at'), 2),\n",
       " (('he', 'found'), 2),\n",
       " (('baggage', '.'), 2),\n",
       " (('it', 'with'), 2),\n",
       " (('it', 'did'), 2),\n",
       " (('i', 'love'), 2),\n",
       " (('be', '.'), 2),\n",
       " (('the', 'fact'), 2),\n",
       " (('fact', 'that'), 2),\n",
       " ((\"'that\", 'was'), 2),\n",
       " (('came', 'to'), 2),\n",
       " (('to', 'getting'), 2),\n",
       " (('to', 'eat'), 2),\n",
       " (('is', 'to'), 2),\n",
       " (('to', 'put'), 2),\n",
       " (('on', 'it'), 2),\n",
       " (('him', 'in'), 2),\n",
       " (('the', 'news'), 2),\n",
       " ((\"n't\", 'understand'), 2),\n",
       " (('air', 'with'), 2),\n",
       " (('with', 'all'), 2),\n",
       " (('turtle', '.'), 2),\n",
       " (('the', 'ground'), 2),\n",
       " (('didnâ€™t', 'want'), 2),\n",
       " (('be', 'his'), 2),\n",
       " (('his', 'friend'), 2),\n",
       " (('up', '.'), 2),\n",
       " (('diamonds', '.'), 2),\n",
       " (('to', 'accept'), 2),\n",
       " (('his', 'fate'), 2),\n",
       " (('of', 'my'), 2),\n",
       " (('my', 'nickname'), 2),\n",
       " (('was', '.'), 2),\n",
       " ((\"'everyone\", 'was'), 2),\n",
       " ((\"'so\", 'i'), 2),\n",
       " (('alone', '.'), 2),\n",
       " (('of', 'girl'), 2),\n",
       " (('the', 'bathroom'), 2),\n",
       " (('what', 'is'), 2),\n",
       " (('is', 'this'), 2),\n",
       " (('all', 'that'), 2),\n",
       " ((';', 'i'), 2),\n",
       " (('i', 'need'), 2),\n",
       " (('exactly', 'as'), 2),\n",
       " (('expected', '.'), 2),\n",
       " (('a', 'snake'), 2),\n",
       " (('to', 'worry'), 2),\n",
       " (('universe', '.'), 2),\n",
       " (('bed', 'of'), 2),\n",
       " (('is', 'very'), 2),\n",
       " ((\"'but\", 'he'), 2),\n",
       " (('to', 'prove'), 2),\n",
       " (('button', '.'), 2),\n",
       " (('of', 'us'), 2),\n",
       " (('not', 'like'), 2),\n",
       " (('the', 'song'), 2),\n",
       " (('class', '.'), 2),\n",
       " (('so', 'she'), 2),\n",
       " (('the', 'socks'), 2),\n",
       " (('the', 'day'), 2),\n",
       " (('a', 'pink'), 2),\n",
       " (('scarf', '.'), 2),\n",
       " ((\"'ve\", 'been'), 2),\n",
       " (('thunder', 'in'), 2),\n",
       " (('make', 'the'), 2),\n",
       " (('her', 'life'), 2),\n",
       " (('whether', 'it'), 2),\n",
       " (('come', 'from'), 2),\n",
       " (('up', 'at'), 2),\n",
       " (('and', 'saw'), 2),\n",
       " (('people', '.'), 2),\n",
       " (('the', 'sunset'), 2),\n",
       " (('in', 'it'), 2),\n",
       " ((\"'she\", 'saw'), 2),\n",
       " (('to', 'change'), 2),\n",
       " ((',', \"'you\"), 2),\n",
       " (('an', 'excellent'), 2),\n",
       " (('until', 'he'), 2),\n",
       " (('she', 'has'), 2),\n",
       " (('but', 'it'), 2),\n",
       " (('sun', '.'), 2),\n",
       " (('on', 'his'), 2),\n",
       " (('since', 'the'), 2),\n",
       " (('i', 'like'), 2),\n",
       " (('like', 'to'), 2),\n",
       " (('and', 'into'), 2),\n",
       " (('the', 'toilet'), 2),\n",
       " (('in', 'time'), 2),\n",
       " (('known', 'what'), 2),\n",
       " (('ever', 'seen'), 2),\n",
       " (('for', 'my'), 2),\n",
       " (('have', 'a'), 2),\n",
       " (('lot', 'of'), 2),\n",
       " (('SENTENCE_START', \"'on\"), 2),\n",
       " (('to', 'turn'), 2),\n",
       " (('ate', 'the'), 2),\n",
       " (('the', 'closet'), 2),\n",
       " (('closet', '.'), 2),\n",
       " (('feet', 'from'), 2),\n",
       " (('that', \"'s\"), 2),\n",
       " (('where', 'the'), 2),\n",
       " (('are', '.'), 2),\n",
       " ((\"'at\", 'that'), 2),\n",
       " (('she', 'realized'), 2),\n",
       " (('a', 'sixth'), 2),\n",
       " (('sixth', 'sense'), 2),\n",
       " ((\"'as\", 'the'), 2),\n",
       " (('more', 'like'), 2),\n",
       " (('good', 'idea'), 2),\n",
       " (('idea', '.'), 2),\n",
       " (('that', 'i'), 2),\n",
       " (('i', 'do'), 2),\n",
       " (('waited', 'for'), 2),\n",
       " (('to', 'a'), 2),\n",
       " ((\"'it\", 'took'), 2),\n",
       " (('took', 'him'), 2),\n",
       " (('had', 'her'), 2),\n",
       " (('that', 'made'), 2),\n",
       " (('voice', '.'), 2),\n",
       " (('chalkboard', '.'), 2),\n",
       " (('SENTENCE_START', \"'people\"), 2),\n",
       " (('person', 'who'), 2),\n",
       " (('love', '.'), 2),\n",
       " (('SENTENCE_START', \"'iâ€™m\"), 2),\n",
       " (('near-death', 'experience'), 2),\n",
       " (('the', 'dog'), 2),\n",
       " (('what', \"'s\"), 2),\n",
       " (('lemonade', '.'), 2),\n",
       " (('i', 'did'), 2),\n",
       " (('cats', 'and'), 2),\n",
       " (('dog', '.'), 2),\n",
       " (('though', 'he'), 2),\n",
       " (('thought', 'the'), 2),\n",
       " (('the', 'irony'), 2),\n",
       " (('irony', 'of'), 2),\n",
       " (('around', 'the'), 2),\n",
       " (('world', '.'), 2),\n",
       " ((\"'she\", 'wanted'), 2),\n",
       " (('because', 'i'), 2),\n",
       " ((\"'m\", 'a'), 2),\n",
       " (('``', 'that'), 2),\n",
       " (('turned', 'out'), 2),\n",
       " (('was', 'willing'), 2),\n",
       " (('longer', 'just'), 2),\n",
       " (('one', 'day'), 2),\n",
       " (('SENTENCE_START', \"'despite\"), 2),\n",
       " ((\"'there\", 'is'), 2),\n",
       " (('lined', 'the'), 2),\n",
       " (('behind', 'the'), 2),\n",
       " (('read', '.'), 2),\n",
       " (('a', 'bad'), 2),\n",
       " (('he', \"'s\"), 2),\n",
       " (('in', 'front'), 2),\n",
       " (('front', 'of'), 2),\n",
       " (('create', 'a'), 2),\n",
       " (('her', 'best'), 2),\n",
       " (('moment', 'i'), 2),\n",
       " (('when', 'i'), 2),\n",
       " (('wondered', 'what'), 2),\n",
       " (('the', 'book'), 2),\n",
       " (('in', 'or'), 2),\n",
       " (('even', 'the'), 2),\n",
       " ((\"'s\", 'head'), 2),\n",
       " ((\"n't\", 'make'), 2),\n",
       " ((\"'he\", 'is'), 2),\n",
       " (('the', 'road'), 2),\n",
       " (('the', 'school'), 2),\n",
       " (('grow', '.'), 2),\n",
       " (('have', 'never'), 2),\n",
       " ((\"'the\", 'father'), 2),\n",
       " (('caused', 'the'), 2),\n",
       " (('the', 'entire'), 2),\n",
       " (('from', 'him'), 2),\n",
       " (('coke', '.'), 2),\n",
       " (('SENTENCE_START', \"'patricia\"), 2),\n",
       " (('walked', 'into'), 2),\n",
       " (('his', 'head'), 2),\n",
       " (('that', 'her'), 2),\n",
       " (('any', 'of'), 2),\n",
       " ((\"'the\", 'secret'), 2),\n",
       " (('brought', 'back'), 2),\n",
       " (('memories', 'of'), 2),\n",
       " (('your', 'brain'), 2),\n",
       " (('worth', 'doing'), 2),\n",
       " (('was', 'disappointed'), 2),\n",
       " (('him', 'to'), 2),\n",
       " (('the', 'museum'), 2),\n",
       " (('would', 'ever'), 2),\n",
       " (('was', 'too'), 2),\n",
       " (('make', 'a'), 2),\n",
       " (('by', 'a'), 2),\n",
       " ((',', 'she'), 2),\n",
       " (('a', 'different'), 2),\n",
       " (('the', 'sugar'), 2),\n",
       " (('the', 'fence'), 2),\n",
       " (('edge', 'of'), 2),\n",
       " (('the', 'deep'), 2),\n",
       " (('you', 'are'), 2),\n",
       " (('the', 'truth'), 2),\n",
       " (('give', 'you'), 2),\n",
       " (('something', 'new'), 2),\n",
       " ((',', 'so'), 2),\n",
       " (('think', 'about'), 2),\n",
       " (('should', 'have'), 2),\n",
       " (('to', 'work'), 2),\n",
       " (('to', 'find'), 2),\n",
       " (('the', 'wrong'), 2),\n",
       " (('keep', 'things'), 2),\n",
       " (('hopes', 'and'), 2),\n",
       " (('and', 'dreams'), 2),\n",
       " (('a', 'few'), 2),\n",
       " (('at', 'least'), 2),\n",
       " (('the', 'neighborhood'), 2),\n",
       " (('neighborhood', '.'), 2),\n",
       " (('this', 'was'), 2),\n",
       " ((\"n't\", 'it'), 2),\n",
       " (('farm', '.'), 2),\n",
       " ((\"'you\", 'have'), 2),\n",
       " ((\"'she\", 'still'), 2),\n",
       " ((',', \"'\"), 2),\n",
       " (('SENTENCE_START', \"'joe\"), 2),\n",
       " (('book', 'is'), 2),\n",
       " (('SENTENCE_START', \"'dan\"), 2),\n",
       " (('rabbit', 'hole'), 2),\n",
       " (('moment', '.'), 2),\n",
       " (('to', 'drive'), 2),\n",
       " (('what', 'i'), 2),\n",
       " (('of', 'random'), 2),\n",
       " (('discovered', 'that'), 2),\n",
       " ((\"'the\", 'random'), 1),\n",
       " (('sentence', 'generator'), 1),\n",
       " (('generator', 'generated'), 1),\n",
       " (('generated', 'a'), 1),\n",
       " (('sentence', 'about'), 1),\n",
       " (('about', 'a'), 1),\n",
       " (('best', 'part'), 1),\n",
       " (('of', 'marriage'), 1),\n",
       " (('marriage', 'is'), 1),\n",
       " (('is', 'animal'), 1),\n",
       " (('animal', 'crackers'), 1),\n",
       " (('crackers', 'with'), 1),\n",
       " (('with', 'peanut'), 1),\n",
       " (('butter', '.'), 1),\n",
       " ((\"'the\", 'waitress'), 1),\n",
       " (('waitress', 'was'), 1),\n",
       " (('not', 'amused'), 1),\n",
       " (('amused', 'when'), 1),\n",
       " (('he', 'ordered'), 1),\n",
       " (('ordered', 'green'), 1),\n",
       " (('green', 'eggs'), 1),\n",
       " (('eggs', 'and'), 1),\n",
       " (('and', 'ham'), 1),\n",
       " (('ham', '.'), 1),\n",
       " ((\"'she\", 'used'), 1),\n",
       " (('used', 'her'), 1),\n",
       " (('own', 'hair'), 1),\n",
       " (('the', 'soup'), 1),\n",
       " (('soup', 'to'), 1),\n",
       " (('give', 'it'), 1),\n",
       " (('it', 'more'), 1),\n",
       " (('more', 'flavor'), 1),\n",
       " (('flavor', '.'), 1),\n",
       " (('it', 'could'), 1),\n",
       " (('be', 'called'), 1),\n",
       " (('called', 'a'), 1),\n",
       " (('a', 'beach'), 1),\n",
       " (('beach', 'if'), 1),\n",
       " (('if', 'there'), 1),\n",
       " (('no', 'sand'), 1),\n",
       " (('sand', '.'), 1),\n",
       " ((\"'nobody\", 'loves'), 1),\n",
       " (('loves', 'a'), 1),\n",
       " (('pig', 'wearing'), 1),\n",
       " (('wearing', 'lipstick'), 1),\n",
       " (('lipstick', '.'), 1),\n",
       " ((\"'if\", 'my'), 1),\n",
       " (('my', 'calculator'), 1),\n",
       " (('history', \"'\"), 1),\n",
       " ((\"'it\", 'would'), 1),\n",
       " (('than', 'my'), 1),\n",
       " (('my', 'browser'), 1),\n",
       " (('browser', 'history'), 1),\n",
       " (('history', '.'), 1),\n",
       " ((\"'the\", 'opportunity'), 1),\n",
       " (('opportunity', 'of'), 1),\n",
       " (('a', 'lifetime'), 1),\n",
       " (('lifetime', 'passed'), 1),\n",
       " (('passed', 'before'), 1),\n",
       " (('before', 'him'), 1),\n",
       " (('him', 'as'), 1),\n",
       " (('he', 'tried'), 1),\n",
       " (('tried', 'to'), 1),\n",
       " (('to', 'decide'), 1),\n",
       " (('decide', 'between'), 1),\n",
       " (('a', 'cone'), 1),\n",
       " (('cone', 'or'), 1),\n",
       " (('a', 'cup'), 1),\n",
       " (('cup', '.'), 1),\n",
       " ((\"'the\", 'changing'), 1),\n",
       " (('changing', 'of'), 1),\n",
       " (('of', 'down'), 1),\n",
       " (('down', 'comforters'), 1),\n",
       " (('comforters', 'to'), 1),\n",
       " (('to', 'cotton'), 1),\n",
       " (('cotton', 'bedspreads'), 1),\n",
       " (('bedspreads', 'always'), 1),\n",
       " (('always', 'meant'), 1),\n",
       " (('meant', 'the'), 1),\n",
       " (('the', 'squirrels'), 1),\n",
       " (('squirrels', 'had'), 1),\n",
       " (('had', 'returned'), 1),\n",
       " (('returned', '.'), 1),\n",
       " (('old', 'apple'), 1),\n",
       " (('apple', 'revels'), 1),\n",
       " (('revels', 'in'), 1),\n",
       " (('in', 'its'), 1),\n",
       " (('its', 'authority'), 1),\n",
       " (('authority', '.'), 1),\n",
       " ((\"'she\", 'tilted'), 1),\n",
       " (('tilted', 'her'), 1),\n",
       " (('her', 'head'), 1),\n",
       " (('head', 'back'), 1),\n",
       " (('back', 'and'), 1),\n",
       " (('and', 'let'), 1),\n",
       " (('let', 'whip'), 1),\n",
       " (('whip', 'cream'), 1),\n",
       " (('cream', 'stream'), 1),\n",
       " (('stream', 'into'), 1),\n",
       " (('into', 'her'), 1),\n",
       " (('her', 'mouth'), 1),\n",
       " (('mouth', 'while'), 1),\n",
       " (('while', 'taking'), 1),\n",
       " (('taking', 'a'), 1),\n",
       " (('a', 'bath'), 1),\n",
       " (('bath', '.'), 1),\n",
       " (('door', 'swung'), 1),\n",
       " (('swung', 'open'), 1),\n",
       " (('open', 'to'), 1),\n",
       " (('to', 'reveal'), 1),\n",
       " (('reveal', 'pink'), 1),\n",
       " (('pink', 'giraffes'), 1),\n",
       " (('giraffes', 'and'), 1),\n",
       " (('and', 'red'), 1),\n",
       " (('red', 'elephants'), 1),\n",
       " (('elephants', '.'), 1),\n",
       " ((\"'three\", 'generations'), 1),\n",
       " (('generations', 'with'), 1),\n",
       " (('with', 'six'), 1),\n",
       " (('six', 'decades'), 1),\n",
       " (('decades', 'of'), 1),\n",
       " (('life', 'experience'), 1),\n",
       " ((\"'he\", 'uses'), 1),\n",
       " (('uses', 'onomatopoeia'), 1),\n",
       " (('onomatopoeia', 'as'), 1),\n",
       " (('a', 'weapon'), 1),\n",
       " (('weapon', 'of'), 1),\n",
       " (('of', 'mental'), 1),\n",
       " (('mental', 'destruction'), 1),\n",
       " (('destruction', '.'), 1),\n",
       " (('it', 'strange'), 1),\n",
       " (('strange', 'that'), 1),\n",
       " (('that', 'people'), 1),\n",
       " (('people', 'use'), 1),\n",
       " (('use', 'their'), 1),\n",
       " (('their', 'cellphones'), 1),\n",
       " (('cellphones', 'to'), 1),\n",
       " (('actually', 'talk'), 1),\n",
       " (('talk', 'to'), 1),\n",
       " (('to', 'one'), 1),\n",
       " (('one', 'another'), 1),\n",
       " (('another', '.'), 1),\n",
       " ...]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking the first 20,000 entries as our vocabulary\n",
    "sorted(b.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aa7fb166-6300-4120-996d-ba081ab64b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set(word for bigram, count in sorted(b.items(), key=lambda kv: -kv[1])[:20000] for word in bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c4d6b40-c4fc-413d-b3c7-904dc19f0bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2764"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = len(word_set)\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9379c3b2-fe2b-4109-b87e-3c5a0aa84236",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'car\": 0,\n",
       " 'hot': 1,\n",
       " 'barbed': 2,\n",
       " 'daily': 3,\n",
       " 'mountains': 4,\n",
       " 'game': 5,\n",
       " 'into': 6,\n",
       " 'mere': 7,\n",
       " 'soul': 8,\n",
       " 'gnome': 9,\n",
       " 'mum': 10,\n",
       " \"'iguanas\": 11,\n",
       " \"'plans\": 12,\n",
       " 'debate': 13,\n",
       " 'raccoon': 14,\n",
       " 'complete': 15,\n",
       " '500': 16,\n",
       " 'looks': 17,\n",
       " 'grumbling': 18,\n",
       " 'dangerous': 19,\n",
       " 'glaze': 20,\n",
       " 'makes': 21,\n",
       " 'coffee': 22,\n",
       " 'cook': 23,\n",
       " 'surgical': 24,\n",
       " 'birds': 25,\n",
       " 'goodbye': 26,\n",
       " 'pesticides': 27,\n",
       " 'pressed': 28,\n",
       " 'useless': 29,\n",
       " 'flat': 30,\n",
       " 'exchanged': 31,\n",
       " 'powerful': 32,\n",
       " 'couldnâ€™t': 33,\n",
       " 'bowling': 34,\n",
       " 'seemed': 35,\n",
       " 'orchard': 36,\n",
       " 'icon': 37,\n",
       " 'gnu': 38,\n",
       " 'platypus': 39,\n",
       " 'songs': 40,\n",
       " 'stairway': 41,\n",
       " 'dancing': 42,\n",
       " 'easily': 43,\n",
       " \"'check\": 44,\n",
       " 'closet': 45,\n",
       " 'the': 46,\n",
       " 'vacant': 47,\n",
       " 'why': 48,\n",
       " 'level': 49,\n",
       " 'rather': 50,\n",
       " 'manuals': 51,\n",
       " 'sister': 52,\n",
       " 'hacked': 53,\n",
       " 'chosen': 54,\n",
       " 'popping': 55,\n",
       " 'pass': 56,\n",
       " 'explaining': 57,\n",
       " 'it': 58,\n",
       " 'come': 59,\n",
       " 'nair': 60,\n",
       " 'race': 61,\n",
       " 'indicated': 62,\n",
       " 'monday': 63,\n",
       " 'lady': 64,\n",
       " 'grew': 65,\n",
       " 'entered': 66,\n",
       " 'useful': 67,\n",
       " 'weaving': 68,\n",
       " 'sparkly': 69,\n",
       " 'quite': 70,\n",
       " 'rowdy': 71,\n",
       " 'candy': 72,\n",
       " 'overnight': 73,\n",
       " 'ate': 74,\n",
       " 'know': 75,\n",
       " 'comes': 76,\n",
       " 'crickets': 77,\n",
       " 'named': 78,\n",
       " 'sad': 79,\n",
       " 'freaked': 80,\n",
       " 'responsibility': 81,\n",
       " \"'each\": 82,\n",
       " \"'he\": 83,\n",
       " 'tumbleweed': 84,\n",
       " 'mr.': 85,\n",
       " 'talking': 86,\n",
       " 'fly': 87,\n",
       " 'exploring': 88,\n",
       " 'with': 89,\n",
       " 'wedding.\\\\ntoday': 90,\n",
       " 'generally': 91,\n",
       " 'labor': 92,\n",
       " 'conditioner': 93,\n",
       " 'beach.\\\\ngary': 94,\n",
       " 'bees': 95,\n",
       " 'brown': 96,\n",
       " 'coherent': 97,\n",
       " 'donuts': 98,\n",
       " \"'pink\": 99,\n",
       " 'arrived': 100,\n",
       " 'concern': 101,\n",
       " 'handful': 102,\n",
       " 'across': 103,\n",
       " 'authority': 104,\n",
       " \"'shakespeare\": 105,\n",
       " 'friends': 106,\n",
       " 'memorable': 107,\n",
       " 'evil': 108,\n",
       " 'ideas': 109,\n",
       " 'through': 110,\n",
       " 'recorded': 111,\n",
       " 'confused.\\\\ntoo': 112,\n",
       " 'craziness': 113,\n",
       " 'ever': 114,\n",
       " 'nails': 115,\n",
       " 'henry': 116,\n",
       " 'rental': 117,\n",
       " 'cubed': 118,\n",
       " 'store': 119,\n",
       " \"'before\": 120,\n",
       " 'without': 121,\n",
       " \"'ll\": 122,\n",
       " 'toes': 123,\n",
       " 'storm': 124,\n",
       " '216th': 125,\n",
       " 'bait': 126,\n",
       " \"'habitat\": 127,\n",
       " 'band': 128,\n",
       " 'kansas': 129,\n",
       " 'diesel': 130,\n",
       " 'ear': 131,\n",
       " 'working': 132,\n",
       " 'roasting': 133,\n",
       " 'amount': 134,\n",
       " 'sound': 135,\n",
       " 'green': 136,\n",
       " 'jellyfish': 137,\n",
       " 'face': 138,\n",
       " 'teens': 139,\n",
       " \"'acres\": 140,\n",
       " 'barefoot': 141,\n",
       " 'heaven': 142,\n",
       " 'strawberries': 143,\n",
       " 'quick': 144,\n",
       " 'marshmallow': 145,\n",
       " 'toy': 146,\n",
       " 'spotted': 147,\n",
       " 'neatly': 148,\n",
       " 'hike': 149,\n",
       " 'jungle': 150,\n",
       " 'buoys': 151,\n",
       " 'hobby': 152,\n",
       " 'our': 153,\n",
       " 'swamp': 154,\n",
       " 'shop': 155,\n",
       " 'rotary': 156,\n",
       " \"'we\": 157,\n",
       " 'sunglasses': 158,\n",
       " 'figured': 159,\n",
       " 'thirst': 160,\n",
       " 'research': 161,\n",
       " 'distance.\\\\nshe': 162,\n",
       " 'sight': 163,\n",
       " 'cars': 164,\n",
       " 'along': 165,\n",
       " 'school': 166,\n",
       " 'snake': 167,\n",
       " 'erect': 168,\n",
       " \"'25\": 169,\n",
       " 'wipes': 170,\n",
       " 'garage': 171,\n",
       " 'west': 172,\n",
       " 'more': 173,\n",
       " 'depended': 174,\n",
       " \"'mary\": 175,\n",
       " 'arm': 176,\n",
       " 'cleaning': 177,\n",
       " 'clock': 178,\n",
       " 'air': 179,\n",
       " 'color': 180,\n",
       " 'crashed': 181,\n",
       " 'quiet': 182,\n",
       " 'can': 183,\n",
       " 'group': 184,\n",
       " 'written': 185,\n",
       " 'thrilled': 186,\n",
       " 'boy': 187,\n",
       " 'fasting': 188,\n",
       " 'cigarettes': 189,\n",
       " 'clean': 190,\n",
       " 'ride': 191,\n",
       " 'urgent': 192,\n",
       " 'ghosts': 193,\n",
       " 'seek': 194,\n",
       " 'crevasse': 195,\n",
       " 'tanned': 196,\n",
       " \"'david\": 197,\n",
       " 'nancy': 198,\n",
       " 'harming': 199,\n",
       " 'consisted': 200,\n",
       " 'specific': 201,\n",
       " 'toothpicks': 202,\n",
       " \"'garlic\": 203,\n",
       " 'i\\\\': 204,\n",
       " 'crashing': 205,\n",
       " 'upstairs': 206,\n",
       " 'tired.\\\\nthe': 207,\n",
       " 'coyotes': 208,\n",
       " 'moon': 209,\n",
       " 'pretty': 210,\n",
       " 'stuff': 211,\n",
       " 'facing': 212,\n",
       " 'commands': 213,\n",
       " 'crazy': 214,\n",
       " 'garden': 215,\n",
       " 'except': 216,\n",
       " 'windows': 217,\n",
       " 'weapons': 218,\n",
       " 'for': 219,\n",
       " \"'this\": 220,\n",
       " 'standing': 221,\n",
       " 'confirmed': 222,\n",
       " 'instructions': 223,\n",
       " 'wear': 224,\n",
       " 'john': 225,\n",
       " 'drummer': 226,\n",
       " 'cheese': 227,\n",
       " \"isn\\\\'t\": 228,\n",
       " 'free': 229,\n",
       " 'prospects': 230,\n",
       " 'lived': 231,\n",
       " \"'of\": 232,\n",
       " \"'iâ€™m\": 233,\n",
       " 'macs0647-jd': 234,\n",
       " 'cost': 235,\n",
       " 'rainstorm': 236,\n",
       " 'places': 237,\n",
       " 'cotton': 238,\n",
       " 'came': 239,\n",
       " \"'always\": 240,\n",
       " 'fun.\\\\nhonestly': 241,\n",
       " 'industrial': 242,\n",
       " 'front': 243,\n",
       " 'mid-sent': 244,\n",
       " 'finish': 245,\n",
       " 'unpredictable': 246,\n",
       " \"'but\": 247,\n",
       " 'slot': 248,\n",
       " 'difference.\\\\nhe': 249,\n",
       " \"'anything\": 250,\n",
       " 'lazy': 251,\n",
       " 'risk': 252,\n",
       " 'pool': 253,\n",
       " 'throw': 254,\n",
       " 'attendant': 255,\n",
       " 'youâ€™re': 256,\n",
       " 'coal': 257,\n",
       " 'paired': 258,\n",
       " 'stole': 259,\n",
       " 'murder': 260,\n",
       " 'packed': 261,\n",
       " 'strange': 262,\n",
       " 'lot.\\\\ngiving': 263,\n",
       " 'simple': 264,\n",
       " 'convinced': 265,\n",
       " 'guinea': 266,\n",
       " 'delivered': 267,\n",
       " 'course': 268,\n",
       " 'trip': 269,\n",
       " 'spirit': 270,\n",
       " 'dark': 271,\n",
       " 'put': 272,\n",
       " 'bedspreads': 273,\n",
       " 'plane': 274,\n",
       " 'sphered': 275,\n",
       " 'list': 276,\n",
       " 'sunburnt': 277,\n",
       " 'sandy': 278,\n",
       " \"'angled\": 279,\n",
       " 'kept': 280,\n",
       " 'hiked': 281,\n",
       " 'beef': 282,\n",
       " 'fourth': 283,\n",
       " 'deadly': 284,\n",
       " \"'erin\": 285,\n",
       " 'burns': 286,\n",
       " 'days.\\\\nhe': 287,\n",
       " 'swirled': 288,\n",
       " 'door.\\\\nhe': 289,\n",
       " 'outsmarted': 290,\n",
       " 'river': 291,\n",
       " 'today': 292,\n",
       " 'under': 293,\n",
       " 'warm': 294,\n",
       " 'view': 295,\n",
       " 'unique': 296,\n",
       " 'designer': 297,\n",
       " 'whiskey': 298,\n",
       " 'second.\\\\nthe': 299,\n",
       " 'told': 300,\n",
       " 'thursday': 301,\n",
       " 'motto': 302,\n",
       " 'susan': 303,\n",
       " 'banana': 304,\n",
       " 'head': 305,\n",
       " 'busy': 306,\n",
       " 'daughters': 307,\n",
       " 'lasting': 308,\n",
       " 'vividly': 309,\n",
       " 'beginning.\\\\nwhile': 310,\n",
       " 'students': 311,\n",
       " 'sped': 312,\n",
       " 'armadillo': 313,\n",
       " 'hospital': 314,\n",
       " \"'wisdom\": 315,\n",
       " \"'jerry\": 316,\n",
       " 'recite': 317,\n",
       " 'book': 318,\n",
       " 'hazard': 319,\n",
       " 'streets.\\\\nhe': 320,\n",
       " 'hawk': 321,\n",
       " 'fishermanâ€™s': 322,\n",
       " 'punk': 323,\n",
       " 'ruin': 324,\n",
       " 'shed': 325,\n",
       " 'cake': 326,\n",
       " 'realise': 327,\n",
       " \"'so\": 328,\n",
       " 'hate': 329,\n",
       " 'neighbor': 330,\n",
       " 'hand': 331,\n",
       " 'choir': 332,\n",
       " 'held': 333,\n",
       " 'artist': 334,\n",
       " 'might': 335,\n",
       " 'forward': 336,\n",
       " 'eight': 337,\n",
       " 'own': 338,\n",
       " 'cell': 339,\n",
       " 'asteroid': 340,\n",
       " 'movie': 341,\n",
       " 'convertible': 342,\n",
       " 'show': 343,\n",
       " 'firm': 344,\n",
       " \"'instead\": 345,\n",
       " 'instruction': 346,\n",
       " 'jumped': 347,\n",
       " 'rvs': 348,\n",
       " 'quickly': 349,\n",
       " 'piercing': 350,\n",
       " 'warning': 351,\n",
       " 'whether': 352,\n",
       " 'vines': 353,\n",
       " 'seasoned': 354,\n",
       " 'rotten.\\\\nblue': 355,\n",
       " 'waving': 356,\n",
       " 'judgment': 357,\n",
       " 'loops': 358,\n",
       " 'armor': 359,\n",
       " 'rhubarb': 360,\n",
       " 'lights': 361,\n",
       " 'tell': 362,\n",
       " 'says': 363,\n",
       " 'times': 364,\n",
       " 'sleep': 365,\n",
       " 'test': 366,\n",
       " 'invited': 367,\n",
       " 'insist': 368,\n",
       " 'content': 369,\n",
       " 'tortoise': 370,\n",
       " 'sufficient': 371,\n",
       " 'facemasks': 372,\n",
       " 'nicely': 373,\n",
       " 'washing': 374,\n",
       " 'after': 375,\n",
       " 'fell': 376,\n",
       " 'impressed': 377,\n",
       " 'otherwise': 378,\n",
       " 'tomatoes': 379,\n",
       " \"'let\": 380,\n",
       " 'tophat': 381,\n",
       " 'eyeing': 382,\n",
       " 'external': 383,\n",
       " \"'never\": 384,\n",
       " 'wheat': 385,\n",
       " 'grow': 386,\n",
       " 'enthralled': 387,\n",
       " \"n't\": 388,\n",
       " 'wield': 389,\n",
       " 'nuts': 390,\n",
       " \"'some\": 391,\n",
       " 'peach': 392,\n",
       " 'ponytail.\\\\nshe': 393,\n",
       " 'destruction': 394,\n",
       " 'novels': 395,\n",
       " \"'cursive\": 396,\n",
       " 'system': 397,\n",
       " 'compliments': 398,\n",
       " 'environment': 399,\n",
       " 'omelets': 400,\n",
       " 'universe': 401,\n",
       " 'machine': 402,\n",
       " 'computer': 403,\n",
       " 'easier': 404,\n",
       " 'caught': 405,\n",
       " 'lined': 406,\n",
       " 'crowd': 407,\n",
       " 'smallest': 408,\n",
       " \"'budgie\": 409,\n",
       " 'unsure': 410,\n",
       " \"'grape\": 411,\n",
       " 'selling': 412,\n",
       " 'gin.\\\\nwarm': 413,\n",
       " 'tenth': 414,\n",
       " 'building': 415,\n",
       " 'teapots': 416,\n",
       " 'determined': 417,\n",
       " 'slept': 418,\n",
       " 'avalanche': 419,\n",
       " 'glittering': 420,\n",
       " 'either': 421,\n",
       " 'travel': 422,\n",
       " 'wore': 423,\n",
       " 'believe': 424,\n",
       " 'leave': 425,\n",
       " 'tent': 426,\n",
       " 'lost': 427,\n",
       " 'model': 428,\n",
       " 'bathtub': 429,\n",
       " 'joy': 430,\n",
       " 'cuddly': 431,\n",
       " 'mode': 432,\n",
       " 'sleepwalking': 433,\n",
       " 'ass': 434,\n",
       " 'parking': 435,\n",
       " 'asking': 436,\n",
       " 'goatee': 437,\n",
       " 'ivy': 438,\n",
       " 'switch': 439,\n",
       " 'trend': 440,\n",
       " 'pits': 441,\n",
       " 'internet': 442,\n",
       " 'comforters': 443,\n",
       " 'barbecue': 444,\n",
       " \"'swim\": 445,\n",
       " \"'sixty-four\": 446,\n",
       " 'olds': 447,\n",
       " 'waste': 448,\n",
       " 'darkness': 449,\n",
       " 'strongly': 450,\n",
       " 'doll': 451,\n",
       " 'begin': 452,\n",
       " 'listener': 453,\n",
       " 'batteries': 454,\n",
       " 'use': 455,\n",
       " 'bright': 456,\n",
       " 'party': 457,\n",
       " 'crash': 458,\n",
       " 'city': 459,\n",
       " 'words': 460,\n",
       " 'waves': 461,\n",
       " 'difficult': 462,\n",
       " 'drawer': 463,\n",
       " 'phobia': 464,\n",
       " 'japanese': 465,\n",
       " 'watch.\\\\nyou\\\\': 466,\n",
       " 'eggplant': 467,\n",
       " 'juicer': 468,\n",
       " 'life.\\\\nthe': 469,\n",
       " 'everyday': 470,\n",
       " 'career': 471,\n",
       " 'bond': 472,\n",
       " 'chalkboard': 473,\n",
       " 'difference': 474,\n",
       " 'room': 475,\n",
       " 'burned': 476,\n",
       " 'wonâ€™t': 477,\n",
       " 'society': 478,\n",
       " 'yet': 479,\n",
       " 'snack': 480,\n",
       " 'chose': 481,\n",
       " 'hear': 482,\n",
       " 'sympathy': 483,\n",
       " 'eyes': 484,\n",
       " \"'thigh-high\": 485,\n",
       " 'sick': 486,\n",
       " 'worm': 487,\n",
       " 'ways': 488,\n",
       " 'christmas': 489,\n",
       " 'letter': 490,\n",
       " 'porta-potty': 491,\n",
       " 'only': 492,\n",
       " 'masterpiece': 493,\n",
       " 'bedroom': 494,\n",
       " 'important': 495,\n",
       " 'estate': 496,\n",
       " 'fishing': 497,\n",
       " 'brick': 498,\n",
       " 'loved': 499,\n",
       " 'open': 500,\n",
       " 'rootbeer': 501,\n",
       " 'promotion': 502,\n",
       " 'chewing': 503,\n",
       " 'example': 504,\n",
       " 'perception': 505,\n",
       " 'ignited': 506,\n",
       " 'ran': 507,\n",
       " 'lyrics': 508,\n",
       " 'wound': 509,\n",
       " 'juice': 510,\n",
       " 'since': 511,\n",
       " 'removing': 512,\n",
       " 'tasting': 513,\n",
       " 'elbows': 514,\n",
       " 'systems': 515,\n",
       " 'angle': 516,\n",
       " 'macho': 517,\n",
       " 'back': 518,\n",
       " 'introvert': 519,\n",
       " 'month': 520,\n",
       " 'there\\\\': 521,\n",
       " 'fishpond': 522,\n",
       " 'anything': 523,\n",
       " 'acting': 524,\n",
       " 'somehow': 525,\n",
       " 'peanut': 526,\n",
       " 'even': 527,\n",
       " 'interesting': 528,\n",
       " 'supposed': 529,\n",
       " 'sloth': 530,\n",
       " 'tight': 531,\n",
       " 'bongos.\\\\npeople': 532,\n",
       " 'windswept': 533,\n",
       " 'cheated': 534,\n",
       " 'worried': 535,\n",
       " 'barley': 536,\n",
       " 'minutes': 537,\n",
       " 't-ball.\\\\nthe': 538,\n",
       " 'warmer': 539,\n",
       " 'encountered': 540,\n",
       " 'sentence': 541,\n",
       " 'masks': 542,\n",
       " 'spy': 543,\n",
       " 'diving': 544,\n",
       " 'elephant': 545,\n",
       " 'random': 546,\n",
       " 'glue': 547,\n",
       " 'frisbees': 548,\n",
       " 'quipped': 549,\n",
       " 'argument': 550,\n",
       " 'joe': 551,\n",
       " 'pizza': 552,\n",
       " 'scale': 553,\n",
       " 'fence': 554,\n",
       " 'opportunity': 555,\n",
       " 'confused': 556,\n",
       " 'on': 557,\n",
       " 'beautiful': 558,\n",
       " 'danger': 559,\n",
       " 'subsist': 560,\n",
       " 'works': 561,\n",
       " 'bird': 562,\n",
       " 'admit': 563,\n",
       " 'allow': 564,\n",
       " 'ask': 565,\n",
       " 'asia': 566,\n",
       " 'liquid': 567,\n",
       " 'limeade': 568,\n",
       " 'despite': 569,\n",
       " 'like.\\\\ntoday': 570,\n",
       " 'sailboat': 571,\n",
       " 'pets': 572,\n",
       " 'gallery': 573,\n",
       " 'shaved': 574,\n",
       " 'cellphones': 575,\n",
       " 'welcoming': 576,\n",
       " 'long.\\\\ni': 577,\n",
       " 'crime': 578,\n",
       " 'forest': 579,\n",
       " \"'the\": 580,\n",
       " 'deep': 581,\n",
       " 'have': 582,\n",
       " 'blasted': 583,\n",
       " 'just': 584,\n",
       " 'corny': 585,\n",
       " 'flew': 586,\n",
       " \"'thirty\": 587,\n",
       " 'driving': 588,\n",
       " 'want': 589,\n",
       " 'type': 590,\n",
       " 'zombies.\\\\nthere': 591,\n",
       " 'women': 592,\n",
       " 'thanks': 593,\n",
       " 'spots': 594,\n",
       " 'interviews': 595,\n",
       " 'couture': 596,\n",
       " 'campers': 597,\n",
       " 'confident': 598,\n",
       " 'wasnâ€™t': 599,\n",
       " \"'chocolate\": 600,\n",
       " 'finds': 601,\n",
       " 'mother': 602,\n",
       " 'weight-gain': 603,\n",
       " 'ping': 604,\n",
       " 'between': 605,\n",
       " 'storage': 606,\n",
       " 'stash': 607,\n",
       " 'staring': 608,\n",
       " 'maize': 609,\n",
       " 'accosted': 610,\n",
       " 'poking': 611,\n",
       " 'clauseâ€™s': 612,\n",
       " 'mirror': 613,\n",
       " 'father': 614,\n",
       " 'fighting': 615,\n",
       " 'rose': 616,\n",
       " 'fire': 617,\n",
       " 'excuses': 618,\n",
       " 'earth': 619,\n",
       " '3': 620,\n",
       " 'choosing': 621,\n",
       " 'almond': 622,\n",
       " 'went': 623,\n",
       " \"''\": 624,\n",
       " 'eaten': 625,\n",
       " 'task': 626,\n",
       " 'roaches': 627,\n",
       " 'so': 628,\n",
       " 'milk': 629,\n",
       " 'confidence': 630,\n",
       " 'noisy': 631,\n",
       " 'scratching': 632,\n",
       " 'cookie': 633,\n",
       " 'fantasy': 634,\n",
       " 'tint': 635,\n",
       " 'rustling': 636,\n",
       " 'teenage': 637,\n",
       " 'now': 638,\n",
       " 'key': 639,\n",
       " 'way': 640,\n",
       " 'news': 641,\n",
       " 'run': 642,\n",
       " \"'pair\": 643,\n",
       " 'diamonds': 644,\n",
       " 'heroes': 645,\n",
       " 'promising': 646,\n",
       " \"'s\": 647,\n",
       " 'ladder': 648,\n",
       " 'plays': 649,\n",
       " 'wings': 650,\n",
       " 'me': 651,\n",
       " 'rate': 652,\n",
       " 'law': 653,\n",
       " 'against': 654,\n",
       " 'button': 655,\n",
       " 'popular': 656,\n",
       " 'last': 657,\n",
       " 'roads': 658,\n",
       " 'dinner': 659,\n",
       " 'college': 660,\n",
       " 'mental': 661,\n",
       " 'anticipated': 662,\n",
       " 'hour': 663,\n",
       " 'circus': 664,\n",
       " 'shell': 665,\n",
       " 'may': 666,\n",
       " 'said': 667,\n",
       " 'president': 668,\n",
       " 'call': 669,\n",
       " 'bills': 670,\n",
       " \"'tuesdays\": 671,\n",
       " 'onto': 672,\n",
       " 'little': 673,\n",
       " 'hitchhiking': 674,\n",
       " 'too': 675,\n",
       " 'lake': 676,\n",
       " \"'joyce\": 677,\n",
       " 'name': 678,\n",
       " \"'truth\": 679,\n",
       " 'race.\\\\ntoday': 680,\n",
       " 'moore': 681,\n",
       " 'taken': 682,\n",
       " 'pastels': 683,\n",
       " 'spa': 684,\n",
       " 'save': 685,\n",
       " 'continued': 686,\n",
       " 'brain': 687,\n",
       " 'dentist': 688,\n",
       " 'hairier': 689,\n",
       " 'eighth': 690,\n",
       " 'worst': 691,\n",
       " 'progressed': 692,\n",
       " 'honestly': 693,\n",
       " 'empathy': 694,\n",
       " 'personâ€™s': 695,\n",
       " 'big': 696,\n",
       " 'mangroves': 697,\n",
       " 'eggs': 698,\n",
       " 'person': 699,\n",
       " 'how': 700,\n",
       " 'far': 701,\n",
       " 'depends': 702,\n",
       " 'biggest': 703,\n",
       " 'wondered': 704,\n",
       " 'acceptable': 705,\n",
       " 'hidden': 706,\n",
       " 'finding': 707,\n",
       " 'broken': 708,\n",
       " 'hog': 709,\n",
       " 'sipped': 710,\n",
       " 'teenagers': 711,\n",
       " 'gave': 712,\n",
       " 'wrong': 713,\n",
       " 'attire': 714,\n",
       " 'basket': 715,\n",
       " 'pick': 716,\n",
       " 'hoop': 717,\n",
       " 'irony': 718,\n",
       " 'older': 719,\n",
       " 'arenâ€™t': 720,\n",
       " 'miss': 721,\n",
       " 'prey': 722,\n",
       " 'decided': 723,\n",
       " 'uses': 724,\n",
       " 'lunch': 725,\n",
       " 'state': 726,\n",
       " 'though': 727,\n",
       " 'insulted': 728,\n",
       " 'flowing': 729,\n",
       " 'baboons': 730,\n",
       " 'elderly': 731,\n",
       " 'stronger': 732,\n",
       " 'these': 733,\n",
       " 'within': 734,\n",
       " 'yourself': 735,\n",
       " 'terrorist': 736,\n",
       " 'galaxy': 737,\n",
       " 'town': 738,\n",
       " 'paradise': 739,\n",
       " 'fire.\\\\nshe': 740,\n",
       " 'you': 741,\n",
       " \"'flesh-colored\": 742,\n",
       " 'plan': 743,\n",
       " 'ink': 744,\n",
       " 'smoke': 745,\n",
       " 'edge': 746,\n",
       " 'true': 747,\n",
       " 'jewel': 748,\n",
       " 'trying': 749,\n",
       " 'hopes': 750,\n",
       " 'childbirth': 751,\n",
       " 'to': 752,\n",
       " 'goal': 753,\n",
       " 'light': 754,\n",
       " 'fly-over': 755,\n",
       " 'mention': 756,\n",
       " 'ranger': 757,\n",
       " 'multiple': 758,\n",
       " 'tasted': 759,\n",
       " 'matter': 760,\n",
       " 'retriever': 761,\n",
       " 'couple': 762,\n",
       " 'reasons': 763,\n",
       " 'things': 764,\n",
       " 'camp': 765,\n",
       " 'bunker': 766,\n",
       " \"'everyone\": 767,\n",
       " 'neighbors': 768,\n",
       " 'substitute': 769,\n",
       " 'scarf': 770,\n",
       " 'location': 771,\n",
       " 'stop': 772,\n",
       " 'set': 773,\n",
       " 'pole': 774,\n",
       " 'yesterday': 775,\n",
       " 'meet': 776,\n",
       " 'issues': 777,\n",
       " 'conclusion': 778,\n",
       " 'ca': 779,\n",
       " 'roar': 780,\n",
       " 'them.\\\\ntraveling': 781,\n",
       " 'message': 782,\n",
       " \"'nothing\": 783,\n",
       " 'children.\\\\nthe': 784,\n",
       " \"'lightning\": 785,\n",
       " 'catching': 786,\n",
       " 'guidelines': 787,\n",
       " 'stand': 788,\n",
       " 'newly': 789,\n",
       " 'chips': 790,\n",
       " 'ghost': 791,\n",
       " 'climbers': 792,\n",
       " 'barked': 793,\n",
       " 'happened': 794,\n",
       " 'prepared': 795,\n",
       " 'great': 796,\n",
       " 'bee': 797,\n",
       " 'security': 798,\n",
       " 'equator.\\\\ngreen': 799,\n",
       " 'shoulders': 800,\n",
       " 'intentional': 801,\n",
       " 'sat': 802,\n",
       " 'read': 803,\n",
       " \"'behind\": 804,\n",
       " 'sticks': 805,\n",
       " 'cow': 806,\n",
       " 'mysterious': 807,\n",
       " 'flavor': 808,\n",
       " 'my': 809,\n",
       " 'longing': 810,\n",
       " 'legos': 811,\n",
       " 'carpet': 812,\n",
       " 'coke': 813,\n",
       " 'curious': 814,\n",
       " 'dressed': 815,\n",
       " 'slope': 816,\n",
       " 'knives': 817,\n",
       " 'worms': 818,\n",
       " 'sabotage': 819,\n",
       " 'talk': 820,\n",
       " 'suit': 821,\n",
       " 'applied': 822,\n",
       " 'finished': 823,\n",
       " 'lesson': 824,\n",
       " 'whip': 825,\n",
       " 'initially': 826,\n",
       " 'sewage': 827,\n",
       " 'soldiers': 828,\n",
       " 'lockpick': 829,\n",
       " 'grabbed': 830,\n",
       " \"'just\": 831,\n",
       " 'points': 832,\n",
       " 'insisted': 833,\n",
       " 'frozen': 834,\n",
       " 'girl': 835,\n",
       " \"'many\": 836,\n",
       " 'parrot': 837,\n",
       " 'weapon': 838,\n",
       " 'itself': 839,\n",
       " 'cared': 840,\n",
       " 'nearby': 841,\n",
       " 'saucepan': 842,\n",
       " 'eagerly': 843,\n",
       " 'always': 844,\n",
       " 'cabin': 845,\n",
       " \"'carol\": 846,\n",
       " 'there': 847,\n",
       " 'robbers': 848,\n",
       " 'packages': 849,\n",
       " 'surfboard': 850,\n",
       " 'dryer': 851,\n",
       " 'listening': 852,\n",
       " 'moment.\\\\nthe': 853,\n",
       " 'douglas': 854,\n",
       " 'guys': 855,\n",
       " 'any': 856,\n",
       " \"'today\": 857,\n",
       " 'slalom': 858,\n",
       " 'creativity': 859,\n",
       " 'sauce-': 860,\n",
       " 'recommend': 861,\n",
       " 'middle': 862,\n",
       " 'fond': 863,\n",
       " 'piece': 864,\n",
       " 'rocks': 865,\n",
       " 'spend': 866,\n",
       " 'basement': 867,\n",
       " 'saturday': 868,\n",
       " 'antenna': 869,\n",
       " 'stranded': 870,\n",
       " 'closed': 871,\n",
       " \"'love\": 872,\n",
       " 'pilings': 873,\n",
       " \"'iron\": 874,\n",
       " 'onomatopoeia': 875,\n",
       " 'lasso': 876,\n",
       " 'were': 877,\n",
       " 'stars': 878,\n",
       " 'symbol': 879,\n",
       " \"'not\": 880,\n",
       " 'today-': 881,\n",
       " 'dreamed': 882,\n",
       " 'bricks': 883,\n",
       " 'expected.\\\\nthe': 884,\n",
       " 'third': 885,\n",
       " 'everyone': 886,\n",
       " 'if': 887,\n",
       " 'knowing': 888,\n",
       " 'when': 889,\n",
       " \"'various\": 890,\n",
       " 'their': 891,\n",
       " 'accepting': 892,\n",
       " 'make': 893,\n",
       " \"'nancy\": 894,\n",
       " 'has': 895,\n",
       " 'exceptional': 896,\n",
       " 'months': 897,\n",
       " 'flies': 898,\n",
       " 'sandwich': 899,\n",
       " 'familyâ€™s': 900,\n",
       " 'snap': 901,\n",
       " 'vs': 902,\n",
       " 'concentration': 903,\n",
       " 'raining': 904,\n",
       " 'hair': 905,\n",
       " 'ruined': 906,\n",
       " 'trees': 907,\n",
       " 'poor': 908,\n",
       " 'fries': 909,\n",
       " 'roll': 910,\n",
       " 'behind': 911,\n",
       " 'could': 912,\n",
       " 'again': 913,\n",
       " 'hurtled': 914,\n",
       " 'pool.\\\\nmartha': 915,\n",
       " 'cup': 916,\n",
       " 'ordered': 917,\n",
       " 'transport': 918,\n",
       " 'proudly': 919,\n",
       " 'cookies': 920,\n",
       " 'paragraphs': 921,\n",
       " 'members': 922,\n",
       " 'apart': 923,\n",
       " 'left.\\\\nthey': 924,\n",
       " 'disgusted': 925,\n",
       " 'ponder': 926,\n",
       " 'slammed': 927,\n",
       " 'hesitation': 928,\n",
       " 'showers': 929,\n",
       " 'anybody': 930,\n",
       " 'taking': 931,\n",
       " 'heavy': 932,\n",
       " 'impassable': 933,\n",
       " 'used': 934,\n",
       " 'confines': 935,\n",
       " 'bowl': 936,\n",
       " 'give': 937,\n",
       " 'bed': 938,\n",
       " 'problems': 939,\n",
       " 'mastermind': 940,\n",
       " 'traveler': 941,\n",
       " 'bear': 942,\n",
       " 'road': 943,\n",
       " 'seedlings': 944,\n",
       " 'another': 945,\n",
       " 'seats': 946,\n",
       " 'subscribes': 947,\n",
       " 'cheating': 948,\n",
       " 'small': 949,\n",
       " 'parties': 950,\n",
       " 'toddlers': 951,\n",
       " 'dolphin': 952,\n",
       " 'happens': 953,\n",
       " 'friday': 954,\n",
       " 'became': 955,\n",
       " 'gentlemanâ€™s': 956,\n",
       " 'tantrum': 957,\n",
       " '2-day': 958,\n",
       " 'job': 959,\n",
       " 'bar': 960,\n",
       " 'it.\\\\nas': 961,\n",
       " 'charisma': 962,\n",
       " 'track': 963,\n",
       " \"'boulders\": 964,\n",
       " 'frames': 965,\n",
       " 'shade': 966,\n",
       " '18': 967,\n",
       " 'recycled': 968,\n",
       " 'stared': 969,\n",
       " 'vague': 970,\n",
       " 'wheels': 971,\n",
       " 'ham': 972,\n",
       " 'they': 973,\n",
       " 'skateboarding': 974,\n",
       " 'jet': 975,\n",
       " 'advised': 976,\n",
       " 'childlike': 977,\n",
       " 'rains': 978,\n",
       " 'kitchen': 979,\n",
       " 'neighborhood.\\\\nthe': 980,\n",
       " 'certain': 981,\n",
       " 'right': 982,\n",
       " 'join': 983,\n",
       " 'played': 984,\n",
       " 'moved': 985,\n",
       " 'brother': 986,\n",
       " 'penguin': 987,\n",
       " 'piglets': 988,\n",
       " '4-month-olds': 989,\n",
       " 'darts': 990,\n",
       " 'red': 991,\n",
       " 'explosive': 992,\n",
       " 'finally': 993,\n",
       " 'plum': 994,\n",
       " 'friendly': 995,\n",
       " 'trick': 996,\n",
       " 'smashing': 997,\n",
       " 'heading': 998,\n",
       " 'jug': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {s:i for i,s in enumerate(word_set)} # string to index\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ea2ef5fa-62e4-4227-b81a-c0bfa9015d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(tokenized_sentences):\n",
    "    for j, word in enumerate(sentence):\n",
    "        if word not in word_set:\n",
    "            tokenized_sentences[i][j] = 'UNKNOWN_TOKEN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7fb1cdf8-1689-4611-9e79-c1b03ab0e1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8784, 8]) torch.Size([8784])\n",
      "torch.Size([2082, 8]) torch.Size([2082])\n",
      "torch.Size([1090, 8]) torch.Size([1090])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(tokenized_sentences):\n",
    "    block_size = 8\n",
    "    X, Y = [], []\n",
    "    for sentence in tokenized_sentences:\n",
    "        context = [0] * block_size\n",
    "        for word in sentence:\n",
    "            if word == 'UNKNOWN_TOKEN':\n",
    "                continue\n",
    "            ix = stoi[word]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(tokenized_sentences)\n",
    "n1 = int(0.8 * len(tokenized_sentences))\n",
    "n2 = int(0.9 * len(tokenized_sentences))\n",
    "\n",
    "Xtr, Ytr = build_dataset(tokenized_sentences[:n1])\n",
    "Xdev, Ydev = build_dataset(tokenized_sentences[n1:n2])\n",
    "Xte, Yte = build_dataset(tokenized_sentences[n2:])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa1a489b-6080-4bef-b029-dfbf7b019faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car 'car 'car 'car 'car 'car 'car 'car --> SENTENCE_START\n",
      "'car 'car 'car 'car 'car 'car 'car SENTENCE_START --> 'he\n",
      "'car 'car 'car 'car 'car 'car SENTENCE_START 'he --> decided\n",
      "'car 'car 'car 'car 'car SENTENCE_START 'he decided --> to\n",
      "'car 'car 'car 'car SENTENCE_START 'he decided to --> live\n",
      "'car 'car 'car SENTENCE_START 'he decided to live --> his\n",
      "'car 'car SENTENCE_START 'he decided to live his --> life\n",
      "'car SENTENCE_START 'he decided to live his life --> by\n",
      "SENTENCE_START 'he decided to live his life by --> the\n",
      "'he decided to live his life by the --> big\n",
      "decided to live his life by the big --> beats\n",
      "to live his life by the big beats --> manifesto\n",
      "live his life by the big beats manifesto --> .\n",
      "his life by the big beats manifesto . --> '\n",
      "life by the big beats manifesto . ' --> SENTENCE_END\n",
      "'car 'car 'car 'car 'car 'car 'car 'car --> SENTENCE_START\n",
      "'car 'car 'car 'car 'car 'car 'car SENTENCE_START --> '100\n",
      "'car 'car 'car 'car 'car 'car SENTENCE_START '100 --> years\n",
      "'car 'car 'car 'car 'car SENTENCE_START '100 years --> old\n",
      "'car 'car 'car 'car SENTENCE_START '100 years old --> is\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip( Xtr[:20], Ytr[:20]):\n",
    "    print(' '.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "302336a3-df5a-4f1b-aeee-a54276e446c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias = True):\n",
    "        self.weight = torch.randn((fan_in, fan_out)) / fan_in ** 0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps = 1e-5, momentum = 0.1):\n",
    "        self.eps = eps # epsilon\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # Parameters trained in backprop\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # Buffers (trained with a running 'momentum update')\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Forward pass\n",
    "        if self.training:\n",
    "            if x.ndim ==2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0,1)\n",
    "            xmean = x.mean(0, keepdim = True)\n",
    "            xvar = x.var(0, keepdim = True, unbiased = True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x-xmean) / torch.sqrt(xvar + self.eps) \n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        # Update our buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1-self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, num_embeddings, embeddings_dim):\n",
    "        self.weight = torch.randn((num_embeddings, embeddings_dim));\n",
    "    def __call__(self, IX):\n",
    "        self.out = self.weight[IX]\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class FlattenConsecutive:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T//self.n, C*self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        # Get all parameters and put them in a list\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8fa63176-2485-4d50-bc78-12c95f61ceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495340\n"
     ]
    }
   ],
   "source": [
    "vocab_size = dim\n",
    "n_embd = 24 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 128 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, n_embd),\n",
    "    FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size),\n",
    "])\n",
    "\n",
    "# # Initalize parameters\n",
    "# with torch.no_grad():\n",
    "#   layers[-1].weight *= 0.1\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3e106a3c-d9e5-4085-9af7-c2459dedc405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 7.5114\n",
      "  10000/ 200000: 0.5206\n",
      "  20000/ 200000: 0.5838\n",
      "  30000/ 200000: 0.1938\n",
      "  40000/ 200000: 0.2704\n",
      "  50000/ 200000: 0.6689\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[1;32m     19\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# update\u001b[39;00m\n\u001b[1;32m     23\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m150000\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.01\u001b[39m \u001b[38;5;66;03m# step learning rate decay\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ca2dfdc2-6d4e-4a44-a2e9-e3f552021a62",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1000]' is invalid for input of size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlossi\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(torch\u001b[38;5;241m.\u001b[39mtensor(lossi)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 1000]' is invalid for input of size 1"
     ]
    }
   ],
   "source": [
    "torch.tensor(lossi).view(-1, 1000).mean(1).shape\n",
    "plt.plot(torch.tensor(lossi).view(-1,100).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6b02c321-9dd3-47f8-91c7-8afb5b13e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tensors to text file with labels\n",
    "def save_tensors_to_txt(filename, tensor_names, tensors):\n",
    "    with open(filename, 'w') as file:\n",
    "        for name, tensor in zip(tensor_names, tensors):\n",
    "            file.write(f'{name}\\n') \n",
    "            flattened_tensor = tensor.detach().numpy()  # Flatten the tensor and convert to numpy array\n",
    "            flattened_tensor_str = ','.join(map(str, flattened_tensor))\n",
    "            file.write(f'{flattened_tensor_str}\\n') \n",
    "\n",
    "parameters = model.parameters()\n",
    "parameter_names = ['embedding_weight', 'linear1_weight', 'linear1_gamma', 'linear1_beta', 'batchnorm1_running_mean', 'batchnorm1_running_var',\n",
    "                   'linear2_weight', 'linear2_gamma', 'linear2_beta', 'batchnorm2_running_mean', 'batchnorm2_running_var',\n",
    "                   'linear3_weight', 'linear3_gamma', 'linear3_beta', 'batchnorm3_running_mean', 'batchnorm3_running_var',\n",
    "                   'linear4_weight', 'linear4_bias']\n",
    "\n",
    "save_tensors_to_txt('model2_parameters.txt', parameter_names, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "62e682bb-9f04-4023-8c14-a141f12a3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all layers into eval mode\n",
    "for layer in model.layers:\n",
    "    layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e3d39141-e67a-47af-ac72-4d4783d0fb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.39030706882476807\n",
      "val 8.892152786254883\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the loss\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "30039a98-e7c4-464e-aaeb-1c676e881a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE_START  'he  figured  a  few  sticks  of  dynamite  were  easier  than  a  fishing  pole  to  catch  fish  .  '  SENTENCE_END\n",
      "SENTENCE_START  'lucifer  was  surprised  at  the  amount  of  life  at  death  valley  .  '  SENTENCE_END\n",
      "SENTENCE_START  'at  that  moment  she  realized  she  had  a  sixth  sense  .  '  SENTENCE_END\n",
      "SENTENCE_START  'in  that  instant  '  ,  'everything  changed  .  '  SENTENCE_END\n",
      "SENTENCE_START  ``  it  would  have  been  a  better  night  if  the  guys  next  to  us  were  n't  in  the  splash\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "block_size = 8\n",
    "for _ in range(5):\n",
    "    counter = 0\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True and counter < 20:\n",
    "        logits = model(torch.tensor([context]))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # Sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        counter = counter + 1\n",
    "        if ix == stoi['SENTENCE_END']:\n",
    "            break\n",
    "    print('  '.join(itos[i] for i in out)) # decode and print the generated word\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Sample from the model\n",
    "\n",
    "# for _ in range (5):\n",
    "#     out = []\n",
    "#     context = [0] * block_size\n",
    "#     counter = 0\n",
    "#     while True & counter < 20:\n",
    "#         # Embed current context using embedding table\n",
    "#         emb = C[torch.tensor([context])] # (1, block_size, d)\n",
    "#         h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "#         logits = h @ W2 + b2\n",
    "#         probs = F.softmax(logits, dim = 1)\n",
    "#         ix = torch.multinomial(probs, num_samples = 1, generator = g).item()\n",
    "#         context = context[1:] + [ix]\n",
    "#         out.append(ix)\n",
    "#         counter +=1\n",
    "#         if ix == stoi['SENTENCE_END']:\n",
    "#             break\n",
    "#         if ix == stoi['SENTENCE_START']:\n",
    "#             break\n",
    "#     print(' '.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
