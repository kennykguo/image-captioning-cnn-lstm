{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a2a30e-5b4d-4c06-81d9-878f4b21afa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Download NLTK model data (you need to do this once)\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d614b5d-907b-41a8-a6c7-4a805fd3fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81a96b5c-8a7e-44d8-ad05-6d0aad0d6b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 68647 sentences.\n"
     ]
    }
   ],
   "source": [
    "with open('../data/reddit-comments-2015-08.csv', 'r', newline='', encoding='utf-8') as f:\n",
    "    # Initalize a reader object\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    # Skip the header row\n",
    "    # next(reader)  \n",
    "    # Split full comments into sentences  - [nltk.sent_tokenize(x[0].lower()) for x in reader] - for the paragraph x[0] from the csv file, make it lowercase and tokenize all sentence\n",
    "    # For all pararaphs in the csv file. * operator unpacks the list into individual sentences, and creates a single iterable\n",
    "    # sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(str(x).lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    # Replace all sentence x in sentences with the start token, sentence body, and text token\"\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print (f\"Parsed {len(sentences)} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac0dc1ea-0b43-43cf-803d-2ec7f03d7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd63d95c-ba97-4582-acfc-0bbcfe288aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [[word for word in sentence if word not in {'[', ']', '(', ')'}] for sentence in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "053435a4-26a2-452c-925a-92938f19ae10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SENTENCE_START', 'a', 'dishonest', 'seller', \"isn\\\\'t\", 'going', 'to', 'run', 'the', 'check', 'in', 'the', 'first', 'place', '.', 'SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "# # List of lists\n",
    "print(tokenized_sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9883e9de-4bc0-4c24-8a26-f7ac00b411c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}\n",
    "\n",
    "for i in range (len(tokenized_sentences)):\n",
    "    for word1, word2, in zip(tokenized_sentences[i], tokenized_sentences[i][1:]):\n",
    "        # Create a tuple\n",
    "        bigram = (word1, word2)\n",
    "        # Index into the dictionary, update it by one\n",
    "        b[bigram] = b.get(bigram, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ed66e52b-3cc6-460f-808e-b4900d26fa04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('.', 'SENTENCE_END'), 47244),\n",
       " (('SENTENCE_START', '``'), 9738),\n",
       " ((\"''\", 'SENTENCE_END'), 7573),\n",
       " (('SENTENCE_START', 'i'), 6466),\n",
       " ((\"'\", 'SENTENCE_END'), 5956),\n",
       " ((',', 'and'), 5707),\n",
       " (('.', \"''\"), 4960),\n",
       " ((',', 'but'), 4942),\n",
       " (('*', '*'), 4696),\n",
       " (('of', 'the'), 4317),\n",
       " (('in', 'the'), 4082),\n",
       " (('?', 'SENTENCE_END'), 3919),\n",
       " (('.', \"'\"), 3412),\n",
       " (('it', \"'s\"), 3313),\n",
       " ((',', 'i'), 3176),\n",
       " (('if', 'you'), 2899),\n",
       " (('do', \"n't\"), 2748),\n",
       " (('&', 'gt'), 2573),\n",
       " (('gt', ';'), 2573),\n",
       " (('to', 'be'), 2485),\n",
       " (('http', ':'), 2354),\n",
       " (('i', \"'m\"), 2336),\n",
       " (('SENTENCE_START', 'it'), 2273),\n",
       " (('SENTENCE_START', 'the'), 2240),\n",
       " (('``', 'SENTENCE_END'), 2116),\n",
       " (('to', 'the'), 2106),\n",
       " (('on', 'the'), 2059),\n",
       " (('``', 'i'), 1996),\n",
       " (('SENTENCE_START', \"'\"), 1870),\n",
       " (('!', 'SENTENCE_END'), 1804),\n",
       " (('is', 'a'), 1693),\n",
       " (('SENTENCE_START', 'if'), 1634),\n",
       " (('https', ':'), 1629),\n",
       " ((',', 'the'), 1599),\n",
       " (('and', 'i'), 1568),\n",
       " (('you', 'can'), 1563),\n",
       " (('amp', ';'), 1535),\n",
       " (('for', 'the'), 1529),\n",
       " (('you', 'have'), 1510),\n",
       " (('SENTENCE_START', 'you'), 1504),\n",
       " (('but', 'i'), 1503),\n",
       " (('&', 'amp'), 1483),\n",
       " ((',', 'it'), 1462),\n",
       " (('and', 'the'), 1423),\n",
       " (('in', 'a'), 1405),\n",
       " ((',', 'you'), 1393),\n",
       " (('i', 'was'), 1378),\n",
       " (('i', 'do'), 1375),\n",
       " (('it', 'is'), 1371),\n",
       " (('with', 'the'), 1330),\n",
       " (('i', 'think'), 1327),\n",
       " (('it', '.'), 1320),\n",
       " (('it', 'was'), 1319),\n",
       " (('have', 'a'), 1307),\n",
       " (('it\\\\', \"'s\"), 1306),\n",
       " (('i', 'have'), 1306),\n",
       " (('you', \"'re\"), 1254),\n",
       " ((',', 'so'), 1251),\n",
       " (('a', 'lot'), 1233),\n",
       " (('i', 'am'), 1181),\n",
       " (('for', 'a'), 1166),\n",
       " (('this', 'is'), 1129),\n",
       " (('SENTENCE_START', 'but'), 1113),\n",
       " (('have', 'to'), 1112),\n",
       " (('want', 'to'), 1097),\n",
       " (('the', 'same'), 1093),\n",
       " (('to', 'get'), 1092),\n",
       " (('you', 'are'), 1092),\n",
       " (('going', 'to'), 1083),\n",
       " (('at', 'the'), 1059),\n",
       " (('that', \"'s\"), 1052),\n",
       " (('would', 'be'), 1027),\n",
       " (('as', 'a'), 1021),\n",
       " (('does', \"n't\"), 1020),\n",
       " (('with', 'a'), 995),\n",
       " (('to', 'do'), 994),\n",
       " (('SENTENCE_START', '*'), 992),\n",
       " (('i', \"'ve\"), 983),\n",
       " ((\"'s\", 'a'), 978),\n",
       " (('SENTENCE_START', 'and'), 975),\n",
       " (('of', 'a'), 969),\n",
       " ((',', 'or'), 957),\n",
       " (('SENTENCE_START', 'they'), 952),\n",
       " (('i', 'would'), 939),\n",
       " (('is', 'the'), 906),\n",
       " (('i\\\\', \"'m\"), 895),\n",
       " (('they', 'are'), 880),\n",
       " (('that', 'the'), 847),\n",
       " (('from', 'the'), 835),\n",
       " (('SENTENCE_START', 'this'), 825),\n",
       " (('it', ','), 820),\n",
       " ((\"'\", 'i'), 820),\n",
       " (('ca', \"n't\"), 813),\n",
       " ((\"'s\", 'not'), 811),\n",
       " (('lot', 'of'), 806),\n",
       " (('i', 'can'), 805),\n",
       " (('SENTENCE_START', 'he'), 785),\n",
       " (('need', 'to'), 780),\n",
       " (('one', 'of'), 773),\n",
       " (('be', 'a'), 767),\n",
       " (('out', 'of'), 761),\n",
       " (('so', 'i'), 750),\n",
       " (('that', 'i'), 748),\n",
       " (('and', 'it'), 746),\n",
       " (('but', 'it'), 742),\n",
       " (('did', \"n't\"), 739),\n",
       " (('all', 'the'), 739),\n",
       " (('to', 'make'), 738),\n",
       " (('will', 'be'), 738),\n",
       " (('a', 'few'), 733),\n",
       " (('there', 'is'), 732),\n",
       " (('of', 'this'), 732),\n",
       " (('SENTENCE_START', 'that'), 729),\n",
       " ((',', 'which'), 729),\n",
       " (('that', 'you'), 726),\n",
       " (('SENTENCE_START', 'so'), 718),\n",
       " (('there', 'are'), 715),\n",
       " (('to', 'a'), 708),\n",
       " (('is', 'not'), 702),\n",
       " (('\\\\n\\\\n', '*'), 697),\n",
       " (('is', \"n't\"), 696),\n",
       " ((',', 'they'), 664),\n",
       " (('SENTENCE_START', 'we'), 662),\n",
       " (('to', 'have'), 662),\n",
       " (('you', 'do'), 659),\n",
       " (('when', 'i'), 658),\n",
       " (('that', 'is'), 654),\n",
       " (('the', 'game'), 653),\n",
       " (('is', 'that'), 642),\n",
       " (('was', 'a'), 639),\n",
       " (('if', 'i'), 639),\n",
       " ((\"'m\", 'not'), 628),\n",
       " (('able', 'to'), 628),\n",
       " ((',', 'then'), 626),\n",
       " (('on', 'a'), 625),\n",
       " (('SENTENCE_START', 'please'), 619),\n",
       " ((',', 'if'), 617),\n",
       " (('the', 'other'), 614),\n",
       " (('have', 'any'), 606),\n",
       " (('\\\\n', '*'), 602),\n",
       " (('trying', 'to'), 601),\n",
       " (('a', 'good'), 600),\n",
       " (('the', 'first'), 585),\n",
       " (('you\\\\', \"'re\"), 584),\n",
       " (('SENTENCE_START', 'there'), 584),\n",
       " ((',', 'that'), 583),\n",
       " (('as', 'well'), 581),\n",
       " (('should', 'be'), 576),\n",
       " (('at', 'least'), 574),\n",
       " (('i', \"'d\"), 570),\n",
       " (('there', \"'s\"), 569),\n",
       " (('has', 'been'), 554),\n",
       " (('i', 'had'), 550),\n",
       " (('can', 'be'), 543),\n",
       " (('in', 'my'), 542),\n",
       " (('and', 'you'), 540),\n",
       " (('when', 'you'), 538),\n",
       " (('but', 'the'), 535),\n",
       " (('a', 'bit'), 534),\n",
       " (('i', 'know'), 534),\n",
       " (('have', 'been'), 531),\n",
       " (('of', 'my'), 528),\n",
       " (('and', 'this'), 526),\n",
       " (('and', 'a'), 522),\n",
       " (('\\\\n\\\\n', '&'), 521),\n",
       " (('you', 'want'), 518),\n",
       " (('they', \"'re\"), 517),\n",
       " (('if', 'it'), 516),\n",
       " (('kind', 'of'), 516),\n",
       " (('about', 'the'), 515),\n",
       " (('i', 'just'), 514),\n",
       " ((',', 'as'), 514),\n",
       " (('and', 'then'), 512),\n",
       " (('the', 'only'), 505),\n",
       " (('which', 'is'), 503),\n",
       " (('SENTENCE_START', '\\\\n\\\\ni'), 502),\n",
       " ((\"n't\", 'have'), 501),\n",
       " (('what', 'you'), 501),\n",
       " (('he', 'was'), 498),\n",
       " (('he', \"'s\"), 493),\n",
       " (('to', 'see'), 488),\n",
       " (('the', 'best'), 488),\n",
       " (('it', 'would'), 485),\n",
       " (('the', 'time'), 485),\n",
       " (('them', '.'), 485),\n",
       " (('if', 'they'), 485),\n",
       " (('?', \"''\"), 482),\n",
       " (('``', '&'), 481),\n",
       " (('a', 'little'), 480),\n",
       " (('*', 'i'), 479),\n",
       " ((',', 'not'), 479),\n",
       " (('by', 'the'), 474),\n",
       " (('i', 'did'), 472),\n",
       " (('a', 'bot'), 467),\n",
       " (('not', 'a'), 467),\n",
       " (('that', 'it'), 466),\n",
       " (('am', 'a'), 463),\n",
       " (('it', 'to'), 462),\n",
       " (('what', 'i'), 462),\n",
       " (('the', 'most'), 461),\n",
       " (('SENTENCE_START', '\\\\n\\\\n'), 460),\n",
       " (('like', 'a'), 458),\n",
       " (('to', 'me'), 458),\n",
       " (('this', 'subreddit'), 458),\n",
       " (('SENTENCE_START', 'in'), 455),\n",
       " (('that\\\\', \"'s\"), 453),\n",
       " (('the', 'moderators'), 453),\n",
       " (('to', 'go'), 452),\n",
       " (('do', 'you'), 450),\n",
       " (('that', '.'), 447),\n",
       " (('!', \"''\"), 447),\n",
       " (('me', '.'), 440),\n",
       " (('have', 'the'), 437),\n",
       " (('that', 'they'), 437),\n",
       " (('/message/compose/', '?'), 433),\n",
       " (('people', 'who'), 431),\n",
       " ((\"''\", '.'), 431),\n",
       " ((',', 'a'), 427),\n",
       " (('because', 'i'), 427),\n",
       " (('any', 'questions'), 427),\n",
       " (('more', 'than'), 425),\n",
       " ((',', 'because'), 425),\n",
       " (('contact', 'the'), 425),\n",
       " (('had', 'a'), 424),\n",
       " (('part', 'of'), 420),\n",
       " ((\"'\", '&'), 419),\n",
       " ((',', 'he'), 419),\n",
       " (('you', '.'), 417),\n",
       " (('i', 'could'), 416),\n",
       " (('bot', ','), 411),\n",
       " (('please', 'contact'), 411),\n",
       " (('SENTENCE_START', 'my'), 408),\n",
       " (('i', \"don\\\\'t\"), 408),\n",
       " (('this', 'action'), 406),\n",
       " ((\"'s\", 'the'), 405),\n",
       " (('SENTENCE_START', 'a'), 403),\n",
       " (('questions', 'or'), 402),\n",
       " (('would', \"n't\"), 400),\n",
       " ((',', 'there'), 399),\n",
       " (('as', 'the'), 399),\n",
       " (('or', 'concerns'), 399),\n",
       " (('of', 'your'), 399),\n",
       " (('and', 'that'), 399),\n",
       " (('they', 'have'), 398),\n",
       " (('concerns', '.'), 398),\n",
       " (('like', 'the'), 398),\n",
       " (('would', 'have'), 397),\n",
       " (('action', 'was'), 397),\n",
       " (('automatically', '.'), 396),\n",
       " (('for', 'me'), 395),\n",
       " (('was', 'performed'), 395),\n",
       " (('performed', 'automatically'), 395),\n",
       " (('moderators', 'of'), 393),\n",
       " (('time', '.'), 391),\n",
       " ((',', 'we'), 390),\n",
       " (('i', \"'ll\"), 390),\n",
       " (('get', 'a'), 385),\n",
       " (('i\\\\', \"'ve\"), 384),\n",
       " (('because', 'it'), 383),\n",
       " ((',', 'just'), 382),\n",
       " (('subreddit', '/message/compose/'), 381),\n",
       " (('has', 'a'), 381),\n",
       " (('in', 'your'), 381),\n",
       " (('``', 'it'), 379),\n",
       " (('--', '--'), 379),\n",
       " (('into', 'the'), 378),\n",
       " (('way', 'to'), 377),\n",
       " (('``', 'the'), 377),\n",
       " (('all', 'of'), 375),\n",
       " (('if', 'the'), 375),\n",
       " (('of', 'it'), 374),\n",
       " ((':', '//www.youtube.com/watch'), 374),\n",
       " (('//www.youtube.com/watch', '?'), 374),\n",
       " (('about', 'it'), 373),\n",
       " (('try', 'to'), 368),\n",
       " (('the', 'way'), 367),\n",
       " (('to', 'say'), 365),\n",
       " (('you', ','), 363),\n",
       " ((\"n't\", 'know'), 363),\n",
       " (('they', 'were'), 363),\n",
       " (('in', 'this'), 362),\n",
       " (('you', 'should'), 361),\n",
       " (('do', 'not'), 360),\n",
       " (('at', 'all'), 357),\n",
       " (('most', 'of'), 357),\n",
       " ((\"'s\", 'just'), 356),\n",
       " (('SENTENCE_START', 'not'), 356),\n",
       " (('of', 'them'), 355),\n",
       " (('it', 'does'), 355),\n",
       " (('even', 'if'), 355),\n",
       " (('we', 'have'), 353),\n",
       " (('as', 'i'), 351),\n",
       " (('and', 'they'), 349),\n",
       " (('SENTENCE_START', '\\\\n'), 349),\n",
       " (('that', ','), 348),\n",
       " (('be', 'able'), 347),\n",
       " (('SENTENCE_START', 'also'), 346),\n",
       " (('me', ','), 346),\n",
       " (('of', 'people'), 345),\n",
       " (('to', 'use'), 344),\n",
       " ((\"''\", ','), 344),\n",
       " (('SENTENCE_START', 'i\\\\'), 342),\n",
       " (('SENTENCE_START', 'just'), 340),\n",
       " (('SENTENCE_START', 'as'), 338),\n",
       " (('be', 'the'), 338),\n",
       " (('SENTENCE_START', 'what'), 337),\n",
       " ((\"'ve\", 'been'), 337),\n",
       " (('some', 'of'), 335),\n",
       " (('you', 'to'), 334),\n",
       " ((':', '\\\\n\\\\n'), 334),\n",
       " ((',', 'in'), 333),\n",
       " (('you', 'could'), 332),\n",
       " (('used', 'to'), 331),\n",
       " (('was', 'the'), 331),\n",
       " (('had', 'to'), 329),\n",
       " (('was', \"n't\"), 328),\n",
       " ((',', 'etc'), 327),\n",
       " (('that', 'he'), 327),\n",
       " (('because', 'of'), 326),\n",
       " (('you', 'know'), 325),\n",
       " (('but', 'you'), 324),\n",
       " (('not', 'the'), 321),\n",
       " (('they', 'do'), 320),\n",
       " (('and', 'not'), 320),\n",
       " ((',', 'even'), 320),\n",
       " (('are', \"n't\"), 320),\n",
       " (('how', 'to'), 319),\n",
       " ((\"n't\", 'think'), 318),\n",
       " (('to', 'play'), 318),\n",
       " (('it', 'will'), 318),\n",
       " (('the', 'world'), 318),\n",
       " (('you', 'need'), 317),\n",
       " (('just', 'a'), 317),\n",
       " (('*', \"''\"), 316),\n",
       " (('because', 'they'), 316),\n",
       " (('you', 'get'), 316),\n",
       " (('he', 'is'), 316),\n",
       " (('think', 'it'), 316),\n",
       " (('wo', \"n't\"), 314),\n",
       " (('*', '|'), 313),\n",
       " (('etc', '.'), 312),\n",
       " (('SENTENCE_START', 'do'), 311),\n",
       " (('SENTENCE_START', 'it\\\\'), 310),\n",
       " (('``', 'you'), 310),\n",
       " (('for', 'you'), 310),\n",
       " (('you', 'think'), 310),\n",
       " (('*', \"'\"), 309),\n",
       " (('the', 'fact'), 308),\n",
       " (('a', 'new'), 307),\n",
       " ((\"n't\", 'be'), 305),\n",
       " (('could', 'be'), 304),\n",
       " (('instead', 'of'), 303),\n",
       " (('at', 'a'), 302),\n",
       " (('to', 'take'), 302),\n",
       " (('look', 'at'), 301),\n",
       " (('on', 'my'), 300),\n",
       " ((',', 'this'), 300),\n",
       " (('up', 'to'), 300),\n",
       " (('well', ','), 299),\n",
       " (('is', 'just'), 298),\n",
       " (('you', 'will'), 297),\n",
       " (('does', 'not'), 295),\n",
       " (('though', '.'), 295),\n",
       " (('are', 'not'), 294),\n",
       " (('due', 'to'), 294),\n",
       " (('SENTENCE_START', 'when'), 294),\n",
       " (('not', 'to'), 293),\n",
       " (('are', 'the'), 292),\n",
       " (('so', 'much'), 291),\n",
       " (('sort', 'of'), 290),\n",
       " (('that', 'are'), 289),\n",
       " (('do', 'it'), 289),\n",
       " (('that', 'was'), 289),\n",
       " (('the', '``'), 288),\n",
       " (('might', 'be'), 287),\n",
       " (('SENTENCE_START', 'then'), 286),\n",
       " (('for', 'your'), 286),\n",
       " (('not', 'be'), 286),\n",
       " (('like', 'this'), 284),\n",
       " (('i', 'guess'), 284),\n",
       " (('SENTENCE_START', 'no'), 281),\n",
       " (('but', 'if'), 281),\n",
       " (('like', 'to'), 280),\n",
       " (('is', 'to'), 279),\n",
       " (('i', 'got'), 279),\n",
       " (('fact', 'that'), 278),\n",
       " (('as', 'you'), 277),\n",
       " (('like', 'that'), 275),\n",
       " (('SENTENCE_START', 'for'), 275),\n",
       " (('think', 'that'), 275),\n",
       " (('there', 'was'), 274),\n",
       " (('?', \"'\"), 274),\n",
       " (('from', 'a'), 274),\n",
       " (('this', '.'), 274),\n",
       " (('*', '\\\\n\\\\n'), 273),\n",
       " (('have', 'no'), 272),\n",
       " (('over', 'the'), 272),\n",
       " ((\"'re\", 'not'), 272),\n",
       " (('make', 'it'), 271),\n",
       " (('back', 'to'), 271),\n",
       " ((',', 'like'), 271),\n",
       " ((',', 'please'), 270),\n",
       " ((',', '``'), 270),\n",
       " (('the', 'last'), 269),\n",
       " (('too', '.'), 269),\n",
       " (('up', '.'), 269),\n",
       " (('SENTENCE_START', 'she'), 269),\n",
       " (('enough', 'to'), 268),\n",
       " (('and', 'he'), 267),\n",
       " (('!', '!'), 266),\n",
       " (('the', 'end'), 265),\n",
       " (('get', 'the'), 265),\n",
       " (('a', 'very'), 264),\n",
       " (('you', \"'ll\"), 264),\n",
       " (('when', 'they'), 263),\n",
       " (('not', 'sure'), 263),\n",
       " (('know', 'what'), 261),\n",
       " (('make', 'a'), 261),\n",
       " (('go', 'to'), 261),\n",
       " (('i', 'feel'), 260),\n",
       " (('me', 'to'), 259),\n",
       " ((';', 'i'), 259),\n",
       " (('is', 'an'), 259),\n",
       " (('we', 'are'), 259),\n",
       " (('but', 'that'), 259),\n",
       " (('of', 'their'), 258),\n",
       " (('they', 'can'), 258),\n",
       " (('to', 'your'), 257),\n",
       " (('with', 'my'), 257),\n",
       " (('to', 'keep'), 256),\n",
       " (('now', '.'), 256),\n",
       " (('time', ','), 256),\n",
       " (('the', 'right'), 255),\n",
       " (('though', ','), 255),\n",
       " (('it', 'has'), 255),\n",
       " (('and', 'have'), 254),\n",
       " (('a', 'couple'), 254),\n",
       " (('out', '.'), 254),\n",
       " (('a', '``'), 254),\n",
       " (('i', 'will'), 254),\n",
       " (('talking', 'about'), 253),\n",
       " (('than', 'the'), 252),\n",
       " (('as', 'it'), 252),\n",
       " (('to', 'you'), 252),\n",
       " (('in', 'that'), 251),\n",
       " ((',', 'is'), 250),\n",
       " (('have', \"n't\"), 250),\n",
       " (('*', ':'), 250),\n",
       " (('are', 'you'), 248),\n",
       " (('on', 'your'), 248),\n",
       " (('however', ','), 248),\n",
       " (('#', '#'), 248),\n",
       " (('right', 'now'), 248),\n",
       " (('time', 'to'), 248),\n",
       " ((',', 'though'), 247),\n",
       " (('people', 'are'), 247),\n",
       " (('there', '.'), 247),\n",
       " (('now', ','), 246),\n",
       " (('is', 'no'), 245),\n",
       " (('of', 'course'), 245),\n",
       " (('for', 'it'), 244),\n",
       " (('the', 'whole'), 244),\n",
       " (('to', 'find'), 244),\n",
       " (('it', 'in'), 243),\n",
       " (('i', 'ca'), 243),\n",
       " (('up', 'with'), 242),\n",
       " (('well', '.'), 242),\n",
       " (('a', 'great'), 241),\n",
       " ((\"n't\", 'really'), 240),\n",
       " (('SENTENCE_START', 'maybe'), 240),\n",
       " (('can', 'not'), 240),\n",
       " (('i', 'mean'), 240),\n",
       " (('i', 'want'), 240),\n",
       " (('i', 'love'), 240),\n",
       " (('them', 'to'), 239),\n",
       " (('it', 'and'), 239),\n",
       " (('are', 'a'), 239),\n",
       " (('game', '.'), 238),\n",
       " (('so', 'you'), 238),\n",
       " (('of', 'that'), 237),\n",
       " ((',', 'no'), 237),\n",
       " (('feel', 'like'), 237),\n",
       " (('said', ','), 237),\n",
       " (('SENTENCE_START', 'even'), 236),\n",
       " (('with', 'it'), 236),\n",
       " (('make', 'sure'), 236),\n",
       " (('wanted', 'to'), 236),\n",
       " (('amount', 'of'), 235),\n",
       " (('on', 'this'), 235),\n",
       " (('the', 'us'), 234),\n",
       " (('like', 'you'), 233),\n",
       " (('your', 'post'), 233),\n",
       " (('this', ','), 233),\n",
       " (('i', 'really'), 233),\n",
       " ((\"n't\", 'get'), 232),\n",
       " (('when', 'it'), 232),\n",
       " (('where', 'you'), 232),\n",
       " (('\\\\n\\\\n', \"''\"), 232),\n",
       " (('the', 'people'), 231),\n",
       " (('to', 'this'), 230),\n",
       " (('when', 'the'), 230),\n",
       " (('and', 'if'), 230),\n",
       " (('they\\\\', \"'re\"), 229),\n",
       " ((\"'ll\", 'be'), 229),\n",
       " (('we', 'can'), 229),\n",
       " (('into', 'a'), 229),\n",
       " (('because', 'you'), 228),\n",
       " (('that', 'would'), 228),\n",
       " (('SENTENCE_START', 'or'), 228),\n",
       " (('i', 'like'), 228),\n",
       " (('but', 'they'), 228),\n",
       " (('of', 'these'), 227),\n",
       " (('so', 'it'), 226),\n",
       " ((',', 'with'), 226),\n",
       " (('based', 'on'), 226),\n",
       " (('think', 'the'), 225),\n",
       " (('to', 'work'), 225),\n",
       " ((',', 'i\\\\'), 224),\n",
       " (('gon', 'na'), 223),\n",
       " (('so', 'that'), 223),\n",
       " (('yes', ','), 223),\n",
       " (('you', \"'ve\"), 223),\n",
       " (('there\\\\', \"'s\"), 222),\n",
       " (('no', 'one'), 222),\n",
       " ((\"n't\", 'want'), 222),\n",
       " (('pretty', 'much'), 222),\n",
       " (('him', '.'), 222),\n",
       " (('can', 'get'), 221),\n",
       " (('or', 'not'), 221),\n",
       " (('here', '.'), 220),\n",
       " (('SENTENCE_START', \"'the\"), 220),\n",
       " (('if', 'he'), 220),\n",
       " (('hard', 'to'), 219),\n",
       " (('SENTENCE_START', 'http'), 218),\n",
       " (('being', 'a'), 218),\n",
       " (('up', 'and'), 218),\n",
       " (('*', '\\\\n'), 218),\n",
       " (('and', 'we'), 217),\n",
       " (('SENTENCE_START', 'all'), 217),\n",
       " (('the', 'rest'), 217),\n",
       " (('SENTENCE_START', ':'), 217),\n",
       " (('rather', 'than'), 216),\n",
       " (('that', 'we'), 216),\n",
       " (('can', 'do'), 216),\n",
       " (('thank', 'you'), 215),\n",
       " ((\"n't\", 'even'), 215),\n",
       " ((',', 'it\\\\'), 215),\n",
       " (('the', 'one'), 215),\n",
       " ((':', '*'), 214),\n",
       " (('out', 'the'), 213),\n",
       " (('what', 'they'), 213),\n",
       " (('of', 'those'), 213),\n",
       " (('*', '.'), 212),\n",
       " (('is', ','), 211),\n",
       " (('because', 'the'), 211),\n",
       " (('%', 'of'), 211),\n",
       " (('do', 'with'), 210),\n",
       " (('say', 'that'), 210),\n",
       " (('as', 'much'), 210),\n",
       " (('them', ','), 209),\n",
       " (('``', '*'), 209),\n",
       " (('we', \"'re\"), 209),\n",
       " (('SENTENCE_START', 'however'), 209),\n",
       " (('looking', 'for'), 208),\n",
       " ((\"''\", 'and'), 207),\n",
       " (('yeah', ','), 206),\n",
       " ((\"'d\", 'be'), 206),\n",
       " (('!', \"'\"), 206),\n",
       " (('like', 'it'), 205),\n",
       " (('i', 'get'), 205),\n",
       " (('may', 'be'), 204),\n",
       " (('better', 'than'), 203),\n",
       " (('|', '*'), 203),\n",
       " (('``', 'this'), 202),\n",
       " (('also', ','), 202),\n",
       " (('to', 'help'), 202),\n",
       " (('of', 'what'), 202),\n",
       " (('how', 'much'), 201),\n",
       " (('up', 'the'), 201),\n",
       " (('then', 'you'), 201),\n",
       " (('end', 'of'), 201),\n",
       " (('``', 'that'), 201),\n",
       " (('to', 'my'), 200),\n",
       " (('i', 'thought'), 200),\n",
       " (('take', 'a'), 200),\n",
       " (('the', 'next'), 199),\n",
       " (('with', 'your'), 199),\n",
       " ((',', \"''\"), 198),\n",
       " (('``', 'yeah'), 198),\n",
       " (('i', 'said'), 197),\n",
       " (('of', 'his'), 197),\n",
       " (('your', 'own'), 197),\n",
       " (('again', ','), 197),\n",
       " (('``', 'well'), 197),\n",
       " (('i', 'see'), 196),\n",
       " (('but', 'not'), 196),\n",
       " (('you', 'would'), 196),\n",
       " ((',', 'what'), 196),\n",
       " (('seem', 'to'), 196),\n",
       " ((':', \"''\"), 196),\n",
       " (('through', 'the'), 195),\n",
       " (('their', 'own'), 195),\n",
       " (('SENTENCE_START', '\\\\n\\\\nthe'), 194),\n",
       " (('and', 'is'), 194),\n",
       " ((',', 'when'), 194),\n",
       " (('think', 'you'), 193),\n",
       " (('a', 'huge'), 193),\n",
       " (('then', 'i'), 193),\n",
       " (('is', 'going'), 192),\n",
       " (('all', '.'), 192),\n",
       " (('such', 'a'), 191),\n",
       " (('see', 'the'), 191),\n",
       " (('to', 'know'), 191),\n",
       " (('is', 'in'), 191),\n",
       " (('that', 'there'), 191),\n",
       " (('SENTENCE_START', 'now'), 190),\n",
       " ((\"'m\", 'a'), 190),\n",
       " (('seems', 'to'), 190),\n",
       " (('``', 'if'), 189),\n",
       " (('you', 'just'), 189),\n",
       " (('she', \"'s\"), 189),\n",
       " ((',', 'my'), 189),\n",
       " ((',', 'do'), 189),\n",
       " (('with', 'you'), 188),\n",
       " (('someone', 'who'), 188),\n",
       " (('you', \"don\\\\'t\"), 188),\n",
       " ((',', 'to'), 187),\n",
       " (('what', 'the'), 187),\n",
       " (('a', 'big'), 187),\n",
       " ((\"''\", \"'\"), 187),\n",
       " (('off', 'the'), 186),\n",
       " (('way', '.'), 185),\n",
       " (('it', 'as'), 185),\n",
       " ((',', 'your'), 185),\n",
       " (('it', 'for'), 185),\n",
       " (('get', 'to'), 184),\n",
       " (('was', 'in'), 183),\n",
       " (('you', 'ca'), 183),\n",
       " (('know', 'that'), 183),\n",
       " (('to', 'give'), 183),\n",
       " (('use', 'the'), 182),\n",
       " (('and', 'get'), 182),\n",
       " ((',', 'for'), 182),\n",
       " (('she', 'was'), 182),\n",
       " (('who', 'are'), 182),\n",
       " (('out', ','), 181),\n",
       " (('they', 'would'), 181),\n",
       " (('it', 'seems'), 181),\n",
       " (('the', 'new'), 180),\n",
       " (('long', 'as'), 180),\n",
       " (('is', 'it'), 179),\n",
       " (('needs', 'to'), 179),\n",
       " (('too', 'much'), 179),\n",
       " (('for', 'that'), 179),\n",
       " (('or', 'something'), 179),\n",
       " (('a', 'while'), 179),\n",
       " (('lots', 'of'), 179),\n",
       " (('after', 'the'), 178),\n",
       " (('you', 'were'), 178),\n",
       " (('the', 'point'), 178),\n",
       " (('what', 'is'), 178),\n",
       " (('up', ','), 178),\n",
       " (('SENTENCE_START', 'how'), 178),\n",
       " (('and', 'my'), 177),\n",
       " (('you', 'for'), 177),\n",
       " (('i', 'agree'), 177),\n",
       " (('the', 'top'), 176),\n",
       " (('or', 'a'), 176),\n",
       " (('he\\\\', \"'s\"), 176),\n",
       " (('rest', 'of'), 176),\n",
       " (('number', 'of'), 176),\n",
       " (('is', 'what'), 175),\n",
       " (('well', 'as'), 175),\n",
       " (('to', 'it'), 175),\n",
       " (('agree', 'with'), 175),\n",
       " (('SENTENCE_START', 'your'), 174),\n",
       " (('as', 'an'), 174),\n",
       " (('each', 'other'), 174),\n",
       " (('on', 'it'), 174),\n",
       " (('time', 'i'), 174),\n",
       " (('here', ','), 174),\n",
       " (('was', 'just'), 174),\n",
       " (('by', 'a'), 174),\n",
       " (('thing', '.'), 173),\n",
       " (('he', 'has'), 173),\n",
       " (('than', 'a'), 173),\n",
       " (('game', ','), 173),\n",
       " (('know', 'how'), 173),\n",
       " (('do', '.'), 171),\n",
       " (('so', ','), 171),\n",
       " (('for', 'example'), 171),\n",
       " (('of', 'all'), 171),\n",
       " (('this', 'was'), 171),\n",
       " (('is', '.'), 171),\n",
       " (('i\\\\', \"'d\"), 170),\n",
       " (('think', 'i'), 170),\n",
       " (('such', 'as'), 169),\n",
       " (('make', 'the'), 169),\n",
       " ((\"'\", '*'), 169),\n",
       " (('SENTENCE_START', 'people'), 169),\n",
       " (('again', '.'), 169),\n",
       " (('a', 'different'), 169),\n",
       " (('one', '.'), 169),\n",
       " (('SENTENCE_START', 'like'), 169),\n",
       " (('in', 'an'), 168),\n",
       " (('SENTENCE_START', 'some'), 168),\n",
       " (('there', ','), 168),\n",
       " (('is', 'pretty'), 168),\n",
       " (('but', 'he'), 168),\n",
       " (('when', 'he'), 167),\n",
       " (('they', 'will'), 167),\n",
       " (('with', 'that'), 167),\n",
       " (('something', 'like'), 167),\n",
       " (('you', 'and'), 167),\n",
       " (('thanks', 'for'), 167),\n",
       " (('having', 'a'), 166),\n",
       " ((',', 'especially'), 166),\n",
       " (('because', 'he'), 166),\n",
       " (('so', 'they'), 166),\n",
       " (('give', 'you'), 166),\n",
       " (('is', 'more'), 166),\n",
       " (('where', 'the'), 166),\n",
       " (('about', 'this'), 165),\n",
       " (('...', \"''\"), 165),\n",
       " (('like', 'i'), 165),\n",
       " (('and', 'there'), 165),\n",
       " (('we', 'were'), 165),\n",
       " ((',', '*'), 165),\n",
       " (('in', 'their'), 164),\n",
       " (('any', 'of'), 164),\n",
       " (('a', 'long'), 164),\n",
       " (('to', 'try'), 164),\n",
       " ((',', 'at'), 164),\n",
       " (('that', 'a'), 164),\n",
       " (('bit', 'of'), 164),\n",
       " ((\"'s\", 'no'), 163),\n",
       " (('the', 'problem'), 163),\n",
       " (('work', '.'), 163),\n",
       " (('people', '.'), 163),\n",
       " (('i', 'also'), 163),\n",
       " (('or', 'the'), 162),\n",
       " ((':', 'i'), 162),\n",
       " (('the', 'question'), 162),\n",
       " (('i', 'hope'), 161),\n",
       " ((\"'\", ','), 161),\n",
       " (('of', 'an'), 161),\n",
       " (('got', 'a'), 161),\n",
       " (('about', 'how'), 161),\n",
       " (('\\\\n', '&'), 161),\n",
       " (('much', 'more'), 161),\n",
       " (('on', '.'), 161),\n",
       " (('the', 'rules'), 160),\n",
       " (('i', 'still'), 160),\n",
       " (('a', 'single'), 160),\n",
       " (('for', 'this'), 160),\n",
       " (('we', 'do'), 160),\n",
       " (('a', 'better'), 160),\n",
       " (('if', 'your'), 160),\n",
       " (('have', 'an'), 160),\n",
       " ((';', 'the'), 160),\n",
       " (('be', 'in'), 159),\n",
       " (('get', 'it'), 159),\n",
       " (('as', 'they'), 159),\n",
       " (('it', 'out'), 159),\n",
       " (('if', 'we'), 158),\n",
       " (('out', 'and'), 158),\n",
       " (('SENTENCE_START', 'at'), 158),\n",
       " (('looks', 'like'), 158),\n",
       " ((\"'m\", 'sure'), 158),\n",
       " (('to', 'pay'), 158),\n",
       " (('if', 'there'), 158),\n",
       " (('end', 'up'), 157),\n",
       " (('in', 'his'), 157),\n",
       " ((',', 'she'), 157),\n",
       " (('and', 'just'), 156),\n",
       " (('it', 'a'), 156),\n",
       " (('\\\\n', \"'\"), 155),\n",
       " (('’', 's'), 155),\n",
       " (('the', 'past'), 155),\n",
       " (('the', 'case'), 155),\n",
       " (('SENTENCE_START', 'to'), 155),\n",
       " (('not', 'have'), 155),\n",
       " (('a', 'year'), 155),\n",
       " (('up', 'in'), 154),\n",
       " (('``', 'no'), 154),\n",
       " (('in', 'order'), 154),\n",
       " (('and', 'do'), 153),\n",
       " (('\\\\n\\\\n\\\\n', '*'), 153),\n",
       " (('it', 'just'), 153),\n",
       " (('has', 'to'), 153),\n",
       " (('could', 'have'), 152),\n",
       " (('with', 'this'), 152),\n",
       " (('SENTENCE_START', 'why'), 151),\n",
       " (('bunch', 'of'), 151),\n",
       " (('far', 'as'), 151),\n",
       " (('the', 'entire'), 151),\n",
       " (('so', 'many'), 151),\n",
       " (('way', ','), 151),\n",
       " (('supposed', 'to'), 151),\n",
       " (('it', 'can'), 150),\n",
       " (('know', ','), 150),\n",
       " (('any', 'other'), 150),\n",
       " (('will', 'not'), 150),\n",
       " (('do', 'the'), 149),\n",
       " (('on', 'that'), 149),\n",
       " (('SENTENCE_START', 'is'), 149),\n",
       " (('\\\\n', \"''\"), 148),\n",
       " (('a', 'bad'), 148),\n",
       " (('to', 'put'), 148),\n",
       " ((\"'s\", 'what'), 148),\n",
       " (('no', ','), 148),\n",
       " (('for', 'my'), 148),\n",
       " (('the', 'idea'), 148),\n",
       " ((',', 'since'), 148),\n",
       " (('must', 'be'), 148),\n",
       " (('you', 'might'), 147),\n",
       " (('years', 'ago'), 147),\n",
       " (('you', \"'d\"), 147),\n",
       " ((',', 'who'), 147),\n",
       " (('did', 'not'), 147),\n",
       " (('think', 'of'), 147),\n",
       " ((\"'s\", 'been'), 147),\n",
       " (('just', 'because'), 146),\n",
       " (('SENTENCE_START', 'https'), 146),\n",
       " (('they', 'did'), 146),\n",
       " (('i', 'believe'), 146),\n",
       " (('is', 'still'), 146),\n",
       " (('and', 'even'), 146),\n",
       " (('other', 'people'), 145),\n",
       " (('a', 'way'), 145),\n",
       " (('up', 'a'), 145),\n",
       " (('a', 'bunch'), 145),\n",
       " (('away', 'from'), 145),\n",
       " (('live', 'in'), 145),\n",
       " ((',', 'maybe'), 145),\n",
       " (('order', 'to'), 145),\n",
       " (('up', 'for'), 145),\n",
       " (('that', 'does'), 145),\n",
       " (('you', 'see'), 145),\n",
       " (('me', 'a'), 145),\n",
       " ((\"n't\", 'see'), 144),\n",
       " (('even', 'though'), 144),\n",
       " (('just', 'like'), 144),\n",
       " (('SENTENCE_START', 'most'), 144),\n",
       " (('who', 'is'), 144),\n",
       " ((\"'ve\", 'never'), 144),\n",
       " (('he', 'had'), 144),\n",
       " (('out', 'there'), 144),\n",
       " (('that', 'can'), 143),\n",
       " (('something', 'that'), 143),\n",
       " (('at', 'this'), 143),\n",
       " (('will', 'have'), 143),\n",
       " (('to', 'look'), 143),\n",
       " (('it', 'up'), 143),\n",
       " (('on', 'their'), 143),\n",
       " (('about', 'a'), 143),\n",
       " (('some', 'people'), 142),\n",
       " ((\"''\", 'is'), 142),\n",
       " (('that', 'this'), 142),\n",
       " (('compared', 'to'), 142),\n",
       " (('*', 'https'), 142),\n",
       " (('you', 'may'), 142),\n",
       " (('SENTENCE_START', \"'you\"), 142),\n",
       " (('what', 'it'), 142),\n",
       " (('good', '.'), 142),\n",
       " (('a', 'game'), 142),\n",
       " (('do', 'that'), 141),\n",
       " (('should', \"n't\"), 141),\n",
       " (('are', 'just'), 141),\n",
       " (('of', 'our'), 141),\n",
       " (('the', 'main'), 141),\n",
       " (('as', 'long'), 141),\n",
       " ((\"'ve\", 'seen'), 141),\n",
       " (('people', 'to'), 140),\n",
       " (('years', '.'), 140),\n",
       " (('could', \"n't\"), 140),\n",
       " (('i\\\\', \"'ll\"), 140),\n",
       " (('to', 'buy'), 140),\n",
       " (('of', 'us'), 140),\n",
       " (('all', ','), 140),\n",
       " (('...', 'i'), 140),\n",
       " (('him', ','), 140),\n",
       " (('people', ','), 139),\n",
       " (('that', 'if'), 139),\n",
       " (('for', 'some'), 139),\n",
       " (('how', 'it'), 139),\n",
       " (('and', 'in'), 139),\n",
       " (('a', 'small'), 139),\n",
       " (('to', 'start'), 139),\n",
       " (('to', 'not'), 138),\n",
       " (('to', 'think'), 138),\n",
       " (('many', 'people'), 138),\n",
       " (('100', '%'), 138),\n",
       " (('of', 'you'), 138),\n",
       " (('talk', 'about'), 138),\n",
       " (('that', 'will'), 138),\n",
       " (('with', 'them'), 137),\n",
       " (('it', 'should'), 137),\n",
       " (('to', 'that'), 137),\n",
       " (('it', 'with'), 137),\n",
       " (('that', 'people'), 137),\n",
       " (('and', 'all'), 136),\n",
       " (('he', 'did'), 136),\n",
       " (('up', 'on'), 136),\n",
       " (('it', 'on'), 136),\n",
       " (('top', 'of'), 136),\n",
       " ((\"n't\", 'like'), 136),\n",
       " (('most', 'people'), 135),\n",
       " ((',', 'while'), 135),\n",
       " (('been', 'a'), 135),\n",
       " (('can', 'see'), 135),\n",
       " ((\"n't\", 'make'), 135),\n",
       " (('you', 'did'), 135),\n",
       " (('the', 'guy'), 135),\n",
       " (('you', 'a'), 135),\n",
       " (('other', 'than'), 135),\n",
       " (('time', 'and'), 135),\n",
       " ((\"'\", \"''\"), 135),\n",
       " (('SENTENCE_START', \"'this\"), 135),\n",
       " (('couple', 'of'), 135),\n",
       " (('tend', 'to'), 135),\n",
       " (('of', '``'), 134),\n",
       " ((',', 'all'), 134),\n",
       " (('been', 'removed'), 134),\n",
       " (('out', 'to'), 134),\n",
       " (('wants', 'to'), 134),\n",
       " (('deal', 'with'), 134),\n",
       " (('not', 'that'), 134),\n",
       " (('willing', 'to'), 134),\n",
       " (('read', 'the'), 134),\n",
       " (('free', 'to'), 134),\n",
       " (('much', 'as'), 134),\n",
       " (('to', 'tell'), 133),\n",
       " (('how', 'you'), 133),\n",
       " (('which', 'i'), 133),\n",
       " ((':', 'http'), 133),\n",
       " (('my', 'own'), 133),\n",
       " (('for', 'them'), 133),\n",
       " (('find', 'a'), 133),\n",
       " (('a', 'whole'), 133),\n",
       " (('are', 'in'), 133),\n",
       " (('\\\\n', '^'), 133),\n",
       " (('the', 'more'), 133),\n",
       " (('him', 'to'), 133),\n",
       " (('this', 'thread'), 133),\n",
       " (('not', 'going'), 132),\n",
       " (('SENTENCE_START', '\\\\n\\\\nif'), 132),\n",
       " (('if', 'that'), 132),\n",
       " (('not', '.'), 132),\n",
       " (('SENTENCE_START', 'thanks'), 132),\n",
       " (('easy', 'to'), 132),\n",
       " (('since', 'i'), 132),\n",
       " (('of', 'time'), 132),\n",
       " (('and', 'your'), 131),\n",
       " (('down', 'to'), 131),\n",
       " (('the', 'original'), 131),\n",
       " (('should', 'have'), 131),\n",
       " (('and', 'other'), 131),\n",
       " (('people', 'in'), 131),\n",
       " (('know', 'the'), 131),\n",
       " (('a', 'problem'), 131),\n",
       " (('to', 'an'), 130),\n",
       " (('*', 'the'), 130),\n",
       " ((\"don\\\\'t\", 'know'), 130),\n",
       " (('it', '?'), 130),\n",
       " (('he', 'would'), 130),\n",
       " (('is', 'why'), 129),\n",
       " (('before', 'the'), 129),\n",
       " (('but', 'there'), 129),\n",
       " (('SENTENCE_START', 'because'), 129),\n",
       " (('may', 'not'), 129),\n",
       " (('it', 'comes'), 129),\n",
       " (('day', '.'), 129),\n",
       " (('thing', 'is'), 129),\n",
       " (('i', 'used'), 129),\n",
       " (('less', 'than'), 129),\n",
       " (('unless', 'you'), 129),\n",
       " (('where', 'i'), 129),\n",
       " (('with', 'an'), 129),\n",
       " ((\"'ve\", 'got'), 129),\n",
       " ((\"'s\", 'like'), 129),\n",
       " (('seems', 'like'), 128),\n",
       " (('why', 'i'), 128),\n",
       " (('is', 'one'), 128),\n",
       " (('me', 'and'), 128),\n",
       " (('is', 'very'), 128),\n",
       " (('it', 'looks'), 128),\n",
       " (('use', 'it'), 128),\n",
       " (('and', 'what'), 128),\n",
       " (('mean', ','), 127),\n",
       " (('what', 'he'), 127),\n",
       " ((\"'ve\", 'had'), 127),\n",
       " (('he', 'does'), 127),\n",
       " (('be', 'more'), 127),\n",
       " (('be', 'an'), 127),\n",
       " (('to', 'come'), 127),\n",
       " (('in', 'mind'), 127),\n",
       " (('around', 'the'), 127),\n",
       " (('i', 'saw'), 127),\n",
       " (('i', 'went'), 127),\n",
       " ...]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking the first 20,000 entries as our vocabulary\n",
    "sorted(b.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c225bafb-ae40-4a0e-afdb-8142ec672457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\\\n\\\\ni\\\\',\n",
       " 'any',\n",
       " 'election',\n",
       " 'weight',\n",
       " 'extent',\n",
       " 'exists',\n",
       " 'complaint',\n",
       " 'pretty',\n",
       " 'altogether',\n",
       " 'hung',\n",
       " 'obsessed',\n",
       " 'committed',\n",
       " 'leaders',\n",
       " 'peace',\n",
       " 'suggests',\n",
       " \"'my\",\n",
       " 'different',\n",
       " 'form',\n",
       " 'chicken',\n",
       " '//www.reddit.com/r/askscience/search',\n",
       " 'anything',\n",
       " 'returned',\n",
       " 'region',\n",
       " 'tired',\n",
       " 'myself',\n",
       " 'native',\n",
       " '82',\n",
       " 'wont',\n",
       " 'submitter',\n",
       " 'locked',\n",
       " 'has',\n",
       " 'width',\n",
       " 'bowl',\n",
       " 'brother',\n",
       " 'reason',\n",
       " 'concluded',\n",
       " 'research',\n",
       " 'breaches',\n",
       " 'dig',\n",
       " 'involved',\n",
       " 'yup',\n",
       " 'finally',\n",
       " 'hundreds',\n",
       " 'chose',\n",
       " 'happy',\n",
       " 'frequency',\n",
       " 'criticism',\n",
       " 'as',\n",
       " 'nope',\n",
       " 'projects',\n",
       " 'invest',\n",
       " 'holding',\n",
       " 'your',\n",
       " 'gcxrep',\n",
       " 'scenario',\n",
       " 'must',\n",
       " 'message=myreminders',\n",
       " 'c3',\n",
       " '//www.reddit.com/r/havoc_bot/wiki/',\n",
       " 'box',\n",
       " '//0fs.me/yis5stledr\\\\n\\\\nhttp',\n",
       " 'raw',\n",
       " 'plot',\n",
       " 'interaction',\n",
       " 'roughly',\n",
       " '20a',\n",
       " 'likely',\n",
       " 'mistakes',\n",
       " '8gb',\n",
       " 'honor',\n",
       " 'privilege',\n",
       " 'sense',\n",
       " 'appreciated',\n",
       " 'medication',\n",
       " 'jail',\n",
       " 'sick',\n",
       " 'representation',\n",
       " 'sneak',\n",
       " 'according',\n",
       " 'supporting',\n",
       " 'england',\n",
       " 'staff',\n",
       " 'solve',\n",
       " 'square',\n",
       " 'threw',\n",
       " 'couch',\n",
       " 'absolute',\n",
       " 'july',\n",
       " 'cards',\n",
       " 'indicate',\n",
       " 'hl',\n",
       " 'pavement',\n",
       " 'portion',\n",
       " 'insurance',\n",
       " '^code',\n",
       " 'unless',\n",
       " 'former',\n",
       " 'http',\n",
       " '3rd',\n",
       " 'again.\\\\n\\\\n',\n",
       " 'voted',\n",
       " 'debt',\n",
       " 'suffering',\n",
       " 'cares',\n",
       " 'discovered',\n",
       " '\\\\n\\\\na',\n",
       " 'question',\n",
       " 'funniest',\n",
       " 'woman\\\\',\n",
       " 'claim',\n",
       " 'seconds',\n",
       " 'directed',\n",
       " '70',\n",
       " 'vs',\n",
       " 'cheese',\n",
       " 'makes',\n",
       " 'there\\\\',\n",
       " '\\\\n\\\\nmy',\n",
       " 'addiction',\n",
       " 'suck',\n",
       " 'self-promotion',\n",
       " 'healthy',\n",
       " 'computer',\n",
       " 'performs',\n",
       " 'in.',\n",
       " '\\\\n\\\\nsecond',\n",
       " 'bothers',\n",
       " 'prices',\n",
       " 'lawyer',\n",
       " 'label',\n",
       " 'via',\n",
       " 'changing',\n",
       " 'refers',\n",
       " 'voice',\n",
       " 'q=title',\n",
       " 'relating',\n",
       " 'assist',\n",
       " 'therapist',\n",
       " 'crime',\n",
       " 'youtube',\n",
       " 'real',\n",
       " 'explain',\n",
       " 'themselves',\n",
       " 'paid',\n",
       " 'r',\n",
       " 'pretending',\n",
       " 'last',\n",
       " 'signed',\n",
       " '\\\\n\\\\nwhich',\n",
       " 'posting',\n",
       " \"'when\",\n",
       " 'through',\n",
       " 'wheels',\n",
       " 'supports',\n",
       " 'benner',\n",
       " 'cases',\n",
       " 'upgraded',\n",
       " 'apples',\n",
       " 'determined',\n",
       " 'apartment',\n",
       " 'longer',\n",
       " 'tgt',\n",
       " 'once',\n",
       " 'colors',\n",
       " 'clicking',\n",
       " 'rift',\n",
       " 'happens',\n",
       " 'excuse',\n",
       " '500',\n",
       " 'exploit',\n",
       " 'tools',\n",
       " 'encouraged',\n",
       " 'omg',\n",
       " 'losing',\n",
       " 'so',\n",
       " \"'yes\",\n",
       " 'harder',\n",
       " 'slowing',\n",
       " 'winter',\n",
       " 'SENTENCE_END',\n",
       " 'keep',\n",
       " 'gear',\n",
       " 'angel',\n",
       " 'statement',\n",
       " 'peers',\n",
       " '\\\\n\\\\nyeah',\n",
       " 'argued',\n",
       " 'tries',\n",
       " 'reconcile',\n",
       " 'puns',\n",
       " 'career',\n",
       " 'repost',\n",
       " 'talking',\n",
       " 'voting.\\\\n\\\\nplease',\n",
       " '/message/compose/',\n",
       " 'learn',\n",
       " 'infrastructure',\n",
       " '^^^bot',\n",
       " 'courts',\n",
       " '30',\n",
       " 'mana',\n",
       " 'tagging',\n",
       " '^^^have',\n",
       " 'helicopter',\n",
       " '\\\\nedit',\n",
       " 'met',\n",
       " 'women\\\\',\n",
       " 'u.s.',\n",
       " 'outcome',\n",
       " 'to=/r/fireteams',\n",
       " 'pills',\n",
       " 'apple',\n",
       " 'lt',\n",
       " 'climate',\n",
       " '\\\\\\\\n',\n",
       " 'extension',\n",
       " 'subject',\n",
       " 'concrete',\n",
       " 'steam',\n",
       " 'i+4',\n",
       " 'define',\n",
       " '^^',\n",
       " 'center',\n",
       " 'linked',\n",
       " 'spirit',\n",
       " 'distinguish',\n",
       " 'arm',\n",
       " '6',\n",
       " 'arguing',\n",
       " 'recently',\n",
       " 'naturally',\n",
       " 'violated',\n",
       " 'fairness',\n",
       " 'supposed',\n",
       " 'beings',\n",
       " 'subsequent',\n",
       " 'actually',\n",
       " 'fired',\n",
       " 'stadium',\n",
       " 'buttons',\n",
       " 'clear',\n",
       " 'policy',\n",
       " 'to=/r/friendsafari',\n",
       " '90',\n",
       " 'sun',\n",
       " 'jungle',\n",
       " 'sources',\n",
       " 'consult',\n",
       " 'segment',\n",
       " 'how',\n",
       " 'to=',\n",
       " 'section',\n",
       " 'samsung',\n",
       " 'largely',\n",
       " 'visual',\n",
       " 'reading',\n",
       " 'them.\\\\n\\\\nwe',\n",
       " 'be',\n",
       " 'value',\n",
       " 'submitting',\n",
       " 'saiyan',\n",
       " 'compensate',\n",
       " 'mid',\n",
       " 'site=search',\n",
       " 'consisting',\n",
       " 'interact',\n",
       " 'log',\n",
       " '\\\\nin',\n",
       " 'conclude',\n",
       " 'exhaust',\n",
       " 'killing',\n",
       " 'turns',\n",
       " 'atx',\n",
       " 'motherboard',\n",
       " \"can\\\\'t\",\n",
       " 'that',\n",
       " 'resort',\n",
       " 'playerbase',\n",
       " 'ugh',\n",
       " '\\\\n\\\\nhere',\n",
       " 'pero',\n",
       " 'ult',\n",
       " \"'first\",\n",
       " 'was',\n",
       " 'shows',\n",
       " 'fps',\n",
       " 'to=/r/videos',\n",
       " 'lean',\n",
       " 'hero',\n",
       " '27',\n",
       " 'bad',\n",
       " \"'hmm\",\n",
       " 'distance',\n",
       " '\\\\nblog',\n",
       " 'needed',\n",
       " 'big',\n",
       " 'smelled',\n",
       " 'meet',\n",
       " 'automatic',\n",
       " 'jumping',\n",
       " 'may',\n",
       " 'albeit',\n",
       " '^^^feedback',\n",
       " 'demo',\n",
       " 'tried',\n",
       " 'age',\n",
       " 'beginning',\n",
       " 'dollars',\n",
       " '\\\\n\\\\n\\\\t',\n",
       " 'rear',\n",
       " 'sanders',\n",
       " 'minutes',\n",
       " '//friendsafaribot.tumblr.com/',\n",
       " 'landlord',\n",
       " 'minority',\n",
       " 'does',\n",
       " 'separate',\n",
       " 'laser',\n",
       " 'northern',\n",
       " 'had',\n",
       " 'complaining',\n",
       " 'intelligent',\n",
       " 'abide',\n",
       " 'pronouns',\n",
       " 'imdb',\n",
       " 'yourself',\n",
       " '^^^if',\n",
       " \"'hello\",\n",
       " '//redd.it/s4chc',\n",
       " 'pack',\n",
       " '\\\\n\\\\ndo',\n",
       " 'help',\n",
       " 'amp',\n",
       " 'tracks',\n",
       " 'orange',\n",
       " 'nurses',\n",
       " 'read',\n",
       " 'bound',\n",
       " 'revealed',\n",
       " 'ok.',\n",
       " '\\\\n-',\n",
       " 'theirs',\n",
       " '29',\n",
       " 'tip',\n",
       " 'condition',\n",
       " 'media',\n",
       " 'traditional',\n",
       " 'closest',\n",
       " 'complete',\n",
       " 'concerned',\n",
       " 'pursuit',\n",
       " '^^i',\n",
       " 'protection',\n",
       " 'vegan',\n",
       " 'lifetime',\n",
       " 'assume',\n",
       " 'humanity',\n",
       " 'somewhere',\n",
       " 'issues',\n",
       " 'gigs',\n",
       " 'beans',\n",
       " 'shower',\n",
       " '960',\n",
       " 'taste',\n",
       " 'poll',\n",
       " 'checked',\n",
       " 'contrary',\n",
       " 'hope',\n",
       " 'aid',\n",
       " 'per',\n",
       " '3f',\n",
       " 'se',\n",
       " 'area',\n",
       " 'unhealthy',\n",
       " 'click',\n",
       " 'regardless',\n",
       " 'doctor.\\\\n',\n",
       " 'marrow',\n",
       " 'working',\n",
       " 'associated',\n",
       " 'roast',\n",
       " 'bs',\n",
       " 'transportation',\n",
       " 'hard\\\\n\\\\n\\\\\\\\',\n",
       " 'feed',\n",
       " 'her',\n",
       " 'pvp',\n",
       " 'magic',\n",
       " \"'astronomy\",\n",
       " 'near',\n",
       " 'gospel',\n",
       " 'able',\n",
       " 'bb',\n",
       " 'minimize',\n",
       " 'trusted',\n",
       " 'playlist',\n",
       " 'addict',\n",
       " 'plans',\n",
       " 'vidilux',\n",
       " 'posts',\n",
       " 'settings',\n",
       " 'commenter',\n",
       " 'screen',\n",
       " 'tempted',\n",
       " 'gt',\n",
       " 'merely',\n",
       " '\\\\n^^^',\n",
       " 'backup',\n",
       " 'us',\n",
       " 'ultimately',\n",
       " '\\\\n\\\\nhave',\n",
       " 'based',\n",
       " 'participating',\n",
       " 'tuesday',\n",
       " 'gain',\n",
       " 'awful',\n",
       " 'perfectly',\n",
       " 'victims',\n",
       " 'captain',\n",
       " 'indicator',\n",
       " 'dm',\n",
       " 'sin',\n",
       " 'continue',\n",
       " 'obtain',\n",
       " 'taken',\n",
       " 'stone',\n",
       " 'tanks',\n",
       " 'equally',\n",
       " 'liberal',\n",
       " 'restrict_sr=',\n",
       " 'processor',\n",
       " 'fill',\n",
       " 'coast',\n",
       " 'united',\n",
       " 'proper',\n",
       " 'money.',\n",
       " 'weak',\n",
       " 'shovel',\n",
       " 'oil',\n",
       " 'listed',\n",
       " 'learned',\n",
       " 'around',\n",
       " 'outside',\n",
       " 'regarding',\n",
       " 'walked',\n",
       " 'please',\n",
       " 'dev',\n",
       " 'nzxt',\n",
       " 'figures',\n",
       " 'north',\n",
       " 'kinda',\n",
       " 'message=my',\n",
       " '0aplease',\n",
       " 'emphasis',\n",
       " 'know',\n",
       " 'replaced',\n",
       " 'takes',\n",
       " 'shed',\n",
       " 'caught',\n",
       " 'synergism',\n",
       " 'madison',\n",
       " 'counting',\n",
       " 'explicit',\n",
       " 'sale',\n",
       " '\\\\n\\\\nwith',\n",
       " 'alcohol',\n",
       " 'hillary',\n",
       " 'peak',\n",
       " 'problem',\n",
       " 'unknown',\n",
       " 'specified',\n",
       " 'users',\n",
       " 'evo-series',\n",
       " 'mass',\n",
       " 'to=jasie3k',\n",
       " 'ashley',\n",
       " '2014',\n",
       " 'character',\n",
       " 'suspect',\n",
       " 'distinct',\n",
       " 'comments',\n",
       " 'nor',\n",
       " '-\\\\n\\\\n',\n",
       " 'fastest',\n",
       " 'entirely',\n",
       " 'p',\n",
       " 'anywhere',\n",
       " \"'agreed\",\n",
       " 'let',\n",
       " 'national',\n",
       " 'upon',\n",
       " 'paint',\n",
       " 'comparison',\n",
       " 'till',\n",
       " 'searing',\n",
       " 'lowest',\n",
       " 'textbox',\n",
       " 'wiki_vague_titles',\n",
       " 'unrelated',\n",
       " \"'one\",\n",
       " 'mal',\n",
       " 'destroys',\n",
       " 'selftext=true',\n",
       " 'blue',\n",
       " 'silver',\n",
       " 'ago',\n",
       " 'thank',\n",
       " 'showing',\n",
       " 'eventually',\n",
       " 'saying',\n",
       " 'models',\n",
       " 'steam\\\\n',\n",
       " 'comfortable',\n",
       " 'suggesting',\n",
       " 'answered',\n",
       " 'mental',\n",
       " 'tablet',\n",
       " 'wound',\n",
       " 'mage',\n",
       " 'starts',\n",
       " 'evga',\n",
       " 'nervous',\n",
       " 'abandoned',\n",
       " '970',\n",
       " 'drawing',\n",
       " 'john',\n",
       " '75',\n",
       " '20to',\n",
       " 'wash',\n",
       " 'height',\n",
       " \"'any\",\n",
       " \"'what\",\n",
       " 'loud',\n",
       " 'powerful',\n",
       " 'storm',\n",
       " 'touch',\n",
       " 'covers',\n",
       " 'candidate',\n",
       " 'basic',\n",
       " 'ensure',\n",
       " 'physical',\n",
       " 'owned',\n",
       " 'q=',\n",
       " 'blow',\n",
       " '20hedron',\n",
       " '‘',\n",
       " 'stands',\n",
       " 'review',\n",
       " '\\\\nthere',\n",
       " 'franchise',\n",
       " 'eliminate',\n",
       " 'percentage',\n",
       " 'model',\n",
       " \"'how\",\n",
       " 'location',\n",
       " '/scrunchieshy-intensifies',\n",
       " 'halfway',\n",
       " 'ikke',\n",
       " 'artist',\n",
       " 'information',\n",
       " 'ocean',\n",
       " 'stronger',\n",
       " 'edition',\n",
       " 'realized',\n",
       " 'male',\n",
       " 'subject=list',\n",
       " 'lights',\n",
       " 'gop',\n",
       " 'willingness',\n",
       " '-\\\\n\\\\n\\\\n',\n",
       " 'to=/r/pokemongiveaway',\n",
       " 'june',\n",
       " 'they\\\\',\n",
       " 'sorry',\n",
       " 'press',\n",
       " 'return',\n",
       " 'break',\n",
       " 'everyday',\n",
       " 'mini',\n",
       " 'ncix',\n",
       " 'pid=cffd2',\n",
       " 'unique',\n",
       " 'lock',\n",
       " 'strength',\n",
       " 'man\\\\',\n",
       " 'response',\n",
       " 'agenda',\n",
       " '^message',\n",
       " '//www.tineye.com/search',\n",
       " 'council',\n",
       " 'below',\n",
       " 'heart',\n",
       " 'davis',\n",
       " 'experimenting',\n",
       " 'back',\n",
       " 'to=/r/askreddit',\n",
       " 'subject=feedback',\n",
       " 'to=hearthscan-bot',\n",
       " 'still',\n",
       " \"'sure\",\n",
       " 'extend',\n",
       " 'type=card',\n",
       " 'success',\n",
       " 'craigslist',\n",
       " 'subject=rsgspolice',\n",
       " '\\\\n\\\\nhowever',\n",
       " \"'which\",\n",
       " 'iraq',\n",
       " 'sidebar',\n",
       " 'hopefully',\n",
       " 'investigate',\n",
       " '\\\\n\\\\non',\n",
       " '2013',\n",
       " 'needless',\n",
       " '-feat',\n",
       " '2015-08-28',\n",
       " 'swear',\n",
       " 'sent',\n",
       " 'remind',\n",
       " \"'lol\",\n",
       " 'stock',\n",
       " 'herself',\n",
       " 'affects',\n",
       " 'dark',\n",
       " 'usual',\n",
       " 'complexity',\n",
       " 'sign',\n",
       " 'complicated',\n",
       " 'replies',\n",
       " '/intensifies',\n",
       " 'metal',\n",
       " 'trapped',\n",
       " 'lose',\n",
       " 'pcpartpicker',\n",
       " '\\\\ncurrent|',\n",
       " 'criticize',\n",
       " '___\\\\n\\\\n',\n",
       " 'syntax=cloudsearch',\n",
       " 'action',\n",
       " '..',\n",
       " 'screenshot',\n",
       " 'mountain',\n",
       " 'blown',\n",
       " 'lost',\n",
       " 'in',\n",
       " 'existing',\n",
       " 'offering',\n",
       " 'raising',\n",
       " \"'do\",\n",
       " 'economic',\n",
       " 'addicted',\n",
       " 'kill',\n",
       " 'guide',\n",
       " 'available',\n",
       " 'rehabilitation',\n",
       " 'diet',\n",
       " 'point',\n",
       " 'drink',\n",
       " \"'while\",\n",
       " 'this',\n",
       " 'waste',\n",
       " 'mess',\n",
       " 'freak',\n",
       " 'incentive',\n",
       " 'received',\n",
       " '//gatherer.wizards.com/pages/card/details.aspx',\n",
       " 'train',\n",
       " 'fell',\n",
       " 'top-level',\n",
       " 'trust',\n",
       " 'catfish',\n",
       " 'resulted',\n",
       " 'wiki_askscience_user_help_page',\n",
       " 'shall',\n",
       " 'implemented',\n",
       " 'subject=mirrorportal',\n",
       " \"'also\",\n",
       " 'appearance',\n",
       " 'ships',\n",
       " 'hurts',\n",
       " 'rule',\n",
       " 'domain',\n",
       " 'hours',\n",
       " 'encourage',\n",
       " 'crown',\n",
       " '20thread',\n",
       " 'roles',\n",
       " 'complex',\n",
       " 'lasts',\n",
       " 'wealthy',\n",
       " 'sell',\n",
       " 'training',\n",
       " 'trolling',\n",
       " 'instance',\n",
       " 'when',\n",
       " 'rp',\n",
       " 'person\\\\',\n",
       " 'computers',\n",
       " 'somebody',\n",
       " 'implement',\n",
       " 'whether',\n",
       " 'offer',\n",
       " 'constantly',\n",
       " 'actively',\n",
       " 'skeleton',\n",
       " 'monday',\n",
       " 'truly',\n",
       " 'brackets',\n",
       " 'beliefs',\n",
       " 'education',\n",
       " 'criminal',\n",
       " 'hear',\n",
       " 'eating',\n",
       " 'bag',\n",
       " 'employees',\n",
       " 'favs',\n",
       " 'consist',\n",
       " 'detection',\n",
       " 'submit',\n",
       " 'blood',\n",
       " '\\\\n\\\\nhis',\n",
       " '//www.google.com/searchbyimage',\n",
       " 'lives',\n",
       " '3a',\n",
       " 'hearing',\n",
       " \"don\\\\'t\",\n",
       " 'respond',\n",
       " 'ufc',\n",
       " 'parody',\n",
       " 'god',\n",
       " 'feeling',\n",
       " '//www.modojo.com/features/nintendo_3ds_friend_codes_everything_you_need_to_know',\n",
       " 'cross',\n",
       " 'hire',\n",
       " 'nbsp',\n",
       " 'paragraph',\n",
       " 'overview',\n",
       " 'super',\n",
       " '//www.reddit.com/r/askscience/wiki/index',\n",
       " 'importantly',\n",
       " 'citizens',\n",
       " '£1.99',\n",
       " 'prepared',\n",
       " 'to=/r/totesmessenger',\n",
       " 'insecure',\n",
       " 'affect',\n",
       " 'denied',\n",
       " 'imply',\n",
       " 'simple',\n",
       " 'whole',\n",
       " 'trade',\n",
       " '^tell',\n",
       " 'cars',\n",
       " 'kurt',\n",
       " 'did',\n",
       " 'disagrees',\n",
       " 'roastee',\n",
       " 'awesome',\n",
       " 'schedule',\n",
       " 'mc',\n",
       " 'rock',\n",
       " 'its',\n",
       " 'frankly',\n",
       " 'grass',\n",
       " 'times',\n",
       " 'implying',\n",
       " 'gpu',\n",
       " 'chase',\n",
       " 'travel',\n",
       " 'first',\n",
       " 'verify',\n",
       " 'week',\n",
       " 'forms',\n",
       " 'SENTENCE_START',\n",
       " 'scenes',\n",
       " 'ending\\\\n\\\\n',\n",
       " 'subject=reminder',\n",
       " 'function',\n",
       " 'map',\n",
       " 'owners',\n",
       " 'prime',\n",
       " 'issue',\n",
       " 'warrant',\n",
       " 'surrounded',\n",
       " 'subreddits',\n",
       " 'cost',\n",
       " 'wing',\n",
       " 'canon',\n",
       " 'searched',\n",
       " 'tests',\n",
       " 'inspired',\n",
       " '//reddit.com/r/randomactsofblowjob/search',\n",
       " 'higher',\n",
       " 'restrictions',\n",
       " '^^a',\n",
       " 'early',\n",
       " 'to=/r/ps4',\n",
       " '2008',\n",
       " 'pocket',\n",
       " 'pointless',\n",
       " 'cruz',\n",
       " 'fully-modular',\n",
       " 'key',\n",
       " 'birth',\n",
       " 'trash',\n",
       " 'cents',\n",
       " 'space',\n",
       " 'home',\n",
       " 'guilt',\n",
       " '*',\n",
       " 'cake',\n",
       " 'ssd',\n",
       " 'playing',\n",
       " 'intent',\n",
       " 'grown',\n",
       " 'gon',\n",
       " 'individual',\n",
       " 'curious',\n",
       " \"'here\",\n",
       " 'practice',\n",
       " 'on\\\\nhttp',\n",
       " '//github.com/silver',\n",
       " 'honesty',\n",
       " 'filled',\n",
       " 'developers',\n",
       " '^i',\n",
       " 'already',\n",
       " 'wind',\n",
       " 'relying',\n",
       " 'shirt',\n",
       " \"'anthropology\",\n",
       " 'woke',\n",
       " 'contrast',\n",
       " 'shots',\n",
       " 'fellow',\n",
       " 'server',\n",
       " 'system/console',\n",
       " 'forward',\n",
       " 'pile',\n",
       " 'lined',\n",
       " 's',\n",
       " '\\\\ni',\n",
       " 'messing',\n",
       " '850',\n",
       " 'bomb',\n",
       " 'give',\n",
       " 'patent',\n",
       " 'diamond',\n",
       " 'previous',\n",
       " 'ass',\n",
       " 'is',\n",
       " 'haki',\n",
       " 'assets',\n",
       " 'it.\\\\n\\\\n5',\n",
       " 'prisons',\n",
       " 'oldest',\n",
       " 'buying',\n",
       " 'spending',\n",
       " 'needs',\n",
       " 'crash',\n",
       " 'plastic',\n",
       " 'involves',\n",
       " 'building',\n",
       " 'hold',\n",
       " 'choices',\n",
       " 'q',\n",
       " 'glad',\n",
       " 'yeah',\n",
       " 'trolls',\n",
       " 'third',\n",
       " '//twitter.com/search',\n",
       " 'honestly',\n",
       " 'result',\n",
       " 'marketing',\n",
       " '\\\\n\\\\nmost',\n",
       " 'desperate',\n",
       " 'fairly',\n",
       " 'companies',\n",
       " 'join',\n",
       " 'learning',\n",
       " 'save',\n",
       " 'fire',\n",
       " 'to=havoc_bot',\n",
       " 'kit',\n",
       " 'weigh',\n",
       " 'cc',\n",
       " 'h',\n",
       " 'mysterious',\n",
       " 'same',\n",
       " 'gigabyte',\n",
       " 'society',\n",
       " 'range',\n",
       " 'ingredients',\n",
       " 'complaints',\n",
       " 'explanation',\n",
       " 'toward',\n",
       " 'occasion',\n",
       " 'direction',\n",
       " '4th',\n",
       " 'mentioned',\n",
       " \"'paleontology\",\n",
       " 'chris',\n",
       " 'disclosure',\n",
       " 'advice',\n",
       " 'remain',\n",
       " 'confirmed',\n",
       " 'involving',\n",
       " 'requiring',\n",
       " 'channel',\n",
       " 'worried',\n",
       " 'coming',\n",
       " \"'absolutely\",\n",
       " 'reward',\n",
       " 'respective',\n",
       " 'indicating',\n",
       " 'lay',\n",
       " 'production',\n",
       " 'retarded',\n",
       " 'being',\n",
       " 'end',\n",
       " 'updated',\n",
       " 'figuring',\n",
       " \"'hey\",\n",
       " 'hurt',\n",
       " 'looking',\n",
       " '/r/mylittlepony/comments/1lwzub/deviantart_imgur_mirror_bot_nightmirrormoon/',\n",
       " \"'m\",\n",
       " 'code',\n",
       " 'and/or',\n",
       " '\\\\',\n",
       " 'mail',\n",
       " 'safety',\n",
       " 'drawn',\n",
       " 'treat',\n",
       " \"'sounds\",\n",
       " 'basically',\n",
       " 'expectation',\n",
       " 'policies',\n",
       " 'worry',\n",
       " 'looked',\n",
       " 'possible.',\n",
       " 'e.g',\n",
       " 'bullet',\n",
       " 'america',\n",
       " 'possibility',\n",
       " 'deaths',\n",
       " 'household',\n",
       " 'unfair',\n",
       " 'chart',\n",
       " 'inch',\n",
       " 'shoulder',\n",
       " 'mint',\n",
       " 'club',\n",
       " 'democratic',\n",
       " 'manner',\n",
       " '28http',\n",
       " 'waters',\n",
       " 'suppose',\n",
       " 'parent',\n",
       " 'creator',\n",
       " 'equal',\n",
       " 'phase',\n",
       " \"'for\",\n",
       " 'memory',\n",
       " 'gamer',\n",
       " 'medical',\n",
       " 'chains',\n",
       " 'gut',\n",
       " 'supported',\n",
       " '7200rpm',\n",
       " 'slots',\n",
       " 'made',\n",
       " 'luck',\n",
       " 'lo',\n",
       " 'maximize',\n",
       " 'servers',\n",
       " 'coffee',\n",
       " '27s',\n",
       " 'aye',\n",
       " '^^^or',\n",
       " 'million',\n",
       " 'reveal',\n",
       " '\\\\n1',\n",
       " 'loose',\n",
       " '^^^/u/fursec',\n",
       " 'dangerous',\n",
       " 'starting',\n",
       " 'irrelevant',\n",
       " 'assumption',\n",
       " 'larger',\n",
       " 'curiosity',\n",
       " 'mecha',\n",
       " 'broken',\n",
       " 'bars',\n",
       " 'instrument',\n",
       " 'removed',\n",
       " 'seem',\n",
       " 'anyone',\n",
       " \"isn\\\\'t\",\n",
       " 'shortly',\n",
       " ...}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_set = set(word for bigram, count in sorted(b.items(), key=lambda kv: -kv[1])[:30000] for word in bigram)\n",
    "word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "29b5fb9b-de16-43f6-ae5c-09c91a61ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5138"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = len(word_set)\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7db1305a-f192-45ff-93ef-475d0863dca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\\\n\\\\ni\\\\': 0,\n",
       " 'any': 1,\n",
       " 'election': 2,\n",
       " 'weight': 3,\n",
       " 'extent': 4,\n",
       " 'exists': 5,\n",
       " 'complaint': 6,\n",
       " 'pretty': 7,\n",
       " 'altogether': 8,\n",
       " 'hung': 9,\n",
       " 'obsessed': 10,\n",
       " 'committed': 11,\n",
       " 'leaders': 12,\n",
       " 'peace': 13,\n",
       " 'suggests': 14,\n",
       " \"'my\": 15,\n",
       " 'different': 16,\n",
       " 'form': 17,\n",
       " 'chicken': 18,\n",
       " '//www.reddit.com/r/askscience/search': 19,\n",
       " 'anything': 20,\n",
       " 'returned': 21,\n",
       " 'region': 22,\n",
       " 'tired': 23,\n",
       " 'myself': 24,\n",
       " 'native': 25,\n",
       " '82': 26,\n",
       " 'wont': 27,\n",
       " 'submitter': 28,\n",
       " 'locked': 29,\n",
       " 'has': 30,\n",
       " 'width': 31,\n",
       " 'bowl': 32,\n",
       " 'brother': 33,\n",
       " 'reason': 34,\n",
       " 'concluded': 35,\n",
       " 'research': 36,\n",
       " 'breaches': 37,\n",
       " 'dig': 38,\n",
       " 'involved': 39,\n",
       " 'yup': 40,\n",
       " 'finally': 41,\n",
       " 'hundreds': 42,\n",
       " 'chose': 43,\n",
       " 'happy': 44,\n",
       " 'frequency': 45,\n",
       " 'criticism': 46,\n",
       " 'as': 47,\n",
       " 'nope': 48,\n",
       " 'projects': 49,\n",
       " 'invest': 50,\n",
       " 'holding': 51,\n",
       " 'your': 52,\n",
       " 'gcxrep': 53,\n",
       " 'scenario': 54,\n",
       " 'must': 55,\n",
       " 'message=myreminders': 56,\n",
       " 'c3': 57,\n",
       " '//www.reddit.com/r/havoc_bot/wiki/': 58,\n",
       " 'box': 59,\n",
       " '//0fs.me/yis5stledr\\\\n\\\\nhttp': 60,\n",
       " 'raw': 61,\n",
       " 'plot': 62,\n",
       " 'interaction': 63,\n",
       " 'roughly': 64,\n",
       " '20a': 65,\n",
       " 'likely': 66,\n",
       " 'mistakes': 67,\n",
       " '8gb': 68,\n",
       " 'honor': 69,\n",
       " 'privilege': 70,\n",
       " 'sense': 71,\n",
       " 'appreciated': 72,\n",
       " 'medication': 73,\n",
       " 'jail': 74,\n",
       " 'sick': 75,\n",
       " 'representation': 76,\n",
       " 'sneak': 77,\n",
       " 'according': 78,\n",
       " 'supporting': 79,\n",
       " 'england': 80,\n",
       " 'staff': 81,\n",
       " 'solve': 82,\n",
       " 'square': 83,\n",
       " 'threw': 84,\n",
       " 'couch': 85,\n",
       " 'absolute': 86,\n",
       " 'july': 87,\n",
       " 'cards': 88,\n",
       " 'indicate': 89,\n",
       " 'hl': 90,\n",
       " 'pavement': 91,\n",
       " 'portion': 92,\n",
       " 'insurance': 93,\n",
       " '^code': 94,\n",
       " 'unless': 95,\n",
       " 'former': 96,\n",
       " 'http': 97,\n",
       " '3rd': 98,\n",
       " 'again.\\\\n\\\\n': 99,\n",
       " 'voted': 100,\n",
       " 'debt': 101,\n",
       " 'suffering': 102,\n",
       " 'cares': 103,\n",
       " 'discovered': 104,\n",
       " '\\\\n\\\\na': 105,\n",
       " 'question': 106,\n",
       " 'funniest': 107,\n",
       " 'woman\\\\': 108,\n",
       " 'claim': 109,\n",
       " 'seconds': 110,\n",
       " 'directed': 111,\n",
       " '70': 112,\n",
       " 'vs': 113,\n",
       " 'cheese': 114,\n",
       " 'makes': 115,\n",
       " 'there\\\\': 116,\n",
       " '\\\\n\\\\nmy': 117,\n",
       " 'addiction': 118,\n",
       " 'suck': 119,\n",
       " 'self-promotion': 120,\n",
       " 'healthy': 121,\n",
       " 'computer': 122,\n",
       " 'performs': 123,\n",
       " 'in.': 124,\n",
       " '\\\\n\\\\nsecond': 125,\n",
       " 'bothers': 126,\n",
       " 'prices': 127,\n",
       " 'lawyer': 128,\n",
       " 'label': 129,\n",
       " 'via': 130,\n",
       " 'changing': 131,\n",
       " 'refers': 132,\n",
       " 'voice': 133,\n",
       " 'q=title': 134,\n",
       " 'relating': 135,\n",
       " 'assist': 136,\n",
       " 'therapist': 137,\n",
       " 'crime': 138,\n",
       " 'youtube': 139,\n",
       " 'real': 140,\n",
       " 'explain': 141,\n",
       " 'themselves': 142,\n",
       " 'paid': 143,\n",
       " 'r': 144,\n",
       " 'pretending': 145,\n",
       " 'last': 146,\n",
       " 'signed': 147,\n",
       " '\\\\n\\\\nwhich': 148,\n",
       " 'posting': 149,\n",
       " \"'when\": 150,\n",
       " 'through': 151,\n",
       " 'wheels': 152,\n",
       " 'supports': 153,\n",
       " 'benner': 154,\n",
       " 'cases': 155,\n",
       " 'upgraded': 156,\n",
       " 'apples': 157,\n",
       " 'determined': 158,\n",
       " 'apartment': 159,\n",
       " 'longer': 160,\n",
       " 'tgt': 161,\n",
       " 'once': 162,\n",
       " 'colors': 163,\n",
       " 'clicking': 164,\n",
       " 'rift': 165,\n",
       " 'happens': 166,\n",
       " 'excuse': 167,\n",
       " '500': 168,\n",
       " 'exploit': 169,\n",
       " 'tools': 170,\n",
       " 'encouraged': 171,\n",
       " 'omg': 172,\n",
       " 'losing': 173,\n",
       " 'so': 174,\n",
       " \"'yes\": 175,\n",
       " 'harder': 176,\n",
       " 'slowing': 177,\n",
       " 'winter': 178,\n",
       " 'SENTENCE_END': 179,\n",
       " 'keep': 180,\n",
       " 'gear': 181,\n",
       " 'angel': 182,\n",
       " 'statement': 183,\n",
       " 'peers': 184,\n",
       " '\\\\n\\\\nyeah': 185,\n",
       " 'argued': 186,\n",
       " 'tries': 187,\n",
       " 'reconcile': 188,\n",
       " 'puns': 189,\n",
       " 'career': 190,\n",
       " 'repost': 191,\n",
       " 'talking': 192,\n",
       " 'voting.\\\\n\\\\nplease': 193,\n",
       " '/message/compose/': 194,\n",
       " 'learn': 195,\n",
       " 'infrastructure': 196,\n",
       " '^^^bot': 197,\n",
       " 'courts': 198,\n",
       " '30': 199,\n",
       " 'mana': 200,\n",
       " 'tagging': 201,\n",
       " '^^^have': 202,\n",
       " 'helicopter': 203,\n",
       " '\\\\nedit': 204,\n",
       " 'met': 205,\n",
       " 'women\\\\': 206,\n",
       " 'u.s.': 207,\n",
       " 'outcome': 208,\n",
       " 'to=/r/fireteams': 209,\n",
       " 'pills': 210,\n",
       " 'apple': 211,\n",
       " 'lt': 212,\n",
       " 'climate': 213,\n",
       " '\\\\\\\\n': 214,\n",
       " 'extension': 215,\n",
       " 'subject': 216,\n",
       " 'concrete': 217,\n",
       " 'steam': 218,\n",
       " 'i+4': 219,\n",
       " 'define': 220,\n",
       " '^^': 221,\n",
       " 'center': 222,\n",
       " 'linked': 223,\n",
       " 'spirit': 224,\n",
       " 'distinguish': 225,\n",
       " 'arm': 226,\n",
       " '6': 227,\n",
       " 'arguing': 228,\n",
       " 'recently': 229,\n",
       " 'naturally': 230,\n",
       " 'violated': 231,\n",
       " 'fairness': 232,\n",
       " 'supposed': 233,\n",
       " 'beings': 234,\n",
       " 'subsequent': 235,\n",
       " 'actually': 236,\n",
       " 'fired': 237,\n",
       " 'stadium': 238,\n",
       " 'buttons': 239,\n",
       " 'clear': 240,\n",
       " 'policy': 241,\n",
       " 'to=/r/friendsafari': 242,\n",
       " '90': 243,\n",
       " 'sun': 244,\n",
       " 'jungle': 245,\n",
       " 'sources': 246,\n",
       " 'consult': 247,\n",
       " 'segment': 248,\n",
       " 'how': 249,\n",
       " 'to=': 250,\n",
       " 'section': 251,\n",
       " 'samsung': 252,\n",
       " 'largely': 253,\n",
       " 'visual': 254,\n",
       " 'reading': 255,\n",
       " 'them.\\\\n\\\\nwe': 256,\n",
       " 'be': 257,\n",
       " 'value': 258,\n",
       " 'submitting': 259,\n",
       " 'saiyan': 260,\n",
       " 'compensate': 261,\n",
       " 'mid': 262,\n",
       " 'site=search': 263,\n",
       " 'consisting': 264,\n",
       " 'interact': 265,\n",
       " 'log': 266,\n",
       " '\\\\nin': 267,\n",
       " 'conclude': 268,\n",
       " 'exhaust': 269,\n",
       " 'killing': 270,\n",
       " 'turns': 271,\n",
       " 'atx': 272,\n",
       " 'motherboard': 273,\n",
       " \"can\\\\'t\": 274,\n",
       " 'that': 275,\n",
       " 'resort': 276,\n",
       " 'playerbase': 277,\n",
       " 'ugh': 278,\n",
       " '\\\\n\\\\nhere': 279,\n",
       " 'pero': 280,\n",
       " 'ult': 281,\n",
       " \"'first\": 282,\n",
       " 'was': 283,\n",
       " 'shows': 284,\n",
       " 'fps': 285,\n",
       " 'to=/r/videos': 286,\n",
       " 'lean': 287,\n",
       " 'hero': 288,\n",
       " '27': 289,\n",
       " 'bad': 290,\n",
       " \"'hmm\": 291,\n",
       " 'distance': 292,\n",
       " '\\\\nblog': 293,\n",
       " 'needed': 294,\n",
       " 'big': 295,\n",
       " 'smelled': 296,\n",
       " 'meet': 297,\n",
       " 'automatic': 298,\n",
       " 'jumping': 299,\n",
       " 'may': 300,\n",
       " 'albeit': 301,\n",
       " '^^^feedback': 302,\n",
       " 'demo': 303,\n",
       " 'tried': 304,\n",
       " 'age': 305,\n",
       " 'beginning': 306,\n",
       " 'dollars': 307,\n",
       " '\\\\n\\\\n\\\\t': 308,\n",
       " 'rear': 309,\n",
       " 'sanders': 310,\n",
       " 'minutes': 311,\n",
       " '//friendsafaribot.tumblr.com/': 312,\n",
       " 'landlord': 313,\n",
       " 'minority': 314,\n",
       " 'does': 315,\n",
       " 'separate': 316,\n",
       " 'laser': 317,\n",
       " 'northern': 318,\n",
       " 'had': 319,\n",
       " 'complaining': 320,\n",
       " 'intelligent': 321,\n",
       " 'abide': 322,\n",
       " 'pronouns': 323,\n",
       " 'imdb': 324,\n",
       " 'yourself': 325,\n",
       " '^^^if': 326,\n",
       " \"'hello\": 327,\n",
       " '//redd.it/s4chc': 328,\n",
       " 'pack': 329,\n",
       " '\\\\n\\\\ndo': 330,\n",
       " 'help': 331,\n",
       " 'amp': 332,\n",
       " 'tracks': 333,\n",
       " 'orange': 334,\n",
       " 'nurses': 335,\n",
       " 'read': 336,\n",
       " 'bound': 337,\n",
       " 'revealed': 338,\n",
       " 'ok.': 339,\n",
       " '\\\\n-': 340,\n",
       " 'theirs': 341,\n",
       " '29': 342,\n",
       " 'tip': 343,\n",
       " 'condition': 344,\n",
       " 'media': 345,\n",
       " 'traditional': 346,\n",
       " 'closest': 347,\n",
       " 'complete': 348,\n",
       " 'concerned': 349,\n",
       " 'pursuit': 350,\n",
       " '^^i': 351,\n",
       " 'protection': 352,\n",
       " 'vegan': 353,\n",
       " 'lifetime': 354,\n",
       " 'assume': 355,\n",
       " 'humanity': 356,\n",
       " 'somewhere': 357,\n",
       " 'issues': 358,\n",
       " 'gigs': 359,\n",
       " 'beans': 360,\n",
       " 'shower': 361,\n",
       " '960': 362,\n",
       " 'taste': 363,\n",
       " 'poll': 364,\n",
       " 'checked': 365,\n",
       " 'contrary': 366,\n",
       " 'hope': 367,\n",
       " 'aid': 368,\n",
       " 'per': 369,\n",
       " '3f': 370,\n",
       " 'se': 371,\n",
       " 'area': 372,\n",
       " 'unhealthy': 373,\n",
       " 'click': 374,\n",
       " 'regardless': 375,\n",
       " 'doctor.\\\\n': 376,\n",
       " 'marrow': 377,\n",
       " 'working': 378,\n",
       " 'associated': 379,\n",
       " 'roast': 380,\n",
       " 'bs': 381,\n",
       " 'transportation': 382,\n",
       " 'hard\\\\n\\\\n\\\\\\\\': 383,\n",
       " 'feed': 384,\n",
       " 'her': 385,\n",
       " 'pvp': 386,\n",
       " 'magic': 387,\n",
       " \"'astronomy\": 388,\n",
       " 'near': 389,\n",
       " 'gospel': 390,\n",
       " 'able': 391,\n",
       " 'bb': 392,\n",
       " 'minimize': 393,\n",
       " 'trusted': 394,\n",
       " 'playlist': 395,\n",
       " 'addict': 396,\n",
       " 'plans': 397,\n",
       " 'vidilux': 398,\n",
       " 'posts': 399,\n",
       " 'settings': 400,\n",
       " 'commenter': 401,\n",
       " 'screen': 402,\n",
       " 'tempted': 403,\n",
       " 'gt': 404,\n",
       " 'merely': 405,\n",
       " '\\\\n^^^': 406,\n",
       " 'backup': 407,\n",
       " 'us': 408,\n",
       " 'ultimately': 409,\n",
       " '\\\\n\\\\nhave': 410,\n",
       " 'based': 411,\n",
       " 'participating': 412,\n",
       " 'tuesday': 413,\n",
       " 'gain': 414,\n",
       " 'awful': 415,\n",
       " 'perfectly': 416,\n",
       " 'victims': 417,\n",
       " 'captain': 418,\n",
       " 'indicator': 419,\n",
       " 'dm': 420,\n",
       " 'sin': 421,\n",
       " 'continue': 422,\n",
       " 'obtain': 423,\n",
       " 'taken': 424,\n",
       " 'stone': 425,\n",
       " 'tanks': 426,\n",
       " 'equally': 427,\n",
       " 'liberal': 428,\n",
       " 'restrict_sr=': 429,\n",
       " 'processor': 430,\n",
       " 'fill': 431,\n",
       " 'coast': 432,\n",
       " 'united': 433,\n",
       " 'proper': 434,\n",
       " 'money.': 435,\n",
       " 'weak': 436,\n",
       " 'shovel': 437,\n",
       " 'oil': 438,\n",
       " 'listed': 439,\n",
       " 'learned': 440,\n",
       " 'around': 441,\n",
       " 'outside': 442,\n",
       " 'regarding': 443,\n",
       " 'walked': 444,\n",
       " 'please': 445,\n",
       " 'dev': 446,\n",
       " 'nzxt': 447,\n",
       " 'figures': 448,\n",
       " 'north': 449,\n",
       " 'kinda': 450,\n",
       " 'message=my': 451,\n",
       " '0aplease': 452,\n",
       " 'emphasis': 453,\n",
       " 'know': 454,\n",
       " 'replaced': 455,\n",
       " 'takes': 456,\n",
       " 'shed': 457,\n",
       " 'caught': 458,\n",
       " 'synergism': 459,\n",
       " 'madison': 460,\n",
       " 'counting': 461,\n",
       " 'explicit': 462,\n",
       " 'sale': 463,\n",
       " '\\\\n\\\\nwith': 464,\n",
       " 'alcohol': 465,\n",
       " 'hillary': 466,\n",
       " 'peak': 467,\n",
       " 'problem': 468,\n",
       " 'unknown': 469,\n",
       " 'specified': 470,\n",
       " 'users': 471,\n",
       " 'evo-series': 472,\n",
       " 'mass': 473,\n",
       " 'to=jasie3k': 474,\n",
       " 'ashley': 475,\n",
       " '2014': 476,\n",
       " 'character': 477,\n",
       " 'suspect': 478,\n",
       " 'distinct': 479,\n",
       " 'comments': 480,\n",
       " 'nor': 481,\n",
       " '-\\\\n\\\\n': 482,\n",
       " 'fastest': 483,\n",
       " 'entirely': 484,\n",
       " 'p': 485,\n",
       " 'anywhere': 486,\n",
       " \"'agreed\": 487,\n",
       " 'let': 488,\n",
       " 'national': 489,\n",
       " 'upon': 490,\n",
       " 'paint': 491,\n",
       " 'comparison': 492,\n",
       " 'till': 493,\n",
       " 'searing': 494,\n",
       " 'lowest': 495,\n",
       " 'textbox': 496,\n",
       " 'wiki_vague_titles': 497,\n",
       " 'unrelated': 498,\n",
       " \"'one\": 499,\n",
       " 'mal': 500,\n",
       " 'destroys': 501,\n",
       " 'selftext=true': 502,\n",
       " 'blue': 503,\n",
       " 'silver': 504,\n",
       " 'ago': 505,\n",
       " 'thank': 506,\n",
       " 'showing': 507,\n",
       " 'eventually': 508,\n",
       " 'saying': 509,\n",
       " 'models': 510,\n",
       " 'steam\\\\n': 511,\n",
       " 'comfortable': 512,\n",
       " 'suggesting': 513,\n",
       " 'answered': 514,\n",
       " 'mental': 515,\n",
       " 'tablet': 516,\n",
       " 'wound': 517,\n",
       " 'mage': 518,\n",
       " 'starts': 519,\n",
       " 'evga': 520,\n",
       " 'nervous': 521,\n",
       " 'abandoned': 522,\n",
       " '970': 523,\n",
       " 'drawing': 524,\n",
       " 'john': 525,\n",
       " '75': 526,\n",
       " '20to': 527,\n",
       " 'wash': 528,\n",
       " 'height': 529,\n",
       " \"'any\": 530,\n",
       " \"'what\": 531,\n",
       " 'loud': 532,\n",
       " 'powerful': 533,\n",
       " 'storm': 534,\n",
       " 'touch': 535,\n",
       " 'covers': 536,\n",
       " 'candidate': 537,\n",
       " 'basic': 538,\n",
       " 'ensure': 539,\n",
       " 'physical': 540,\n",
       " 'owned': 541,\n",
       " 'q=': 542,\n",
       " 'blow': 543,\n",
       " '20hedron': 544,\n",
       " '‘': 545,\n",
       " 'stands': 546,\n",
       " 'review': 547,\n",
       " '\\\\nthere': 548,\n",
       " 'franchise': 549,\n",
       " 'eliminate': 550,\n",
       " 'percentage': 551,\n",
       " 'model': 552,\n",
       " \"'how\": 553,\n",
       " 'location': 554,\n",
       " '/scrunchieshy-intensifies': 555,\n",
       " 'halfway': 556,\n",
       " 'ikke': 557,\n",
       " 'artist': 558,\n",
       " 'information': 559,\n",
       " 'ocean': 560,\n",
       " 'stronger': 561,\n",
       " 'edition': 562,\n",
       " 'realized': 563,\n",
       " 'male': 564,\n",
       " 'subject=list': 565,\n",
       " 'lights': 566,\n",
       " 'gop': 567,\n",
       " 'willingness': 568,\n",
       " '-\\\\n\\\\n\\\\n': 569,\n",
       " 'to=/r/pokemongiveaway': 570,\n",
       " 'june': 571,\n",
       " 'they\\\\': 572,\n",
       " 'sorry': 573,\n",
       " 'press': 574,\n",
       " 'return': 575,\n",
       " 'break': 576,\n",
       " 'everyday': 577,\n",
       " 'mini': 578,\n",
       " 'ncix': 579,\n",
       " 'pid=cffd2': 580,\n",
       " 'unique': 581,\n",
       " 'lock': 582,\n",
       " 'strength': 583,\n",
       " 'man\\\\': 584,\n",
       " 'response': 585,\n",
       " 'agenda': 586,\n",
       " '^message': 587,\n",
       " '//www.tineye.com/search': 588,\n",
       " 'council': 589,\n",
       " 'below': 590,\n",
       " 'heart': 591,\n",
       " 'davis': 592,\n",
       " 'experimenting': 593,\n",
       " 'back': 594,\n",
       " 'to=/r/askreddit': 595,\n",
       " 'subject=feedback': 596,\n",
       " 'to=hearthscan-bot': 597,\n",
       " 'still': 598,\n",
       " \"'sure\": 599,\n",
       " 'extend': 600,\n",
       " 'type=card': 601,\n",
       " 'success': 602,\n",
       " 'craigslist': 603,\n",
       " 'subject=rsgspolice': 604,\n",
       " '\\\\n\\\\nhowever': 605,\n",
       " \"'which\": 606,\n",
       " 'iraq': 607,\n",
       " 'sidebar': 608,\n",
       " 'hopefully': 609,\n",
       " 'investigate': 610,\n",
       " '\\\\n\\\\non': 611,\n",
       " '2013': 612,\n",
       " 'needless': 613,\n",
       " '-feat': 614,\n",
       " '2015-08-28': 615,\n",
       " 'swear': 616,\n",
       " 'sent': 617,\n",
       " 'remind': 618,\n",
       " \"'lol\": 619,\n",
       " 'stock': 620,\n",
       " 'herself': 621,\n",
       " 'affects': 622,\n",
       " 'dark': 623,\n",
       " 'usual': 624,\n",
       " 'complexity': 625,\n",
       " 'sign': 626,\n",
       " 'complicated': 627,\n",
       " 'replies': 628,\n",
       " '/intensifies': 629,\n",
       " 'metal': 630,\n",
       " 'trapped': 631,\n",
       " 'lose': 632,\n",
       " 'pcpartpicker': 633,\n",
       " '\\\\ncurrent|': 634,\n",
       " 'criticize': 635,\n",
       " '___\\\\n\\\\n': 636,\n",
       " 'syntax=cloudsearch': 637,\n",
       " 'action': 638,\n",
       " '..': 639,\n",
       " 'screenshot': 640,\n",
       " 'mountain': 641,\n",
       " 'blown': 642,\n",
       " 'lost': 643,\n",
       " 'in': 644,\n",
       " 'existing': 645,\n",
       " 'offering': 646,\n",
       " 'raising': 647,\n",
       " \"'do\": 648,\n",
       " 'economic': 649,\n",
       " 'addicted': 650,\n",
       " 'kill': 651,\n",
       " 'guide': 652,\n",
       " 'available': 653,\n",
       " 'rehabilitation': 654,\n",
       " 'diet': 655,\n",
       " 'point': 656,\n",
       " 'drink': 657,\n",
       " \"'while\": 658,\n",
       " 'this': 659,\n",
       " 'waste': 660,\n",
       " 'mess': 661,\n",
       " 'freak': 662,\n",
       " 'incentive': 663,\n",
       " 'received': 664,\n",
       " '//gatherer.wizards.com/pages/card/details.aspx': 665,\n",
       " 'train': 666,\n",
       " 'fell': 667,\n",
       " 'top-level': 668,\n",
       " 'trust': 669,\n",
       " 'catfish': 670,\n",
       " 'resulted': 671,\n",
       " 'wiki_askscience_user_help_page': 672,\n",
       " 'shall': 673,\n",
       " 'implemented': 674,\n",
       " 'subject=mirrorportal': 675,\n",
       " \"'also\": 676,\n",
       " 'appearance': 677,\n",
       " 'ships': 678,\n",
       " 'hurts': 679,\n",
       " 'rule': 680,\n",
       " 'domain': 681,\n",
       " 'hours': 682,\n",
       " 'encourage': 683,\n",
       " 'crown': 684,\n",
       " '20thread': 685,\n",
       " 'roles': 686,\n",
       " 'complex': 687,\n",
       " 'lasts': 688,\n",
       " 'wealthy': 689,\n",
       " 'sell': 690,\n",
       " 'training': 691,\n",
       " 'trolling': 692,\n",
       " 'instance': 693,\n",
       " 'when': 694,\n",
       " 'rp': 695,\n",
       " 'person\\\\': 696,\n",
       " 'computers': 697,\n",
       " 'somebody': 698,\n",
       " 'implement': 699,\n",
       " 'whether': 700,\n",
       " 'offer': 701,\n",
       " 'constantly': 702,\n",
       " 'actively': 703,\n",
       " 'skeleton': 704,\n",
       " 'monday': 705,\n",
       " 'truly': 706,\n",
       " 'brackets': 707,\n",
       " 'beliefs': 708,\n",
       " 'education': 709,\n",
       " 'criminal': 710,\n",
       " 'hear': 711,\n",
       " 'eating': 712,\n",
       " 'bag': 713,\n",
       " 'employees': 714,\n",
       " 'favs': 715,\n",
       " 'consist': 716,\n",
       " 'detection': 717,\n",
       " 'submit': 718,\n",
       " 'blood': 719,\n",
       " '\\\\n\\\\nhis': 720,\n",
       " '//www.google.com/searchbyimage': 721,\n",
       " 'lives': 722,\n",
       " '3a': 723,\n",
       " 'hearing': 724,\n",
       " \"don\\\\'t\": 725,\n",
       " 'respond': 726,\n",
       " 'ufc': 727,\n",
       " 'parody': 728,\n",
       " 'god': 729,\n",
       " 'feeling': 730,\n",
       " '//www.modojo.com/features/nintendo_3ds_friend_codes_everything_you_need_to_know': 731,\n",
       " 'cross': 732,\n",
       " 'hire': 733,\n",
       " 'nbsp': 734,\n",
       " 'paragraph': 735,\n",
       " 'overview': 736,\n",
       " 'super': 737,\n",
       " '//www.reddit.com/r/askscience/wiki/index': 738,\n",
       " 'importantly': 739,\n",
       " 'citizens': 740,\n",
       " '£1.99': 741,\n",
       " 'prepared': 742,\n",
       " 'to=/r/totesmessenger': 743,\n",
       " 'insecure': 744,\n",
       " 'affect': 745,\n",
       " 'denied': 746,\n",
       " 'imply': 747,\n",
       " 'simple': 748,\n",
       " 'whole': 749,\n",
       " 'trade': 750,\n",
       " '^tell': 751,\n",
       " 'cars': 752,\n",
       " 'kurt': 753,\n",
       " 'did': 754,\n",
       " 'disagrees': 755,\n",
       " 'roastee': 756,\n",
       " 'awesome': 757,\n",
       " 'schedule': 758,\n",
       " 'mc': 759,\n",
       " 'rock': 760,\n",
       " 'its': 761,\n",
       " 'frankly': 762,\n",
       " 'grass': 763,\n",
       " 'times': 764,\n",
       " 'implying': 765,\n",
       " 'gpu': 766,\n",
       " 'chase': 767,\n",
       " 'travel': 768,\n",
       " 'first': 769,\n",
       " 'verify': 770,\n",
       " 'week': 771,\n",
       " 'forms': 772,\n",
       " 'SENTENCE_START': 773,\n",
       " 'scenes': 774,\n",
       " 'ending\\\\n\\\\n': 775,\n",
       " 'subject=reminder': 776,\n",
       " 'function': 777,\n",
       " 'map': 778,\n",
       " 'owners': 779,\n",
       " 'prime': 780,\n",
       " 'issue': 781,\n",
       " 'warrant': 782,\n",
       " 'surrounded': 783,\n",
       " 'subreddits': 784,\n",
       " 'cost': 785,\n",
       " 'wing': 786,\n",
       " 'canon': 787,\n",
       " 'searched': 788,\n",
       " 'tests': 789,\n",
       " 'inspired': 790,\n",
       " '//reddit.com/r/randomactsofblowjob/search': 791,\n",
       " 'higher': 792,\n",
       " 'restrictions': 793,\n",
       " '^^a': 794,\n",
       " 'early': 795,\n",
       " 'to=/r/ps4': 796,\n",
       " '2008': 797,\n",
       " 'pocket': 798,\n",
       " 'pointless': 799,\n",
       " 'cruz': 800,\n",
       " 'fully-modular': 801,\n",
       " 'key': 802,\n",
       " 'birth': 803,\n",
       " 'trash': 804,\n",
       " 'cents': 805,\n",
       " 'space': 806,\n",
       " 'home': 807,\n",
       " 'guilt': 808,\n",
       " '*': 809,\n",
       " 'cake': 810,\n",
       " 'ssd': 811,\n",
       " 'playing': 812,\n",
       " 'intent': 813,\n",
       " 'grown': 814,\n",
       " 'gon': 815,\n",
       " 'individual': 816,\n",
       " 'curious': 817,\n",
       " \"'here\": 818,\n",
       " 'practice': 819,\n",
       " 'on\\\\nhttp': 820,\n",
       " '//github.com/silver': 821,\n",
       " 'honesty': 822,\n",
       " 'filled': 823,\n",
       " 'developers': 824,\n",
       " '^i': 825,\n",
       " 'already': 826,\n",
       " 'wind': 827,\n",
       " 'relying': 828,\n",
       " 'shirt': 829,\n",
       " \"'anthropology\": 830,\n",
       " 'woke': 831,\n",
       " 'contrast': 832,\n",
       " 'shots': 833,\n",
       " 'fellow': 834,\n",
       " 'server': 835,\n",
       " 'system/console': 836,\n",
       " 'forward': 837,\n",
       " 'pile': 838,\n",
       " 'lined': 839,\n",
       " 's': 840,\n",
       " '\\\\ni': 841,\n",
       " 'messing': 842,\n",
       " '850': 843,\n",
       " 'bomb': 844,\n",
       " 'give': 845,\n",
       " 'patent': 846,\n",
       " 'diamond': 847,\n",
       " 'previous': 848,\n",
       " 'ass': 849,\n",
       " 'is': 850,\n",
       " 'haki': 851,\n",
       " 'assets': 852,\n",
       " 'it.\\\\n\\\\n5': 853,\n",
       " 'prisons': 854,\n",
       " 'oldest': 855,\n",
       " 'buying': 856,\n",
       " 'spending': 857,\n",
       " 'needs': 858,\n",
       " 'crash': 859,\n",
       " 'plastic': 860,\n",
       " 'involves': 861,\n",
       " 'building': 862,\n",
       " 'hold': 863,\n",
       " 'choices': 864,\n",
       " 'q': 865,\n",
       " 'glad': 866,\n",
       " 'yeah': 867,\n",
       " 'trolls': 868,\n",
       " 'third': 869,\n",
       " '//twitter.com/search': 870,\n",
       " 'honestly': 871,\n",
       " 'result': 872,\n",
       " 'marketing': 873,\n",
       " '\\\\n\\\\nmost': 874,\n",
       " 'desperate': 875,\n",
       " 'fairly': 876,\n",
       " 'companies': 877,\n",
       " 'join': 878,\n",
       " 'learning': 879,\n",
       " 'save': 880,\n",
       " 'fire': 881,\n",
       " 'to=havoc_bot': 882,\n",
       " 'kit': 883,\n",
       " 'weigh': 884,\n",
       " 'cc': 885,\n",
       " 'h': 886,\n",
       " 'mysterious': 887,\n",
       " 'same': 888,\n",
       " 'gigabyte': 889,\n",
       " 'society': 890,\n",
       " 'range': 891,\n",
       " 'ingredients': 892,\n",
       " 'complaints': 893,\n",
       " 'explanation': 894,\n",
       " 'toward': 895,\n",
       " 'occasion': 896,\n",
       " 'direction': 897,\n",
       " '4th': 898,\n",
       " 'mentioned': 899,\n",
       " \"'paleontology\": 900,\n",
       " 'chris': 901,\n",
       " 'disclosure': 902,\n",
       " 'advice': 903,\n",
       " 'remain': 904,\n",
       " 'confirmed': 905,\n",
       " 'involving': 906,\n",
       " 'requiring': 907,\n",
       " 'channel': 908,\n",
       " 'worried': 909,\n",
       " 'coming': 910,\n",
       " \"'absolutely\": 911,\n",
       " 'reward': 912,\n",
       " 'respective': 913,\n",
       " 'indicating': 914,\n",
       " 'lay': 915,\n",
       " 'production': 916,\n",
       " 'retarded': 917,\n",
       " 'being': 918,\n",
       " 'end': 919,\n",
       " 'updated': 920,\n",
       " 'figuring': 921,\n",
       " \"'hey\": 922,\n",
       " 'hurt': 923,\n",
       " 'looking': 924,\n",
       " '/r/mylittlepony/comments/1lwzub/deviantart_imgur_mirror_bot_nightmirrormoon/': 925,\n",
       " \"'m\": 926,\n",
       " 'code': 927,\n",
       " 'and/or': 928,\n",
       " '\\\\': 929,\n",
       " 'mail': 930,\n",
       " 'safety': 931,\n",
       " 'drawn': 932,\n",
       " 'treat': 933,\n",
       " \"'sounds\": 934,\n",
       " 'basically': 935,\n",
       " 'expectation': 936,\n",
       " 'policies': 937,\n",
       " 'worry': 938,\n",
       " 'looked': 939,\n",
       " 'possible.': 940,\n",
       " 'e.g': 941,\n",
       " 'bullet': 942,\n",
       " 'america': 943,\n",
       " 'possibility': 944,\n",
       " 'deaths': 945,\n",
       " 'household': 946,\n",
       " 'unfair': 947,\n",
       " 'chart': 948,\n",
       " 'inch': 949,\n",
       " 'shoulder': 950,\n",
       " 'mint': 951,\n",
       " 'club': 952,\n",
       " 'democratic': 953,\n",
       " 'manner': 954,\n",
       " '28http': 955,\n",
       " 'waters': 956,\n",
       " 'suppose': 957,\n",
       " 'parent': 958,\n",
       " 'creator': 959,\n",
       " 'equal': 960,\n",
       " 'phase': 961,\n",
       " \"'for\": 962,\n",
       " 'memory': 963,\n",
       " 'gamer': 964,\n",
       " 'medical': 965,\n",
       " 'chains': 966,\n",
       " 'gut': 967,\n",
       " 'supported': 968,\n",
       " '7200rpm': 969,\n",
       " 'slots': 970,\n",
       " 'made': 971,\n",
       " 'luck': 972,\n",
       " 'lo': 973,\n",
       " 'maximize': 974,\n",
       " 'servers': 975,\n",
       " 'coffee': 976,\n",
       " '27s': 977,\n",
       " 'aye': 978,\n",
       " '^^^or': 979,\n",
       " 'million': 980,\n",
       " 'reveal': 981,\n",
       " '\\\\n1': 982,\n",
       " 'loose': 983,\n",
       " '^^^/u/fursec': 984,\n",
       " 'dangerous': 985,\n",
       " 'starting': 986,\n",
       " 'irrelevant': 987,\n",
       " 'assumption': 988,\n",
       " 'larger': 989,\n",
       " 'curiosity': 990,\n",
       " 'mecha': 991,\n",
       " 'broken': 992,\n",
       " 'bars': 993,\n",
       " 'instrument': 994,\n",
       " 'removed': 995,\n",
       " 'seem': 996,\n",
       " 'anyone': 997,\n",
       " \"isn\\\\'t\": 998,\n",
       " 'shortly': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {s:i for i,s in enumerate(word_set)} # string to index\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "stoi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "528d2903-24ae-4b90-80b1-b5ceb9042fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'UNKNOWN_TOKEN' does not exist in the list tokenized_sentences.\n"
     ]
    }
   ],
   "source": [
    "found = False\n",
    "for sentence in tokenized_sentences:\n",
    "    if 'UNKNOWN_TOKEN' in sentence:\n",
    "        found = True\n",
    "        break\n",
    "\n",
    "if found:\n",
    "    print(\"'UNKNOWN_TOKEN' exists in the list tokenized_sentences.\")\n",
    "else:\n",
    "    print(\"'UNKNOWN_TOKEN' does not exist in the list tokenized_sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9284acdd-07cf-4df7-aac3-5cf267cf00de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENTENCE_START',\n",
       " 'this',\n",
       " 'is',\n",
       " 'definitely',\n",
       " 'where',\n",
       " 'it\\\\',\n",
       " \"'s\",\n",
       " 'at',\n",
       " '.',\n",
       " \"'\",\n",
       " 'SENTENCE_END']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "08589bc6-f148-4b23-bd5a-bf61bbdbf7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = [word for sentence in tokenized_sentences for word in sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fa94e928-eaee-4ef3-a46b-c5d3ff47fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = [word for sentence in tokenized_sentences for word in sentence if word in word_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830dcf20-24ab-489a-8491-867c2640e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = [ch for sentence in tokenized_sentences for word in sentence if word in word_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "082e812d-0072-4e01-860f-e20992026e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = [stoi[word] for word in tokenized_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "38aeaa35-dbb9-44fc-aba4-6c6d2bd66a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500138\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1e0cd637-a15f-414f-8967-79041bb0f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(tokenized_sentences)\n",
    "n1 = int(0.8 * len(tokenized_sentences))\n",
    "n2 = int(0.9 * len(tokenized_sentences))\n",
    "\n",
    "Xtr = tokenized_words[:n1]\n",
    "Ytr = tokenized_words[1:n1+1]\n",
    "Xdev = tokenized_words[n1:n2]\n",
    "Ydev = tokenized_words[n1+1:n2+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8db1e525-5277-4e72-9257-71e863f25e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[773, 659, 850, 3789, 3610, 2416, 4409, 3824, 3684, 4608]\n",
      "[659, 850, 3789, 3610, 2416, 4409, 3824, 3684, 4608, 179]\n"
     ]
    }
   ],
   "source": [
    "print(Xtr[:10])\n",
    "print(Ytr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "69566eab-f3ca-4e4f-98f1-232750530dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5138\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs, targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    # Create four dictionaries\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    # Copy the last hidden state into hprev\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    ### Forward pass (each time step)\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        # At this time step, input an encoded char, as a (27, 5) one hot vector\n",
    "        xs[t][inputs[t]] = 1\n",
    "        # At this time step, follow the formulas to get the hidden state at this time step\n",
    "        # (100,27) @ (27, 1) = (100, 1), (100, 100) @ (100, 1) = (100, 1) + (100, 1) -> (100, 1) (column vector of 100 outputs\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "\n",
    "        # At this time step, compute the output state\n",
    "        # (27, 100) @ (100, 1) = (27, 1) + (27, 1) = (1, 27)\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "\n",
    "        # Normalize our probabiltiies using softmax\n",
    "        # ps = (1, 27)\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "\n",
    "        # Take the loss of the correct output probabilities\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    \n",
    "    ### Backward pass: compute gradients going backwards\n",
    "    # dWxh (100, 27), dWhh (100, 100), dWhy = (27, 100)\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    # dbh (100, 1),  dby (27, 1)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    # (100, 1)\n",
    "    # Start off with zero\n",
    "    dhnext = np.zeros_like(hs[0]) \n",
    "    \n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backpropogate through the softmax to the logits\n",
    "        # (27, 5)\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "\n",
    "        # (27, 5) @ (5, 100) = (27, 100)\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        # (27, 5) -> This secrety should sum across the columns dim = 1 to produce (27, 1)\n",
    "        dby += dy\n",
    "\n",
    "        # dh(t), dh(t)raw\n",
    "        # Backpropogate into the previous layer (100, 27) @ (27, 5) = (100, 5)\n",
    "        dh = np.dot(Why.T, dy) + dhnext\n",
    "        # Backprop through tanh nonlinearity (element-wise forward -> element-wise backward)\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh \n",
    "\n",
    "        # dWxh, dWhh, dbh\n",
    "        # dWxh = (100, 5) @ (5, 27) = (100, 27)\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        # dWhh = (100, 5) (5, 100) = (100, 100)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        # Calculate dbh and dWxh\n",
    "        dbh += dhraw\n",
    "\n",
    "        # Derive part of the next time step's gradient\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "    # Clipping gradients\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "477f3cfd-f145-439e-b3f4-860a9b63a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n): # Passed in the previous inputs, the first letter of input, and 200 samples\n",
    "    \"\"\" \n",
    "    Sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    # Create a one hot vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[stoi['SENTENCE_START']] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "96906098-7dc4-4582-88f7-627f0695a808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " contacted articles bit however place shooters ndp ie picks individual weekends //www.google.com/search cast sciences \\n\\n6 individuals social drinks cpu results . competing viable air boring 400 raids susceptible question relay stuff \\n\\nwhat damage number to=remindmebot coast almost unit player belief relation involving alright pretty \\n\\nunfortunately pulled alcohol fingers front basic finally refused relay generate keyboard 'try affect copy s talked paid theory monster puts happy approach carrier step kidding spectrum mouth modern political santa hill \\n\\nunfortunately kit to=/r/music 5th walking ridiculous below ashley -\\n\\n\\n while technical bothered bottom cycle unique ^^^suggestions still whatsoever application allows \\nkarmadecay| yea gg clothes cried original edited mass band navy solution battles involved bit toward sufficient issue while theme ^^bot \\n^^^ notes considered pills far hardware bomb neat 20eye mode average on\\nhttp catching dollars worries together violating exist clock z 'thanks im rid 'at searching used gg awkward campus tend vidilux combat although followed freaked supreme cause solid answer alternatively morning quit arts relevant responsibility \\n\\nlook standing received history closed ii \\n\\nanyways person\\ boat image asian constant policy perfect boyfriend jesus murder it. factor co-workers utc operation shown lv common extreme dm //www.reddit.com/message/compose/ //www.imdb.com/title wrath meta theft 8.1 bringing mine 3.5 slots owner 140mm leads \n",
      "----\n",
      "iter 0, loss: 213.610486\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[163], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m----\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (txt, ))\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Forward seq_length characters through the net and fetch gradient\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m loss, dWxh, dWhh, dWhy, dbh, dby, hprev \u001b[38;5;241m=\u001b[39m \u001b[43mlossFun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhprev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m smooth_loss \u001b[38;5;241m=\u001b[39m smooth_loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.999\u001b[39m \u001b[38;5;241m+\u001b[39m loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n",
      "Cell \u001b[0;32mIn[147], line 25\u001b[0m, in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     21\u001b[0m hs[t] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(np\u001b[38;5;241m.\u001b[39mdot(Wxh, xs[t]) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Whh, hs[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m bh) \u001b[38;5;66;03m# hidden state\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# At this time step, compute the output state\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# (27, 100) @ (100, 1) = (27, 1) + (27, 1) = (1, 27)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m ys[t] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWhy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m by \u001b[38;5;66;03m# unnormalized log probabilities for next chars\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Normalize our probabiltiies using softmax\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# ps = (1, 27)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m ps[t] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(ys[t]) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mexp(ys[t])) \u001b[38;5;66;03m# probabilities for next chars\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 # input to hidden - (100, 27)\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 # hidden to hidden (100, 100)\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 # hidden to output (27, 100)\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias (100, 1)\n",
    "by = np.zeros((vocab_size, 1)) # output bias (27, 1)\n",
    "\n",
    "n, p = 0, 0\n",
    "\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size) * seq_length # loss at iteration 0\n",
    "\n",
    "while True:\n",
    "    # Prepare inputs (Sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # Go from start of data\n",
    "    inputs = Xtr[p:p+seq_length]\n",
    "    targets = Ytr[p:p+seq_length]\n",
    "\n",
    "    \n",
    "    # Sample from the model now and then, and print the result\n",
    "    if n % 100 == 0:\n",
    "        # Pass in the previous inputs, the first letter of input, and 200 samples\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ' '.join(itos[ix] for ix in sample_ix)\n",
    "        print ('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    \n",
    "    # Forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: \n",
    "        print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update   \n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "71ea5368-5949-4d77-8168-c3ac1289665d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi['SENTENCE_END']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fa2b2524-3491-4e0c-b976-4f99aa04d1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi['SENTENCE_START']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "640d7d03-0ac2-4fb9-8d7b-a2e70565743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " a bodies aggressive to ^ because they ! '' SENTENCE_END SENTENCE_START there to when * * gt ; SENTENCE_START '' himself . SENTENCE_END SENTENCE_START `` https you for however interaction and example much : * * probably next generally and the . SENTENCE_END SENTENCE_START ' actually you\\ one i card takes to maybe : many of does 's taking , is i most a seem in are jumping as no and is n't better weird to accept just cause * because if me so and on `` : on united nearly on i ever video or it ; ! '' SENTENCE_END SENTENCE_START can west get , and answered a review i to= automatically ? SENTENCE_END SENTENCE_START \\n\\ni 's such . SENTENCE_END SENTENCE_START `` 'when and not whole and long to * * it\\ 's right SENTENCE_END SENTENCE_START control you and removed get ; like to common , to to say of be 20you , and she 'm heaven awful up that handle the towards , , you have haha hours she\\ it . SENTENCE_END SENTENCE_START read and : it 'm entry my few or every the to.\\n\\nif source from is ? SENTENCE_END SENTENCE_START i ca to from what : flavor \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "sample_ix = sample(hprev, 773, 200)\n",
    "txt = ' '.join(itos[ix] for ix in sample_ix)\n",
    "print ('----\\n %s \\n----' % (txt, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
