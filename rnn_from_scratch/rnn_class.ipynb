{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffb5a22-8abc-4f20-9cd4-4aca64fd27aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "simple character rnn from Karpathy's blog\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def random_init(num_rows, num_cols):\n",
    "    return np.random.rand(num_rows, num_cols)*0.01\n",
    "\n",
    "def zero_init(num_rows, num_cols):\n",
    "    return np.zeros((num_rows, num_cols))\n",
    "\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length):\n",
    "        self.fp = open(path, \"r\")\n",
    "        self.data = self.fp.read()\n",
    "        chars = list(set(self.data))\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        self.data_size = len(self.data)\n",
    "        self.vocab_size = len(chars)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # model parameters\n",
    "        self.Wxh = random_init(hidden_size, vocab_size) # input to hidden\n",
    "        self.Whh = random_init(hidden_size, hidden_size) # hidden to hidden\n",
    "        self.Why = random_init(vocab_size, hidden_size) # hidden to output\n",
    "        self.bh = zero_init(hidden_size, 1) # bias for hidden layer\n",
    "        self.by = zero_init(vocab_size, 1) # bias for output\n",
    "\n",
    "        # memory vars for adagrad\n",
    "        self.mWxh = np.zeros_like(self.Wxh)\n",
    "        self.mWhh = np.zeros_like(self.Whh)\n",
    "        self.mWhy = np.zeros_like(self.Why)\n",
    "        self.mbh = np.zeros_like(self.bh)\n",
    "        self.mby = np.zeros_like(self.by)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, hprev):\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        for t in xrange(len(inputs)):\n",
    "            xs[t] = zero_init(self.vocab_size,1)\n",
    "            xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
    "            hs[t] = np.tanh(np.dot(self.Wxh,xs[t]) + np.dot(self.Whh,hs[t-1]) + self.bh) # hidden state\n",
    "            ys[t] = np.dot(self.Why,hs[t]) + self.by # unnormalised log probs for next char\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probs for next char\n",
    "        return xs, hs, ps\n",
    "\n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "        # backward pass: compute gradients going backwards\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        for t in reversed(xrange(self.seq_length)):\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1 # backprop into y\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext # backprop into h\n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh  # backprop through tanh non-linearity\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        return dWxh, dWhh, dWhy, dbh, dby\n",
    "\n",
    "    def loss(self, ps, targets):\n",
    "        \"\"\"loss for a sequence\"\"\"\n",
    "        return sum(-np.log(ps[t][targets[t],0]) for t in xrange(self.seq_length))\n",
    "\n",
    "    def update_model(self, dWxh, dWhh, dWhy, dbh, dby):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.Wxh, self.Whh, self.Why, self.bh, self.by],\n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  [self.mWxh, self.mWhh, self.mWhy, self.mbh, self.mby]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "\n",
    "    def sample(self, h, seed_ix, n):\n",
    "        \"\"\"\n",
    "        sample a sequence of integers from the model\n",
    "        h is memory state, seed_ix is seed letter from the first time step\n",
    "        \"\"\"\n",
    "        x = zero_init(self.vocab_size, 1)\n",
    "        x[seed_ix] = 1\n",
    "        ixes = []\n",
    "        for t in xrange(n):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = zero_init(self.vocab_size,1)\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        return ixes\n",
    "\n",
    "    def train(self, data_reader):\n",
    "        iter_num = 0\n",
    "        smooth_loss = -np.log(1.0/data_reader.vocab_size)*self.seq_length\n",
    "        while True:\n",
    "            if data_reader.just_started():\n",
    "                hprev = zero_init(self.hidden_size,1)\n",
    "            inputs, targets = data_reader.next_batch()\n",
    "            xs, hs, ps = self.forward(inputs, hprev)\n",
    "            dWxh, dWhh, dWhy, dbh, dby = self.backward(xs, hs, ps, targets)\n",
    "            loss = self.loss(ps, targets)\n",
    "            self.update_model(dWxh, dWhh, dWhy, dbh, dby)\n",
    "            smooth_loss = smooth_loss*0.999 + loss*0.001\n",
    "            hprev = hs[self.seq_length-1]\n",
    "            if not iter_num%500:\n",
    "                sample_ix = self.sample(hprev, inputs[0], 200)\n",
    "                print ''.join(data_reader.ix_to_char[ix] for ix in sample_ix)\n",
    "                print \"\\n\\niter :%d, loss:%f\"%(iter_num, smooth_loss)\n",
    "            iter_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9290ed-9dee-4cee-9c85-669f92b111ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    seq_length = 25\n",
    "    data_reader = DataReader(\"input.txt\", seq_length)\n",
    "    rnn = SimpleRNN(hidden_size=100, vocab_size=data_reader.vocab_size,seq_length=seq_length,learning_rate=1e-1)\n",
    "    rnn.train(data_reader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
