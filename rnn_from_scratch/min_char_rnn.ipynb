{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a2a30e-5b4d-4c06-81d9-878f4b21afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e61c0ad8-1576-4039-aaa0-d5fea67bb453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fd5d3f2-5ef0-4efa-a3e0-33bef6d628a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the words\n",
    "words = open('../data/names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22a04bf8-ba95-4faa-8185-9be7af86dc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 228145 characters, 27 unique.\n"
     ]
    }
   ],
   "source": [
    "data = open('../data/names.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92f84e2d-6b5f-409b-a1d1-907deb7fb87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'h', 1: '\\n', 2: 'm', 3: 'x', 4: 'n', 5: 'c', 6: 'p', 7: 'a', 8: 'o', 9: 'y', 10: 'w', 11: 's', 12: 'z', 13: 'l', 14: 'b', 15: 'g', 16: 't', 17: 'd', 18: 'e', 19: 'k', 20: 'u', 21: 'v', 22: 'r', 23: 'f', 24: 'q', 25: 'i', 26: 'j'}\n"
     ]
    }
   ],
   "source": [
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7a2098e-45f7-4e90-8877-f6fbb9716884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba1b621f-5890-45f9-9bfc-22a0dc35ee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98dd3e08-fb68-4bf8-8ae4-9f9f3f83d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "472390dc-b33f-4847-ad85-3e75a70d544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden - (100, 27)\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden (100, 100)\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output (27, 100)\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias (100, 1)\n",
    "by = np.zeros((vocab_size, 1)) # output bias (27, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69566eab-f3ca-4e4f-98f1-232750530dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs, targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    # Create four dictionaries\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    # Copy the last hidden state into hprev\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    # forward pass (each time step)\n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        # At this time step, input an encoded char, as a (27, 5) one hot vector\n",
    "        xs[t][inputs[t]] = 1\n",
    "        # At this time step, follow the formulas to get the hidden state at this time step\n",
    "        # (100,27) @ (27, 5) = (100, 5), (100, 100) @ (100, 5) = (100, 5) + (100, 1) -> (100, n) (column vector of 100 outputs\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "\n",
    "        # At this time step, compute the output state\n",
    "        # (27, 100) @ (100, 5) = (27, 5) + (27, 1) = (5, 27)\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "\n",
    "        # Normalize our probabiltiies using softmax\n",
    "        # ps = (5, 27)\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "\n",
    "        # Take the loss of the correct output probabilities\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    \n",
    "    # backward pass: compute gradients going backwards\n",
    "    # dWxh (100, 27), dWhh (100, 100), dWhy = (27, 100)\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    # dbh (100, 1),  dby (27, 1)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    ### ???\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "    # len(inputs) = 5\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backpropogate through the softmax to the logits\n",
    "        # (27, 5)\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "\n",
    "        # (27, 5) @ (5, 100) = (27, 100)\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        \n",
    "        # (27, 5) -> This secrety should sum across the columns dim = 1 to produce (27, 1)\n",
    "        dby += dy\n",
    "\n",
    "        # Backpropogate into the previous layer (100, 27) @ (27, 5) = (100, 5)\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "\n",
    "        # Backprop through tanh nonlinearity (element-wise forward -> element-wise backward)\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh \n",
    "\n",
    "        # \n",
    "        dbh += dhraw\n",
    "\n",
    "        # dWxh = (100, 5) @ (5, 27) = (100, 27)\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "\n",
    "        # dWhh = (100, 5) (5, 100) = (100, 100)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        # dWhh = (100, 5) @ (5, 27) \n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    # Clipping gradients\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
