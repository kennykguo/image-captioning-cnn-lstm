{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dPuz7WksL2FJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8OMEIfPPpIl",
    "outputId": "c228ebd4-7e42-40c1-c308-406d1ccd2807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "e5eIyEr-H9XW",
    "outputId": "1107d27d-4519-4218-a143-ee30008a1304"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use DataFrame.head() and DataFrame.tail() to view the top and bottom rows of the frame respectively:\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "4REb08ykI8pH",
    "outputId": "417a6e8f-fea0-4469-c0a0-f662571b0d6a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAJaUlEQVR4nO3cv2td9R/H8XNjQrg0WOoghVjQRIoUzGAISBXtUFEHHRxErEOGdikORWNEKiV/Qafq4CIZxAYKESchBAQFcVEHF4NiKVUQKZqhRZuY8x388przPjY/+3jM98XnQMN93jP002vbtm0AoGmagZ1+AAB2D1EAIEQBgBAFAEIUAAhRACBEAYAQBQBicLMf7PV6W/kcAGyxzfxfZW8KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAxuNMPALtFr9crb06ePFnevPTSS+VN0zTNoUOHypvJycnyZnFxsbx5//33y5urV6+WN2w9bwoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA4UI8+L+XX365vPn444/Lmx9//LG86bq7cuVKefPGG2+UN13Mzs5uyznUeFMAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQAiF7btu2mPtjrbfWzwB3z9ttvlzfPPPNMeTM0NFTefPDBB+VN0zTNwsJCebO+vl7e9Pv98ub27dvlzT///FPe8N9s5uvemwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAuBCPXe+jjz4qbyYmJsqbixcvljdLS0vlzfXr18sbuBNciAdAiSgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAxOBOPwB70z333FPeTE1NdTrrscceK28ef/zx8mZ1dbW84V/9fr+8ue+++zqdNTo6Wt789NNP5c2NGzfKm/3AmwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAuBCPTg4fPlzefPXVV53OOnv2bHnjcrt/dbkY8Lnnnitvnn/++fKm6wWJ3377bXnzwgsvdDrrbuRNAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBciMe2adu2025paekOP8nOGh4e7rSbm5srb86dO1fedPl3+uGHH8qbY8eOlTdN0zTXrl0rb27dutXprLuRNwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGA6LWbvP2q1+tt9bOwh/T7/fJmYWGh01nj4+PlzWeffVbefPrpp+XN/fffX97MzMyUN03TNFNTU+XN77//Xt48++yz5c13331X3rD9NvN1700BgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgHBLKttmZGSk0+77778vb44cOVLe7Pa/8VdffbW8+eKLL8qbX375pbxhb3BLKgAlogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEC/HY9R588MHyZnl5ubx56KGHypvt9MADD5Q3v/766xY8CXuVC/EAKBEFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIFyIx7Z54oknOu0uX75c3oyOjpY3Gxsb5c309HR5c/To0fKmaZrmqaeeKm9OnDjR6Sz2JxfiAVAiCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEAM7vQDsDcdPHiwvJmfn+90VpfL7W7evFneTE5OljcrKyvlzenTp8ubpul2Id6TTz5Z3nz55ZflDfuHNwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAcCEenVy4cKG8GRsb63TW2tpaeTM9PV3edLncrovx8fFtOadpmmZgwO8+avzFABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABC9tm3bTX2w19vqZ2GHvPbaa+XN/Px8efP333+XN03TNKdOnSpvFhcXO51V9fTTT5c3y8vLnc765ptvypvjx4+XN+vr6+UNe8Nmvu69KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDE4E4/AHfWyMhIeTMzM1PedLkg8cMPPyxvmmb7LrcbGxsrb86fP1/eDAx0+y32yiuvlDcut6PKmwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAuBBvn5mdnS1vJiYmypvffvutvHnzzTfLm67uvffe8uadd94pb/r9fnnz1ltvlTdN0zQ///xzpx1UeFMAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACBfi7TMHDhzYlnPW19fLm7/++qvTWUNDQ+XNJ598Ut5sbGyUNy+++GJ501Xbttt2FncvbwoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA4UK8fWZtbW1bzhkYqP+eeOSRRzqdtbCwUN48+uij5c2ZM2fKmz/++KO8gd3MmwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA0Wvbtt3UB3u9rX4W7oCHH364vFlZWdmCJ9lZ7733Xnnz7rvvljerq6vlDeyUzXzde1MAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACBfi7TPDw8Plzeuvv17ezM3NlTeff/55edM0TXPp0qXy5uuvvy5v/vzzz/IG9hIX4gFQIgoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAuBAP4C7hQjwASkQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAIjBzX6wbdutfA4AdgFvCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAMT/ANtsQEJOJvgWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "\n",
    "def plot_mnist_image(image_array):\n",
    "    image = image_array.reshape(28, 28)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.show()\n",
    "\n",
    "plot_mnist_image(data[4, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hG-IID2ZJCar",
    "outputId": "c24847d0-bded-4654-fbe0-185668455aab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "42000 785\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "m, n = data.shape\n",
    "print(m,n)\n",
    "# m is the number of training examples, n is the number of features + 1 (Y column)\n",
    "# m is # of rows, n is the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvDjeD2UPxZm",
    "outputId": "f3934e4b-64fe-45f5-c423-3c4e35fd496a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2, 6, ..., 3, 9, 4],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(data) # Shuffles all the individual rows\n",
    "data_dev = data[0:1000].T #Take the first 1000 rows, and transpose the matrix to get 1000 examples as column vectors\n",
    "data_dev\n",
    "\n",
    "# 1000 x 28 x 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxP2oNOjmy4k",
    "outputId": "18515c55-9a29-475d-da22-b2b7d01de611",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 6, 7, 8, 0, 7, 1, 8, 1, 8, 7, 3, 6, 1, 3, 3, 3, 9, 3, 7, 2,\n",
       "       5, 7, 4, 6, 6, 5, 0, 3, 7, 1, 0, 4, 0, 3, 2, 7, 1, 5, 0, 6, 1, 9,\n",
       "       6, 2, 2, 2, 3, 8, 5, 6, 7, 0, 9, 5, 9, 3, 3, 7, 4, 7, 1, 5, 7, 3,\n",
       "       8, 0, 8, 1, 3, 9, 0, 2, 8, 3, 2, 0, 3, 7, 4, 3, 8, 2, 0, 1, 2, 0,\n",
       "       3, 1, 8, 6, 2, 4, 2, 3, 0, 3, 4, 4, 5, 3, 0, 9, 1, 5, 8, 0, 5, 5,\n",
       "       6, 1, 1, 3, 2, 9, 5, 8, 7, 5, 2, 4, 7, 8, 0, 0, 1, 3, 4, 2, 3, 6,\n",
       "       5, 9, 4, 1, 8, 7, 8, 3, 1, 1, 5, 0, 5, 8, 0, 6, 7, 9, 2, 4, 9, 0,\n",
       "       6, 1, 1, 1, 2, 9, 7, 2, 2, 4, 0, 8, 4, 1, 3, 4, 3, 8, 4, 8, 7, 5,\n",
       "       0, 4, 1, 1, 3, 8, 3, 4, 7, 3, 0, 2, 7, 4, 5, 8, 3, 7, 7, 3, 7, 5,\n",
       "       2, 7, 0, 6, 8, 2, 7, 9, 1, 7, 9, 4, 1, 3, 9, 7, 5, 3, 6, 2, 6, 7,\n",
       "       9, 1, 4, 9, 4, 4, 0, 1, 1, 5, 1, 0, 7, 5, 1, 3, 8, 8, 7, 1, 8, 2,\n",
       "       2, 3, 0, 4, 3, 1, 9, 3, 4, 4, 9, 7, 3, 1, 9, 5, 4, 4, 5, 5, 9, 6,\n",
       "       4, 7, 8, 7, 9, 8, 5, 5, 2, 8, 2, 2, 8, 4, 5, 2, 6, 8, 4, 0, 2, 0,\n",
       "       4, 2, 6, 9, 3, 7, 8, 1, 2, 6, 9, 7, 9, 6, 0, 3, 4, 7, 6, 0, 9, 1,\n",
       "       8, 4, 8, 6, 6, 8, 7, 0, 2, 8, 5, 3, 7, 8, 5, 7, 5, 2, 6, 1, 8, 5,\n",
       "       8, 4, 7, 3, 3, 6, 2, 8, 6, 1, 6, 6, 6, 8, 4, 4, 0, 1, 3, 3, 7, 5,\n",
       "       6, 6, 4, 5, 1, 0, 1, 9, 6, 3, 9, 6, 5, 0, 5, 0, 3, 6, 4, 6, 1, 9,\n",
       "       1, 0, 6, 9, 4, 0, 9, 5, 5, 0, 5, 0, 2, 2, 4, 9, 6, 9, 5, 4, 1, 3,\n",
       "       2, 5, 1, 2, 1, 4, 7, 7, 5, 5, 2, 5, 4, 8, 8, 5, 5, 7, 8, 0, 9, 9,\n",
       "       3, 7, 5, 7, 3, 6, 6, 8, 3, 6, 3, 0, 9, 9, 3, 3, 7, 6, 2, 1, 7, 9,\n",
       "       0, 8, 5, 6, 6, 9, 6, 4, 8, 4, 2, 0, 9, 0, 8, 4, 3, 6, 8, 5, 1, 4,\n",
       "       0, 2, 0, 5, 6, 4, 5, 8, 6, 4, 8, 7, 4, 4, 1, 9, 5, 0, 5, 8, 5, 8,\n",
       "       2, 0, 7, 9, 7, 3, 0, 1, 4, 1, 9, 4, 2, 8, 8, 5, 4, 9, 7, 7, 7, 6,\n",
       "       6, 5, 2, 6, 1, 2, 5, 4, 4, 3, 1, 6, 8, 0, 4, 3, 2, 4, 0, 4, 1, 7,\n",
       "       9, 1, 9, 2, 4, 8, 8, 2, 5, 3, 4, 4, 3, 2, 1, 0, 7, 7, 3, 0, 0, 1,\n",
       "       1, 2, 6, 7, 9, 9, 8, 6, 7, 7, 5, 5, 0, 1, 1, 1, 3, 8, 2, 4, 6, 4,\n",
       "       9, 0, 9, 4, 7, 0, 2, 9, 5, 4, 7, 7, 4, 4, 3, 8, 1, 7, 6, 0, 8, 9,\n",
       "       3, 2, 1, 7, 1, 4, 8, 3, 9, 1, 1, 7, 1, 9, 9, 2, 2, 6, 9, 7, 7, 7,\n",
       "       8, 7, 3, 0, 6, 6, 1, 3, 1, 7, 7, 6, 4, 4, 5, 1, 5, 0, 0, 1, 2, 3,\n",
       "       2, 3, 9, 7, 0, 1, 1, 1, 0, 4, 9, 6, 3, 6, 8, 2, 1, 8, 2, 2, 1, 0,\n",
       "       3, 6, 9, 9, 0, 2, 2, 4, 1, 9, 2, 7, 7, 8, 7, 7, 7, 8, 8, 4, 2, 5,\n",
       "       6, 1, 2, 6, 6, 3, 1, 4, 3, 1, 6, 2, 8, 2, 8, 5, 9, 2, 8, 5, 3, 8,\n",
       "       9, 6, 2, 9, 8, 3, 7, 1, 3, 1, 7, 5, 4, 3, 2, 0, 5, 2, 1, 6, 0, 4,\n",
       "       7, 9, 5, 1, 1, 0, 9, 1, 9, 4, 6, 3, 6, 8, 7, 9, 3, 8, 8, 2, 2, 5,\n",
       "       1, 3, 4, 8, 4, 4, 5, 7, 7, 7, 7, 9, 4, 7, 5, 8, 6, 7, 4, 1, 8, 7,\n",
       "       8, 7, 2, 5, 3, 1, 2, 5, 9, 4, 7, 1, 6, 4, 1, 1, 7, 1, 5, 2, 7, 1,\n",
       "       7, 4, 2, 2, 8, 5, 1, 9, 9, 4, 4, 5, 1, 9, 1, 9, 3, 2, 3, 3, 9, 4,\n",
       "       1, 6, 1, 9, 3, 3, 9, 4, 1, 0, 0, 9, 4, 2, 7, 7, 2, 6, 3, 1, 0, 0,\n",
       "       7, 0, 5, 1, 0, 4, 9, 8, 6, 8, 4, 8, 0, 2, 1, 6, 5, 2, 3, 0, 9, 8,\n",
       "       6, 6, 0, 3, 2, 7, 2, 3, 1, 5, 6, 9, 9, 4, 3, 2, 6, 2, 3, 2, 1, 1,\n",
       "       3, 8, 8, 7, 6, 3, 6, 7, 5, 2, 2, 9, 1, 9, 3, 0, 3, 3, 6, 3, 9, 1,\n",
       "       0, 9, 9, 3, 6, 4, 9, 2, 0, 8, 6, 1, 8, 4, 9, 1, 2, 8, 2, 8, 4, 4,\n",
       "       8, 6, 1, 6, 9, 2, 4, 2, 6, 7, 6, 3, 7, 9, 2, 6, 1, 9, 3, 1, 6, 0,\n",
       "       0, 0, 0, 2, 3, 2, 5, 2, 4, 7, 8, 7, 3, 3, 9, 4, 4, 9, 2, 0, 8, 0,\n",
       "       0, 9, 9, 8, 5, 3, 5, 7, 8, 0, 6, 5, 4, 7, 6, 9, 7, 3, 5, 3, 3, 1,\n",
       "       4, 8, 0, 7, 0, 7, 4, 3, 9, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev = data_dev[0] #Takes the first row, which contains all of the answers to the numbers (the Y is what we want)\n",
    "Y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWeKiAlsm1oF",
    "outputId": "943393de-eb67-4bd3-ce8d-b5dd3e45741e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 1000)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01568627 0.01176471 0.30980392 0.         0.\n",
      " 0.         0.80784314 0.36470588 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.30980392\n",
      " 0.44705882 0.08627451 0.         0.         0.         0.90588235\n",
      " 0.13333333 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.22352941 0.36078431 0.4627451  0.58039216 0.28627451 0.\n",
      " 0.         0.         0.47058824 0.85098039 0.02745098 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.43921569 0.72941176\n",
      " 0.77254902 0.36078431 0.00392157 0.         0.         0.15294118\n",
      " 0.92941176 0.40784314 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.4627451  0.79215686 0.97254902 0.45098039 0.         0.\n",
      " 0.         0.         0.         0.76862745 0.9254902  0.06666667\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.07058824 0.77254902 0.92156863 0.99607843 0.96470588\n",
      " 0.56078431 0.21176471 0.         0.         0.         0.02352941\n",
      " 0.77254902 0.98431373 0.30980392 0.00784314 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.23921569\n",
      " 0.99607843 0.99607843 0.84313725 0.20784314 0.         0.\n",
      " 0.         0.         0.00392157 0.6        0.99607843 0.77647059\n",
      " 0.05098039 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.0745098  0.68235294 0.72156863\n",
      " 0.09019608 0.         0.         0.         0.         0.\n",
      " 0.4        0.99607843 0.96862745 0.2745098  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.46666667 0.99607843 0.6627451\n",
      " 0.14117647 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.22352941 0.90196078 0.99215686 0.23921569 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.17647059 0.94509804 0.99607843\n",
      " 0.63137255 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.04313725 0.64313725 0.99607843 0.83921569 0.14901961 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.65490196 0.99607843\n",
      " 0.82352941 0.16470588 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.16078431 0.76470588 0.80392157 0.27843137 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.04313725 0.15686275 0.4        0.50196078 0.71372549 0.9254902\n",
      " 0.99607843 0.3254902  0.0627451  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.08627451 0.69411765 0.88235294 0.99607843\n",
      " 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843\n",
      " 0.9372549  0.27058824 0.00392157 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.03921569\n",
      " 0.88627451 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843\n",
      " 0.99607843 0.99607843 0.85882353 0.82352941 0.92156863 0.99607843\n",
      " 0.2745098  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.22352941 0.99607843 0.96470588\n",
      " 0.50980392 0.23529412 0.54509804 0.99607843 0.79215686 0.3254902\n",
      " 0.03137255 0.         0.30196078 0.99607843 0.6745098  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.28235294 0.99607843 0.7372549  0.58823529 0.88627451\n",
      " 0.99607843 0.98823529 0.25490196 0.         0.         0.\n",
      " 0.35686275 0.99607843 0.86666667 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.05098039\n",
      " 1.         1.         1.         0.89803922 0.90588235 0.26666667\n",
      " 0.         0.         0.         0.         0.09411765 0.96078431\n",
      " 0.96470588 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "X_dev = data_dev[1:] #Takes all of the data corresponding to all of the entries (the X values)\n",
    "X_dev = X_dev / 255.\n",
    "# We divide all elements, elementwise, by 255 so that all numbers are decimal values, between 0 and 1\n",
    "print(X_dev.shape)\n",
    "print(X_dev[:, 1])\n",
    "print(X_dev[:, 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFJGdfIkSkP6",
    "outputId": "d5f61668-03f2-4afd-9361-b018e7bbc062",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   4, 147, 217,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  10, 218, 251, 100,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  85, 253,  76,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  54, 214, 131,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   7, 215, 237,\n",
       "        35,   0,   0,   0,   0,   0,   0, 113, 140,  16,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 117,\n",
       "       254, 111,   0,   0,   0,   0,   0,   0,  86, 248, 238, 124,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        30, 239, 229,  15,   0,   0,   0,   0,   0,   6, 226, 228,  28,\n",
       "         5,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0, 180, 236,  54,   0,   0,   0,   0,   0,   0,  89, 254,\n",
       "       185,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  47, 240,  87,   0,   0,   0,   0,   0,   0,   0,\n",
       "       201, 254,  73,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 161, 244,  13,   0,   0,   0,   0,   0,\n",
       "         0,   8, 224, 254,  29,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   4, 242, 172,   0,   0,   0,   0,\n",
       "         0,   0,   0,  89, 254, 212,  14,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   5, 254, 205,  14,   0,\n",
       "         0,   0,   0,   0,   0, 201, 254,  73,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   2, 212, 254,\n",
       "       225, 138,  75,  55,  12,   0,   9, 222, 254,  33,   0,   0,   0,\n",
       "         0,  77,  23,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        53, 168, 238, 254, 254, 254, 232, 226, 231, 254, 254, 230, 226,\n",
       "       226, 226, 226, 253, 165,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,  24, 109, 144, 192, 192, 192, 243, 254, 242,\n",
       "       192, 192, 192, 192, 168, 109,  20,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 182,\n",
       "       254,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0, 209, 254,  37,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0, 209, 254,  37,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 183, 254, 193,  87,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  57, 241, 194,  12,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = data[1000:m].T\n",
    "data_train[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3k210Xo9ooen"
   },
   "outputs": [],
   "source": [
    "Y_train = data_train[0] #Takes the first row, which contains all of the answers to the numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wCnPsdqxotp0"
   },
   "outputs": [],
   "source": [
    "X_train= data_train[1:n] #Takes all of the data corresponding to all of the entries\n",
    "X_train\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "n_TnFSWzUOdz"
   },
   "outputs": [],
   "source": [
    "#X_train\n",
    "#Y_train\n",
    "\n",
    "def params(batch_size):\n",
    "  W1 = np.random.rand(15, 784) - 0.5\n",
    "  b1 = np.random.rand(15, batch_size) - 0.5\n",
    "  W2 = np.random.rand(10, 15) - 0.5\n",
    "  b2 = np.random.rand(10, batch_size) - 0.5\n",
    "  return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z): # Takes in a scalar, returns a scalar\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def softmax(Z):\n",
    "    # Apply softmax column-wise\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=0))  # Subtracting the maximum value in each column to avoid overflow\n",
    "    return exp_Z / np.sum(exp_Z, axis=0)\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X, batch_size):\n",
    "    Z1 = W1.dot(X) + b1 # (10 x784) (784 x n) + (10 x n) -> (10 x n)\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2 # (10 x 10) (10 x n) + (10 x n) = (10 x n)\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def der_ReLU(Z):\n",
    "  return Z > 0\n",
    "\n",
    "def create(Y): # Passing in a 1 x 41000 matrix (41000 columns, 1 row)\n",
    "  mat_Y = np.zeros((Y.size, 10)) # 41000 x 10\n",
    "  mat_Y[np.arange(Y.size), Y] = 1 # array([    0,     1,     2, ..., 40997, 40998, 40999]), then indexing the Y values aswell at each column, changing that value to 1\n",
    "  mat_Y = mat_Y.T # 10 x 41000\n",
    "  return mat_Y\n",
    "\n",
    "\n",
    "def back_prop(Z1, A1, Z2, A2, W1, W2, X, Y, batch_size):\n",
    "  mat_Y = create(Y)\n",
    "  dZ2 = (1/batch_size) * (A2 - mat_Y) # 10 x 41000 - - - Back propogation eq. #1\n",
    "  dW2 = dZ2.dot(A1.T) # (10 x 41000) (41000 x 10) -> (10 x 10) - - - Back propogation eq. #4\n",
    "  db2 = np.sum(dZ2) # scalar  - - - Back propogation eq. #3\n",
    "  dZ1 = (1/batch_size) * (W2.T.dot(dZ2) * der_ReLU(Z1)) # (10 x 10) (10 x 41000) * elementwise (1 or 0) Back propogation eq. #2\n",
    "  dW1=  dZ1.dot(X.T) # (10 x 41000) (41000 x 784) -> (10 x 784) - - - Back propogation eq. #4\n",
    "  db1 = np.sum(dZ1) #scalar - - - Back propogation eq. #3\n",
    "  return dW1, db1, dW2, db2\n",
    "\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "  W1 = W1 - alpha * dW1\n",
    "  b1 = b1 - alpha * db1\n",
    "  W2 = W2 - alpha * dW2\n",
    "  b2 = b2 - alpha * db2\n",
    "  return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "def get_predictions(A2):\n",
    "  return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "  # print(predictions)\n",
    "  # print(Y)\n",
    "  return np.sum(predictions == Y) / Y.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uF4cuvomklzP",
    "outputId": "2ffe30dc-e5a6-4f4a-d355-bdabff22b96d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41000"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_columns = X_train.shape[1]\n",
    "num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "FWQAzUy_kK3l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stochastic_gradient_descent(X, Y, epochs, alpha, batch_size):\n",
    "    # Initialize the random parameters\n",
    "    W1, b1, W2, b2 = params(batch_size)\n",
    "\n",
    "    # Calculate the number of examples\n",
    "    num_examples = X.shape[1]\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Generate a random permutation of indices\n",
    "        permuted_indices = np.random.permutation(num_examples)\n",
    "\n",
    "        # Shuffle both X and Y using the same permutation of indices\n",
    "        X_shuffled = X[:, permuted_indices]\n",
    "        Y_shuffled = Y[permuted_indices]\n",
    "\n",
    "        # Iterate over the shuffled data in batches\n",
    "        for j in range(0, num_examples, batch_size):\n",
    "            X_batch = X_shuffled[:, j:j + batch_size]\n",
    "            Y_batch = Y_shuffled[j:j + batch_size]\n",
    "\n",
    "            # Forward propagation\n",
    "            Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X_batch, batch_size)\n",
    "\n",
    "            # Back propagation\n",
    "            dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, W1, W2, X_batch, Y_batch, batch_size)\n",
    "\n",
    "            # Update weights\n",
    "            W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "\n",
    "        print(\"Epoch:\", i)\n",
    "        # Calculate accuracy using the entire dataset\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X_batch, batch_size)\n",
    "        print(\"Accuracy:\", get_accuracy(get_predictions(A2), Y_batch))\n",
    "\n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VfYy_XctvU87",
    "outputId": "1dbd0a79-0e61-488e-9020-1e6ee3d70e28",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Accuracy: 0.43\n",
      "Epoch: 1\n",
      "Accuracy: 0.57\n",
      "Epoch: 2\n",
      "Accuracy: 0.53\n",
      "Epoch: 3\n",
      "Accuracy: 0.53\n",
      "Epoch: 4\n",
      "Accuracy: 0.6\n",
      "Epoch: 5\n",
      "Accuracy: 0.68\n",
      "Epoch: 6\n",
      "Accuracy: 0.63\n",
      "Epoch: 7\n",
      "Accuracy: 0.67\n",
      "Epoch: 8\n",
      "Accuracy: 0.67\n",
      "Epoch: 9\n",
      "Accuracy: 0.69\n",
      "Epoch: 10\n",
      "Accuracy: 0.7\n",
      "Epoch: 11\n",
      "Accuracy: 0.76\n",
      "Epoch: 12\n",
      "Accuracy: 0.73\n",
      "Epoch: 13\n",
      "Accuracy: 0.71\n",
      "Epoch: 14\n",
      "Accuracy: 0.78\n",
      "Epoch: 15\n",
      "Accuracy: 0.7\n",
      "Epoch: 16\n",
      "Accuracy: 0.81\n",
      "Epoch: 17\n",
      "Accuracy: 0.75\n",
      "Epoch: 18\n",
      "Accuracy: 0.73\n",
      "Epoch: 19\n",
      "Accuracy: 0.75\n",
      "Epoch: 20\n",
      "Accuracy: 0.76\n",
      "Epoch: 21\n",
      "Accuracy: 0.8\n",
      "Epoch: 22\n",
      "Accuracy: 0.77\n",
      "Epoch: 23\n",
      "Accuracy: 0.77\n",
      "Epoch: 24\n",
      "Accuracy: 0.87\n",
      "Epoch: 25\n",
      "Accuracy: 0.75\n",
      "Epoch: 26\n",
      "Accuracy: 0.78\n",
      "Epoch: 27\n",
      "Accuracy: 0.76\n",
      "Epoch: 28\n",
      "Accuracy: 0.84\n",
      "Epoch: 29\n",
      "Accuracy: 0.73\n",
      "Epoch: 30\n",
      "Accuracy: 0.77\n",
      "Epoch: 31\n",
      "Accuracy: 0.77\n",
      "Epoch: 32\n",
      "Accuracy: 0.78\n",
      "Epoch: 33\n",
      "Accuracy: 0.82\n",
      "Epoch: 34\n",
      "Accuracy: 0.87\n",
      "Epoch: 35\n",
      "Accuracy: 0.87\n",
      "Epoch: 36\n",
      "Accuracy: 0.83\n",
      "Epoch: 37\n",
      "Accuracy: 0.86\n",
      "Epoch: 38\n",
      "Accuracy: 0.85\n",
      "Epoch: 39\n",
      "Accuracy: 0.84\n",
      "Epoch: 40\n",
      "Accuracy: 0.81\n",
      "Epoch: 41\n",
      "Accuracy: 0.82\n",
      "Epoch: 42\n",
      "Accuracy: 0.83\n",
      "Epoch: 43\n",
      "Accuracy: 0.86\n",
      "Epoch: 44\n",
      "Accuracy: 0.9\n",
      "Epoch: 45\n",
      "Accuracy: 0.83\n",
      "Epoch: 46\n",
      "Accuracy: 0.8\n",
      "Epoch: 47\n",
      "Accuracy: 0.84\n",
      "Epoch: 48\n",
      "Accuracy: 0.91\n",
      "Epoch: 49\n",
      "Accuracy: 0.89\n",
      "Epoch: 50\n",
      "Accuracy: 0.85\n",
      "Epoch: 51\n",
      "Accuracy: 0.82\n",
      "Epoch: 52\n",
      "Accuracy: 0.79\n",
      "Epoch: 53\n",
      "Accuracy: 0.84\n",
      "Epoch: 54\n",
      "Accuracy: 0.84\n",
      "Epoch: 55\n",
      "Accuracy: 0.82\n",
      "Epoch: 56\n",
      "Accuracy: 0.9\n",
      "Epoch: 57\n",
      "Accuracy: 0.77\n",
      "Epoch: 58\n",
      "Accuracy: 0.89\n",
      "Epoch: 59\n",
      "Accuracy: 0.87\n",
      "Epoch: 60\n",
      "Accuracy: 0.91\n",
      "Epoch: 61\n",
      "Accuracy: 0.83\n",
      "Epoch: 62\n",
      "Accuracy: 0.92\n",
      "Epoch: 63\n",
      "Accuracy: 0.82\n",
      "Epoch: 64\n",
      "Accuracy: 0.9\n",
      "Epoch: 65\n",
      "Accuracy: 0.87\n",
      "Epoch: 66\n",
      "Accuracy: 0.82\n",
      "Epoch: 67\n",
      "Accuracy: 0.84\n",
      "Epoch: 68\n",
      "Accuracy: 0.85\n",
      "Epoch: 69\n",
      "Accuracy: 0.88\n",
      "Epoch: 70\n",
      "Accuracy: 0.9\n",
      "Epoch: 71\n",
      "Accuracy: 0.83\n",
      "Epoch: 72\n",
      "Accuracy: 0.85\n",
      "Epoch: 73\n",
      "Accuracy: 0.81\n",
      "Epoch: 74\n",
      "Accuracy: 0.83\n",
      "Epoch: 75\n",
      "Accuracy: 0.9\n",
      "Epoch: 76\n",
      "Accuracy: 0.89\n",
      "Epoch: 77\n",
      "Accuracy: 0.87\n",
      "Epoch: 78\n",
      "Accuracy: 0.87\n",
      "Epoch: 79\n",
      "Accuracy: 0.87\n",
      "Epoch: 80\n",
      "Accuracy: 0.86\n",
      "Epoch: 81\n",
      "Accuracy: 0.84\n",
      "Epoch: 82\n",
      "Accuracy: 0.88\n",
      "Epoch: 83\n",
      "Accuracy: 0.84\n",
      "Epoch: 84\n",
      "Accuracy: 0.88\n",
      "Epoch: 85\n",
      "Accuracy: 0.9\n",
      "Epoch: 86\n",
      "Accuracy: 0.87\n",
      "Epoch: 87\n",
      "Accuracy: 0.81\n",
      "Epoch: 88\n",
      "Accuracy: 0.87\n",
      "Epoch: 89\n",
      "Accuracy: 0.84\n",
      "Epoch: 90\n",
      "Accuracy: 0.88\n",
      "Epoch: 91\n",
      "Accuracy: 0.8\n",
      "Epoch: 92\n",
      "Accuracy: 0.91\n",
      "Epoch: 93\n",
      "Accuracy: 0.84\n",
      "Epoch: 94\n",
      "Accuracy: 0.87\n",
      "Epoch: 95\n",
      "Accuracy: 0.86\n",
      "Epoch: 96\n",
      "Accuracy: 0.92\n",
      "Epoch: 97\n",
      "Accuracy: 0.89\n",
      "Epoch: 98\n",
      "Accuracy: 0.82\n",
      "Epoch: 99\n",
      "Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = stochastic_gradient_descent(X_train, Y_train, 100, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "shhDA3BmvqiP",
    "outputId": "36eeab71-c65b-4c70-a1ad-ef36f072fef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 3 8 3 1 5 8 0 5 5 6 1 1 6 2 9 5 3 7 0 2 4 7 8 0 0 1 3 4 2 9 6 5 9 4 1 8\n",
      " 7 8 3 1 1 5 0 7 0 0 6 7 9 2 4 9 0 6 1 1 1 8 9 7 2 7 4 0 8 4 1 3 4 8 8 4 8\n",
      " 9 5 0 9 1 1 3 5 3 4 7 9 0 2 3 9 5 5 3 7 7 3 7 7 2 7]\n",
      "[5 3 0 9 1 5 8 0 5 5 6 1 1 3 2 9 5 8 7 5 2 4 7 8 0 0 1 3 4 2 3 6 5 9 4 1 8\n",
      " 7 8 3 1 1 5 0 5 8 0 6 7 9 2 4 9 0 6 1 1 1 2 9 7 2 2 4 0 8 4 1 3 4 3 8 4 8\n",
      " 7 5 0 4 1 1 3 8 3 4 7 3 0 2 7 4 5 8 3 7 7 3 7 5 2 7]\n",
      "Accuracy is: 81.0 %\n"
     ]
    }
   ],
   "source": [
    "# Testing our model:\n",
    "\n",
    "# Create batch from unseen data\n",
    "training_example = X_dev[:,100:200]\n",
    "Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, training_example, batch_size = 1)\n",
    "predictions = get_predictions (A2)\n",
    "print(predictions)\n",
    "print(Y_dev[100:200])\n",
    "\n",
    "print(\"Accuracy is:\", get_accuracy(Y_dev[100:200], predictions) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZjaNbHcxiz7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
